{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNn0CQT3hHQQO65PRxEZXOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ffc75b07466945bf92309a1e3ba57213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a181f057802f4011b743f321b7d5ee80",
              "IPY_MODEL_a1ea3e0ede194f3da97f8a4975ecfdf4",
              "IPY_MODEL_c7ac87c9810645498c175e196319af10"
            ],
            "layout": "IPY_MODEL_ef7ea3d33bb240acb81afd06888088c9"
          }
        },
        "a181f057802f4011b743f321b7d5ee80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb470b4f27e44a78abf9ce2594b956cc",
            "placeholder": "​",
            "style": "IPY_MODEL_6205d5bdf9a84eabb3d2390ec83091ad",
            "value": "config.json: 100%"
          }
        },
        "a1ea3e0ede194f3da97f8a4975ecfdf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61c768899eec4817b74383803772c015",
            "max": 644,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5bba1660f3fd40958ac36901a61a6d46",
            "value": 644
          }
        },
        "c7ac87c9810645498c175e196319af10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8117c2785e6543e39b788c61177bb74d",
            "placeholder": "​",
            "style": "IPY_MODEL_2c6e589b8ef44d2fa56894711e2bf36f",
            "value": " 644/644 [00:00&lt;00:00, 55.8kB/s]"
          }
        },
        "ef7ea3d33bb240acb81afd06888088c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb470b4f27e44a78abf9ce2594b956cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6205d5bdf9a84eabb3d2390ec83091ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61c768899eec4817b74383803772c015": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bba1660f3fd40958ac36901a61a6d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8117c2785e6543e39b788c61177bb74d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c6e589b8ef44d2fa56894711e2bf36f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38efce1dc63140279a2a1d06e5f4557c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_522c981bb84e44d5b48d6b6cad1be8bd",
              "IPY_MODEL_5c59d517ba1f446e9d722bdbabd4df8e",
              "IPY_MODEL_407292bd9ed74389aa9cc5f5d68d8524"
            ],
            "layout": "IPY_MODEL_97ca9eec8cb2478c9f9a4e7a691614d2"
          }
        },
        "522c981bb84e44d5b48d6b6cad1be8bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eac5d469a8d349a29257825e175f7151",
            "placeholder": "​",
            "style": "IPY_MODEL_66977b304c0941e9bd300dfd0fe8bb79",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "5c59d517ba1f446e9d722bdbabd4df8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_169495489d3d44da8a23a6a7b776e2b1",
            "max": 662513657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f6b3b191d014013ab0878beab77b6cd",
            "value": 662513657
          }
        },
        "407292bd9ed74389aa9cc5f5d68d8524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2927ec6fd27942ce9a9f2180c25ed74b",
            "placeholder": "​",
            "style": "IPY_MODEL_208a362c6fe84586b96fa5e2c809e4c5",
            "value": " 663M/663M [00:02&lt;00:00, 208MB/s]"
          }
        },
        "97ca9eec8cb2478c9f9a4e7a691614d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eac5d469a8d349a29257825e175f7151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66977b304c0941e9bd300dfd0fe8bb79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "169495489d3d44da8a23a6a7b776e2b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f6b3b191d014013ab0878beab77b6cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2927ec6fd27942ce9a9f2180c25ed74b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "208a362c6fe84586b96fa5e2c809e4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be7392f3711e4720a83accc26ee4b057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d085652da690449fa622665b99bb85bf",
              "IPY_MODEL_5e44fdcca0854d7b94b90205c167a7a3",
              "IPY_MODEL_3217170117eb4c4093b00c471e25b323"
            ],
            "layout": "IPY_MODEL_cdf94bfa80084883b93a4a1a163a6716"
          }
        },
        "d085652da690449fa622665b99bb85bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0b90fc048a648a4974e594d4b179cd6",
            "placeholder": "​",
            "style": "IPY_MODEL_f49a10054a8547e1a025eb6ade074cc2",
            "value": "generation_config.json: 100%"
          }
        },
        "5e44fdcca0854d7b94b90205c167a7a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f3a7a3252e94399ac9eae4ff5c183c8",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16df36b2bed74cc1ad43e3b366fe5256",
            "value": 137
          }
        },
        "3217170117eb4c4093b00c471e25b323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44502fb453bf4af3b50400c76e344f80",
            "placeholder": "​",
            "style": "IPY_MODEL_cf035dc7caa24f16b78cc057f2298244",
            "value": " 137/137 [00:00&lt;00:00, 11.6kB/s]"
          }
        },
        "cdf94bfa80084883b93a4a1a163a6716": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0b90fc048a648a4974e594d4b179cd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f49a10054a8547e1a025eb6ade074cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f3a7a3252e94399ac9eae4ff5c183c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16df36b2bed74cc1ad43e3b366fe5256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44502fb453bf4af3b50400c76e344f80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf035dc7caa24f16b78cc057f2298244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae83ee60a3df4f8989d2333fcda92a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b88e74afa544b87bdc1cedcaef136d0",
              "IPY_MODEL_60c1e95b32364105a3befb6097cc5e3b",
              "IPY_MODEL_b126746f863945b19a46f58ffb3043f3"
            ],
            "layout": "IPY_MODEL_80294eb9885e44e09793fa15f183aa2c"
          }
        },
        "3b88e74afa544b87bdc1cedcaef136d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10d04f78cc1c44648ef42c815838e46e",
            "placeholder": "​",
            "style": "IPY_MODEL_200519b70e094bee975c4adb6897c379",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "60c1e95b32364105a3befb6097cc5e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbefb45ec6ee4a489eb65ce07eb0cb01",
            "max": 685,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c47f610ab754023babcf4f919d97213",
            "value": 685
          }
        },
        "b126746f863945b19a46f58ffb3043f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_521808c459384d9fbe2e5112479f6a0f",
            "placeholder": "​",
            "style": "IPY_MODEL_7af04fd064394f6cb7ae1229fc19b46b",
            "value": " 685/685 [00:00&lt;00:00, 62.7kB/s]"
          }
        },
        "80294eb9885e44e09793fa15f183aa2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10d04f78cc1c44648ef42c815838e46e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "200519b70e094bee975c4adb6897c379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbefb45ec6ee4a489eb65ce07eb0cb01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c47f610ab754023babcf4f919d97213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "521808c459384d9fbe2e5112479f6a0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7af04fd064394f6cb7ae1229fc19b46b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe01f409e97c41df966c907606f7528a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0e3a88ea2724b9192b94de96dc0cd46",
              "IPY_MODEL_ad0ed64a562349cdaaf3d39ddb6a528d",
              "IPY_MODEL_66f227f50292475eba5ec31b84336504"
            ],
            "layout": "IPY_MODEL_44324b9c5e55429ca9871624594b0557"
          }
        },
        "c0e3a88ea2724b9192b94de96dc0cd46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb0dd012533448f6aaa8ee997e724ecd",
            "placeholder": "​",
            "style": "IPY_MODEL_0a9f31b1c5c2429481d10bdcd3e442da",
            "value": "vocab.json: 100%"
          }
        },
        "ad0ed64a562349cdaaf3d39ddb6a528d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82c7866363d14e9e98e32c0c07753ff0",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd09a264eec345f187d22595524aa4d0",
            "value": 898822
          }
        },
        "66f227f50292475eba5ec31b84336504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6087604359754e0593fc926813bb4dea",
            "placeholder": "​",
            "style": "IPY_MODEL_47d3f791286842e38a179bf95e4adac1",
            "value": " 899k/899k [00:00&lt;00:00, 1.24MB/s]"
          }
        },
        "44324b9c5e55429ca9871624594b0557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0dd012533448f6aaa8ee997e724ecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a9f31b1c5c2429481d10bdcd3e442da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82c7866363d14e9e98e32c0c07753ff0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd09a264eec345f187d22595524aa4d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6087604359754e0593fc926813bb4dea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47d3f791286842e38a179bf95e4adac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f69a3923f97848bb83a399c1966f2975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e2db761e000480994b1ea04e11ca6dc",
              "IPY_MODEL_c226f61f17b64ff5899cce942a65eddb",
              "IPY_MODEL_8f9020ce117f4693b7dbdcc8bc6e0d3b"
            ],
            "layout": "IPY_MODEL_d339253d65ba454a8f0af5968052a84f"
          }
        },
        "0e2db761e000480994b1ea04e11ca6dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eeb7a193a6c443f9daba93ea2e14aeb",
            "placeholder": "​",
            "style": "IPY_MODEL_e14984483e774984a3b221dd12a2848c",
            "value": "merges.txt: 100%"
          }
        },
        "c226f61f17b64ff5899cce942a65eddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35a4ccffb1ca4c7dbc82239ee985b60b",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_193e92f291994916bd74260a6d35467a",
            "value": 456318
          }
        },
        "8f9020ce117f4693b7dbdcc8bc6e0d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82bcfb2ac513481abf89204175ee7584",
            "placeholder": "​",
            "style": "IPY_MODEL_097437fab2164dada7bd174acd147685",
            "value": " 456k/456k [00:00&lt;00:00, 921kB/s]"
          }
        },
        "d339253d65ba454a8f0af5968052a84f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eeb7a193a6c443f9daba93ea2e14aeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e14984483e774984a3b221dd12a2848c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35a4ccffb1ca4c7dbc82239ee985b60b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "193e92f291994916bd74260a6d35467a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82bcfb2ac513481abf89204175ee7584": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "097437fab2164dada7bd174acd147685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b1550b6f3504a458507157222817e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba14ffc9d1a749e18045ebd9caf3e1ed",
              "IPY_MODEL_49da5217427746908704e99a49dd292e",
              "IPY_MODEL_6408ae90853247eebb73837e197c42c5"
            ],
            "layout": "IPY_MODEL_9b0b19716ae24c51b0055aa22402994d"
          }
        },
        "ba14ffc9d1a749e18045ebd9caf3e1ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67721e49d1714a95aa53022d60c136ac",
            "placeholder": "​",
            "style": "IPY_MODEL_5b7ca7bea8f54536aa3071ff90588fef",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "49da5217427746908704e99a49dd292e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aead6096f5dc4053bb79261fa65aedfb",
            "max": 441,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d9a1e81b31740719ea8bd13de7dd6ae",
            "value": 441
          }
        },
        "6408ae90853247eebb73837e197c42c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eea2ed4e0514a2c8114ab5fb371718d",
            "placeholder": "​",
            "style": "IPY_MODEL_3280ddc0dfaa4fddade86a0b8f83622c",
            "value": " 441/441 [00:00&lt;00:00, 45.7kB/s]"
          }
        },
        "9b0b19716ae24c51b0055aa22402994d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67721e49d1714a95aa53022d60c136ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b7ca7bea8f54536aa3071ff90588fef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aead6096f5dc4053bb79261fa65aedfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d9a1e81b31740719ea8bd13de7dd6ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7eea2ed4e0514a2c8114ab5fb371718d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3280ddc0dfaa4fddade86a0b8f83622c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekkiprop/llms/blob/main/G02_Quantized_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhe8RIJVBV_h",
        "outputId": "92aa00c8-fa48-429a-b5e3-6c3451d875f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting trl\n",
            "  Downloading trl-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: torch~=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.2.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.47.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch~=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0->bitsandbytes) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch~=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch~=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch~=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch~=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch~=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch~=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch~=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch~=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch~=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch~=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch~=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch~=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.14.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, bitsandbytes, trl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.1 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.14.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# If you're running on Colab\n",
        "!pip install datasets bitsandbytes trl\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from accelerate import init_empty_weights\n",
        "from accelerate.utils.modeling import find_tied_parameters, get_mixed_precision_context_manager\n",
        "from accelerate.utils.operations import convert_outputs_to_fp32\n",
        "from bitsandbytes.nn import Linear8bitLt, Linear4bit, LinearFP4, LinearNF4\n",
        "from collections import Counter\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, AutoConfig\n",
        "from transformers.integrations.bitsandbytes import get_keys_to_not_convert\n",
        "from types import MethodType\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_size(num_params, num_bits):\n",
        "\n",
        "  return (num_params*(num_bits/8)/1e6)"
      ],
      "metadata": {
        "id": "2bN_89dUEU-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_size(360e6, 4)"
      ],
      "metadata": {
        "id": "MuGgoKDvCZAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(11)\n",
        "weights = torch.randn(1000) * .07\n",
        "weights.min(), weights.max()"
      ],
      "metadata": {
        "id": "uqHVITrSGWHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# binning the values in to 4 bins\n",
        "n_bins = 4\n",
        "bins =  torch.linspace(weights.min(), weights.max(), n_bins+1)\n",
        "bin_width = bins[1] - bins[0]\n",
        "bins, bin_width"
      ],
      "metadata": {
        "id": "UqPEDQCWGxMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bins\n"
      ],
      "metadata": {
        "id": "YyT2RBQ2GzSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(weights, bins)"
      ],
      "metadata": {
        "id": "n6Jgt2gXIqLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the bind indexes for each weight\n",
        "bin_indexes = (weights.view(-1, 1)>bins).to(torch.int).argmin(dim=1) * 1\n",
        "print(weights[:20], bin_indexes[:20])"
      ],
      "metadata": {
        "id": "K8MXrUNbJLIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bins"
      ],
      "metadata": {
        "id": "mGjA1OgIJ8W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bin_values = bins[:-1]\n",
        "first_bin = bin_values[0]\n",
        "bin_values"
      ],
      "metadata": {
        "id": "61y0ZylUKA4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_bin"
      ],
      "metadata": {
        "id": "vFaQ5kv6K3ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieving the approximate original values\n",
        "torch.arange(0, n_bins) * bin_width + first_bin"
      ],
      "metadata": {
        "id": "gDGWMHThLTmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "approx_values  = bin_indexes * bin_width + first_bin\n",
        "print(approx_values[:20])"
      ],
      "metadata": {
        "id": "-qOfTAgxMyUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Use MSE to check simillarity between the original weights and the approximated values\n"
      ],
      "metadata": {
        "id": "21vZXxKvNZY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse_fn = nn.MSELoss()\n",
        "mse_fn(approx_values, weights).sqrt()"
      ],
      "metadata": {
        "id": "-Yb7nHFdM9Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for Quantization and Dequantization\n",
        "\n"
      ],
      "metadata": {
        "id": "zs4a380lNaZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize(weights, n_bits=8):\n",
        "  assert n_bits <=16, \"Using more bits may result in slow execution and/or crashing.\"\n",
        "  n_bits  = 2 ** n_bits\n",
        "  bins = torch.linspace(weights.min(), weights.max(), n_bits+1)\n",
        "  first_bin =  bins[0]\n",
        "  bin_width = bins[1] - bins[0]\n",
        "  bin_indexes = ((weights.view(-1, 1) > bins).to(torch.int).argmin(dim=1) * 1)\n",
        "  return bin_indexes, bin_width, first_bin\n",
        "\n",
        "\n",
        "def dequantize(bin_indexes, bin_width, first_bin):\n",
        "  approx_values = bin_indexes * bin_width + first_bin\n",
        "  return approx_values"
      ],
      "metadata": {
        "id": "1kNSAAcMNN8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing RMSE of quantization choices\n",
        "for n_bits in [2, 4, 8,16]:\n",
        "  res = quantize(weights, n_bits=n_bits)\n",
        "  approx_values = dequantize(*res)\n",
        "  print(f'{n_bits}-bit Quantization:')\n",
        "  print(approx_values[:6])\n",
        "  print(weights[:6])\n",
        "  print(mse_fn(approx_values, weights).sqrt())"
      ],
      "metadata": {
        "id": "Bo3V0XZYQ4W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights.dtype\n"
      ],
      "metadata": {
        "id": "Z1hMgE1fQ-dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp16_weights = weights.to(torch.float16)\n",
        "fp16_weights.dtype"
      ],
      "metadata": {
        "id": "tmdzG3MRTWsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights"
      ],
      "metadata": {
        "id": "XRFvpqC6Tlyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_fn(fp16_weights, weights)"
      ],
      "metadata": {
        "id": "s3qTM2v2Tslv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(14)\n",
        "tiny_values = torch.randn(1000) * 1e-5\n",
        "fp16_tiny_values = tiny_values.to(torch.float16)\n",
        "mse_fn(fp16_tiny_values, tiny_values)"
      ],
      "metadata": {
        "id": "AMoMz3k7T2o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(tiny_values[155:160])\n",
        "print(fp16_tiny_values[155:160])"
      ],
      "metadata": {
        "id": "4v9ZTAqSUZxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(19)\n",
        "large_values = torch.randn(1000) * 1e5\n",
        "fp16_large_values = large_values.to(torch.float16)\n",
        "print(large_values[:5])\n",
        "print(fp16_large_values[:5])"
      ],
      "metadata": {
        "id": "QHZ7ujP8Uiu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp16_info = torch.finfo(torch.float16)\n",
        "fp16_info"
      ],
      "metadata": {
        "id": "CgeoY58OVUOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp32_info = torch.finfo(torch.float32)\n",
        "fp32_info"
      ],
      "metadata": {
        "id": "EeDtXti-Vjfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bf16_info = torch.finfo(torch.bfloat16)\n",
        "fp16_info, bf16_info, fp32_info"
      ],
      "metadata": {
        "id": "BFqcAh9jVrYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smallest_subnormal = fp16_info.smallest_normal * 2**-10\n",
        "smallest_subnormal\n"
      ],
      "metadata": {
        "id": "UJJnf_SFopsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([0.5555555555])\n",
        "torch.set_printoptions(precision= 9)\n",
        "print(x)\n",
        "print(x.to(torch.float32))\n",
        "print(x.to(torch.float16))\n",
        "print(x.to(torch.bfloat16))\n",
        "torch.set_printoptions(precision=4)"
      ],
      "metadata": {
        "id": "dWJVFIUdrVnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Models"
      ],
      "metadata": {
        "id": "wvjUujmZtcF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_parm_dtypes(iterable, top_k=3):\n",
        "  return Counter([p.dtype for p in iterable]).most_common(top_k)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", device_map = 'cuda:0')\n",
        "print(model.get_memory_footprint()/1e6, get_parm_dtypes(model.parameters()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "ffc75b07466945bf92309a1e3ba57213",
            "a181f057802f4011b743f321b7d5ee80",
            "a1ea3e0ede194f3da97f8a4975ecfdf4",
            "c7ac87c9810645498c175e196319af10",
            "ef7ea3d33bb240acb81afd06888088c9",
            "cb470b4f27e44a78abf9ce2594b956cc",
            "6205d5bdf9a84eabb3d2390ec83091ad",
            "61c768899eec4817b74383803772c015",
            "5bba1660f3fd40958ac36901a61a6d46",
            "8117c2785e6543e39b788c61177bb74d",
            "2c6e589b8ef44d2fa56894711e2bf36f",
            "38efce1dc63140279a2a1d06e5f4557c",
            "522c981bb84e44d5b48d6b6cad1be8bd",
            "5c59d517ba1f446e9d722bdbabd4df8e",
            "407292bd9ed74389aa9cc5f5d68d8524",
            "97ca9eec8cb2478c9f9a4e7a691614d2",
            "eac5d469a8d349a29257825e175f7151",
            "66977b304c0941e9bd300dfd0fe8bb79",
            "169495489d3d44da8a23a6a7b776e2b1",
            "4f6b3b191d014013ab0878beab77b6cd",
            "2927ec6fd27942ce9a9f2180c25ed74b",
            "208a362c6fe84586b96fa5e2c809e4c5",
            "be7392f3711e4720a83accc26ee4b057",
            "d085652da690449fa622665b99bb85bf",
            "5e44fdcca0854d7b94b90205c167a7a3",
            "3217170117eb4c4093b00c471e25b323",
            "cdf94bfa80084883b93a4a1a163a6716",
            "e0b90fc048a648a4974e594d4b179cd6",
            "f49a10054a8547e1a025eb6ade074cc2",
            "1f3a7a3252e94399ac9eae4ff5c183c8",
            "16df36b2bed74cc1ad43e3b366fe5256",
            "44502fb453bf4af3b50400c76e344f80",
            "cf035dc7caa24f16b78cc057f2298244"
          ]
        },
        "id": "CwJC5Y5rrbs9",
        "outputId": "809f2d46-e199-49a9-f3a9-2c1a791c9094"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffc75b07466945bf92309a1e3ba57213"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38efce1dc63140279a2a1d06e5f4557c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be7392f3711e4720a83accc26ee4b057"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1324.785664 [(torch.float32, 388)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://huggingface.co/facebook/opt-350m/resolve/main/pytorch_model.bin\n",
        "!ls -la pytorch_model.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT6nygEqrnv5",
        "outputId": "98700441-b554-485c-e613-eda9c0a4d27c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 662513657 May 11  2022 pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('pytorch_model.bin')\n",
        "get_parm_dtypes(iter(state_dict.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFr1xCG5ubvw",
        "outputId": "d8f1bf05-84fb-4042-d02b-f7217962e0bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-dab1cbede24c>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('pytorch_model.bin')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(torch.float16, 388)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-350m\", device_map = 'cuda:0', torch_dtype = torch.float32\n",
        ")\n"
      ],
      "metadata": {
        "id": "YBJze_lOvKlv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a8ohNtkvrf4",
        "outputId": "85e4ab1e-83d1-460c-bf5c-0ddc348b4277"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
              "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
              "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x OPTDecoderLayer(\n",
              "          (self_attn): OPTSdpaAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "batch = tokenizer(['This is a simple test'], return_tensors= 'pt')\n",
        "batch['labels'] = batch['input_ids']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "out=model(**batch)\n",
        "out.loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "ae83ee60a3df4f8989d2333fcda92a0b",
            "3b88e74afa544b87bdc1cedcaef136d0",
            "60c1e95b32364105a3befb6097cc5e3b",
            "b126746f863945b19a46f58ffb3043f3",
            "80294eb9885e44e09793fa15f183aa2c",
            "10d04f78cc1c44648ef42c815838e46e",
            "200519b70e094bee975c4adb6897c379",
            "dbefb45ec6ee4a489eb65ce07eb0cb01",
            "4c47f610ab754023babcf4f919d97213",
            "521808c459384d9fbe2e5112479f6a0f",
            "7af04fd064394f6cb7ae1229fc19b46b",
            "fe01f409e97c41df966c907606f7528a",
            "c0e3a88ea2724b9192b94de96dc0cd46",
            "ad0ed64a562349cdaaf3d39ddb6a528d",
            "66f227f50292475eba5ec31b84336504",
            "44324b9c5e55429ca9871624594b0557",
            "cb0dd012533448f6aaa8ee997e724ecd",
            "0a9f31b1c5c2429481d10bdcd3e442da",
            "82c7866363d14e9e98e32c0c07753ff0",
            "fd09a264eec345f187d22595524aa4d0",
            "6087604359754e0593fc926813bb4dea",
            "47d3f791286842e38a179bf95e4adac1",
            "f69a3923f97848bb83a399c1966f2975",
            "0e2db761e000480994b1ea04e11ca6dc",
            "c226f61f17b64ff5899cce942a65eddb",
            "8f9020ce117f4693b7dbdcc8bc6e0d3b",
            "d339253d65ba454a8f0af5968052a84f",
            "3eeb7a193a6c443f9daba93ea2e14aeb",
            "e14984483e774984a3b221dd12a2848c",
            "35a4ccffb1ca4c7dbc82239ee985b60b",
            "193e92f291994916bd74260a6d35467a",
            "82bcfb2ac513481abf89204175ee7584",
            "097437fab2164dada7bd174acd147685",
            "0b1550b6f3504a458507157222817e16",
            "ba14ffc9d1a749e18045ebd9caf3e1ed",
            "49da5217427746908704e99a49dd292e",
            "6408ae90853247eebb73837e197c42c5",
            "9b0b19716ae24c51b0055aa22402994d",
            "67721e49d1714a95aa53022d60c136ac",
            "5b7ca7bea8f54536aa3071ff90588fef",
            "aead6096f5dc4053bb79261fa65aedfb",
            "0d9a1e81b31740719ea8bd13de7dd6ae",
            "7eea2ed4e0514a2c8114ab5fb371718d",
            "3280ddc0dfaa4fddade86a0b8f83622c"
          ]
        },
        "id": "0DgkJ9KdvvWL",
        "outputId": "80c748ee-3cd8-47d5-da57-b30e9d8e6dba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae83ee60a3df4f8989d2333fcda92a0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe01f409e97c41df966c907606f7528a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f69a3923f97848bb83a399c1966f2975"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b1550b6f3504a458507157222817e16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.8001, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieCVEYTowj12",
        "outputId": "ba36be04-deea-4c45-dff4-ab6b87e9ee4d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   2,  713,   16,   10, 2007, 1296]], device='cuda:0'),\n",
              " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
              " 'labels': tensor([[   2,  713,   16,   10, 2007, 1296]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ram5msNwzP9",
        "outputId": "ae5be9c8-57ad-4432-af3d-28c1bc48f471"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('input_ids', tensor([[   2,  713,   16,   10, 2007, 1296]], device='cuda:0')), ('attention_mask', tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')), ('labels', tensor([[   2,  713,   16,   10, 2007, 1296]], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch2 = tokenizer(['This is a simple test'], return_tensors= 'pt')\n",
        "batch2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY-RFwEIzXLx",
        "outputId": "c72e9ae1-c38c-44b1-ff86-cd70ae23066b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   2,  713,   16,   10, 2007, 1296]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = batch.items()\n",
        "\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktaC69ufzcBX",
        "outputId": "b27b700f-75d1-4802-b947-9ccf5d55d46f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('input_ids', tensor([[   2,  713,   16,   10, 2007, 1296]], device='cuda:0')), ('attention_mask', tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')), ('labels', tensor([[   2,  713,   16,   10, 2007, 1296]], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ob3My0gLz-Qa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bX6w5QP1srP",
        "outputId": "b967fbf0-a0b3-4932-aac7-bd97a7a51a3b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.8001, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "supported =torch.cuda.is_bf16_supported(including_emulation=False)\n",
        "dtype16 = (torch.bfloat16 if supported else torch.float16)\n",
        "dtype16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOGwh0ed2BI8",
        "outputId": "eb040b60-0146-4e80-af14-9bf29442d5c4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.bfloat16"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_memory_footprint()/1e6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUBxuRqF2ZaP",
        "outputId": "fe8b2e76-63da-4d67-ced3-fb3f74451a4e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1324.785664"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(torch.bfloat16)\n",
        "print(model.get_memory_footprint()/1e6, get_parm_dtypes(model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HROFEalu2sez",
        "outputId": "4b65d057-f183-4e74-cfd0-001fcd198528"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "662.392832 [(torch.bfloat16, 388)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-350m\", device_map = 'cuda:0'\n",
        ")\n",
        "print(model2.get_memory_footprint()/1e6, get_parm_dtypes(model2.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUSwplxk3SW7",
        "outputId": "48268f6a-2817-42c5-f94d-24d36c04acb3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1324.785664 [(torch.float32, 388)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(**batch)\n",
        "out2 = model2(**batch)\n",
        "out.loss, out2.loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idlqHj093wpI",
        "outputId": "be3b9bd1-de23-4854-d314-a77043c2c47b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(3.8125, device='cuda:0', dtype=torch.bfloat16,\n",
              "        grad_fn=<NllLossBackward0>),\n",
              " tensor(3.8001, device='cuda:0', grad_fn=<NllLossBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mixed precision\n"
      ],
      "metadata": {
        "id": "tUCXYwFR4vr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MixedModel(nn.Module):\n",
        "  def __init__(self, dtype):\n",
        "    super().__init__()\n",
        "    self.a = nn.Linear(1000, 1000, dtype=dtype)\n",
        "    self.b = nn.Linear(1000, 1000, dtype=dtype)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.b(self.a(x))"
      ],
      "metadata": {
        "id": "d8skxaYN4Gno"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixed32 = MixedModel(torch.float32)\n",
        "mixed32.to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb8QTgMv5PgL",
        "outputId": "131ac148-641d-4fdc-bb77-7546936a3b1e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MixedModel(\n",
              "  (a): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (b): Linear(in_features=1000, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit mixed32(torch.randn(1000, 1000, dtype=torch.float32, device='cuda'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJFkHObJ5Z4x",
        "outputId": "dd7b5e01-468a-4312-f521-987824a94eca"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "487 µs ± 7.64 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mixed16 = MixedModel(torch.bfloat16)\n",
        "mixed16.to(\"cuda\")\n",
        "%timeit mixed16(torch.randn(1000, 1000, dtype=torch.bfloat16, device=\"cuda\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STmVnbW65jLK",
        "outputId": "400bb547-0f4a-407a-e98a-d73876700a3c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "121 µs ± 472 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mmixed16 = MixedModel(torch.float16)\n",
        "mmixed16.to(\"cuda\")\n",
        "%timeit mmixed16(torch.randn(1000, 1000, dtype=torch.float16, device=\"cuda\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qj4kLqi6ggk",
        "outputId": "d3e879b6-1826-4b78-829f-69b4b7626d30"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "126 µs ± 198 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using PyTorch autocast Context manager\n"
      ],
      "metadata": {
        "id": "wlUwTxwO7QZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "  %timeit mixed32(torch.randn(1000, 1000, dtype=torch.float32, device=\"cuda\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSdQVIyW6QkH",
        "outputId": "ff0a3292-acb8-470c-f126-392d889d8e8b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "133 µs ± 2.21 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autocast_context = torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
        "# Original forward method\n",
        "model_forward_func = mixed32.forward.__func__\n",
        "# wrapping the method with the new context manager\n",
        "new_forward = autocast_context(model_forward_func)\n",
        "# assign the wrapped method back to the model"
      ],
      "metadata": {
        "id": "Rxzw0d2g7glt"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixed32.forward = MethodType(new_forward, mixed32)"
      ],
      "metadata": {
        "id": "chGlM7Bb-MHk"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = mixed32(torch.randn(1000, 1000, dtype=torch.float32, device=\"cuda\"))\n",
        "res.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO7R14Iq-TxJ",
        "outputId": "cfa573e9-267e-45c0-b6f7-29268d052b35"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.bfloat16"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mixed32.forward =MethodType(convert_outputs_to_fp32(mixed32.forward.__func__), mixed32)"
      ],
      "metadata": {
        "id": "d_WAVWAc_Ffs"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res  =mixed32(torch.randn(1000, 1000, dtype=torch.float32, device=\"cuda\"))\n",
        "res.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy_mXPXFASU9",
        "outputId": "bb6135ce-b942-4bab-acb1-fdf729773bb0"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit mixed32(torch.randn(1000, 1000, dtype=torch.float32, device=\"cuda\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWbE-o-0_r2E",
        "outputId": "bc68ad95-8275-445a-a57c-27e19b588e3e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "246 µs ± 6.74 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sdOQBB0B_vea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BitsAndBytes"
      ],
      "metadata": {
        "id": "XlCXuqBcBJAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig()\n",
        "bnb_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GKsADY3BLyM",
        "outputId": "068aebae-e6fc-41cd-c051-47e5b04392b6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BitsAndBytesConfig {\n",
              "  \"_load_in_4bit\": false,\n",
              "  \"_load_in_8bit\": false,\n",
              "  \"bnb_4bit_compute_dtype\": \"float32\",\n",
              "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
              "  \"bnb_4bit_quant_type\": \"fp4\",\n",
              "  \"bnb_4bit_use_double_quant\": false,\n",
              "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
              "  \"llm_int8_has_fp16_weight\": false,\n",
              "  \"llm_int8_skip_modules\": null,\n",
              "  \"llm_int8_threshold\": 6.0,\n",
              "  \"load_in_4bit\": false,\n",
              "  \"load_in_8bit\": false,\n",
              "  \"quant_method\": \"bitsandbytes\"\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config_q8 =  BitsAndBytesConfig(load_in_8bit=True)\n",
        "model_q8 = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-350m\", device_map= \"cuda:0\", quantization_config =  bnb_config_q8, torch_dtype=torch.float32\n",
        ")\n",
        "\n",
        "print(model_q8.get_memory_footprint()/1e6, get_parm_dtypes(model_q8.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uupAj4QYBauy",
        "outputId": "454340e3-37ad-4789-8722-7c4c2529e4df"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "415.670272 [(torch.float32, 242), (torch.int8, 146)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model_q8(**batch)\n",
        "out.loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WviUGe-WBdJK",
        "outputId": "3fed281d-1c6a-4482-9f5f-2f54fe1d9ba7"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.7965, device='cuda:0', dtype=torch.float32,\n",
              "       grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config_q8 =  BitsAndBytesConfig(load_in_8bit=True)\n",
        "model_q8_32 = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-350m\", device_map= \"cuda:0\", quantization_config =  bnb_config_q8, torch_dtype=torch.float32\n",
        ")"
      ],
      "metadata": {
        "id": "20fVbCxsFLIk"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec_layer = model_q8_32.model.decoder.layers[0]\n",
        "dec_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hny4zk8YEWeT",
        "outputId": "52020152-0bc6-4e44-b22a-a4898af74aeb"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTDecoderLayer(\n",
              "  (self_attn): OPTSdpaAttention(\n",
              "    (k_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
              "    (v_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
              "    (q_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
              "    (out_proj): Linear8bitLt(in_features=1024, out_features=1024, bias=True)\n",
              "  )\n",
              "  (activation_fn): ReLU()\n",
              "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc1): Linear8bitLt(in_features=1024, out_features=4096, bias=True)\n",
              "  (fc2): Linear8bitLt(in_features=4096, out_features=1024, bias=True)\n",
              "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q8_layer= dec_layer.self_attn.k_proj\n",
        "q8_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Iuz9aKCFCWU",
        "outputId": "905e5874-0c4d-4c0b-ea98-a14883766bb6"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear8bitLt(in_features=1024, out_features=1024, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q8_state = q8_layer.state_dict()\n",
        "q8_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URTR_UpQGLbH",
        "outputId": "48129e87-defb-49b7-ca4c-e04f1cc16bdf"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[ -67, -113,  -89,  ...,   65,  -16,  -87],\n",
              "                      [  60,  120,   90,  ...,  -50,   32,   80],\n",
              "                      [  47,  127,   86,  ...,  -34,    8,   90],\n",
              "                      ...,\n",
              "                      [ -65,   65,   34,  ...,  -64,   35,   64],\n",
              "                      [  57,   67,   21,  ...,   63,  -64,  -64],\n",
              "                      [ -64,   63,  -11,  ...,  -64,   34,   63]], device='cuda:0',\n",
              "                     dtype=torch.int8)),\n",
              "             ('bias',\n",
              "              tensor([-0.0134,  0.0082,  0.0161,  ..., -0.0242, -0.0150,  0.0203],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('SCB',\n",
              "              tensor([0.1250, 0.1252, 0.1250,  ..., 0.1252, 0.1250, 0.1254], device='cuda:0',\n",
              "                     dtype=torch.float32)),\n",
              "             ('weight_format', tensor(0, dtype=torch.uint8))])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.model.decoder.embed_tokens)\n",
        "print(model.lm_head)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQTQVgkEGUPQ",
        "outputId": "52c41e03-4707-4957-8b48-8eb710560a9e"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(50272, 512, padding_idx=1)\n",
            "Linear(in_features=512, out_features=50272, bias=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(model.model.decoder.embed_tokens.weight, model.lm_head.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B57No7roGrj6",
        "outputId": "22d1cdf4-989e-4ecb-a26d-9381de8f42b5"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained('facebook/opt-350m')\n",
        "config.tie_word_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuB7nh48G1CN",
        "outputId": "587d9beb-ffa2-418a-8981-f6f07de10b72"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_tied_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBKzOHBAISLn",
        "outputId": "5e303b6e-6c23-4eb3-ee3a-0749c436ef34"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['lm_head.weight', 'model.decoder.embed_tokens.weight']]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with init_empty_weights():\n",
        "  empty_model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "empty_model.lm_head.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_RINElTIeoH",
        "outputId": "3b02464d-65e0-4648-cb63-6a9453d740fa"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor(..., device='meta', size=(50272, 512), requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skip_modules = get_keys_to_not_convert(empty_model)\n",
        "skip_modules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-OYm4lOIy8K",
        "outputId": "e92bf6e3-4314-45ab-8454-35b4486170d4"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.decoder.embed_tokens', 'lm_head']"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_keys_to_not_convert(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE08saXoI-Li",
        "outputId": "4109693a-081c-4f92-d319-2084f2c93936"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.decoder.embed_tokens',\n",
              " 'lm_head',\n",
              " 'model.decoder.layers.23.final_layer_norm']"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for module in skip_modules:\n",
        "  parm = next(model_q8.get_submodule(module).parameters())\n",
        "  print(f\"{module}:{parm.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrGo6hkmJ9yM",
        "outputId": "cfca52a6-73cc-4e4b-9ee8-edb1ec5659cd"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.decoder.embed_tokens:torch.float32\n",
            "lm_head:torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Using own list o modules to skip\n",
        "# This configuration will raise an exception while trying to load weihgts for the tied layer\n",
        "\n",
        "#bnb_config_skip = BitsAndBytesConfig(load_in_8bit = True, llm_int8_skip_modules= ['o_proj'])\n",
        "\n",
        "bnb_config_skip = BitsAndBytesConfig(load_in_8bit=True, llm_int8_skip_modules=['o_proj', 'lm_head'])\n",
        "model_skip = AutoModelForCausalLM.from_pretrained(\n",
        "  \"facebook/opt-350m\", device_map = \"cuda:0\", torch_dtype=torch.float32, quantization_config=bnb_config_skip)"
      ],
      "metadata": {
        "id": "9BWDtCVYMTrg"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_in = 10\n",
        "n_out = 10\n",
        "\n",
        "torch.manual_seed(11)\n",
        "fp_layer = nn.Linear(n_in, n_out)\n",
        "int8_layer = Linear8bitLt(n_in, n_out, has_fp16_weights=False)\n",
        "int8_layer.load_state_dict(fp_layer.state_dict())\n",
        "int8_layer.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHrCOZ6pNbxB",
        "outputId": "379246a7-100a-44e2-b558-f4ec7ba3aa71"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[ 0.2844,  0.2170,  0.0247,  0.0281,  0.1041,  0.2937,  0.0831, -0.2136,\n",
              "                        0.2078, -0.1361],\n",
              "                      [-0.0537,  0.2445, -0.1245,  0.1865, -0.1038,  0.2362,  0.1284,  0.2510,\n",
              "                        0.0729, -0.1195],\n",
              "                      [-0.0383, -0.1476, -0.2729,  0.2769,  0.2600,  0.0114,  0.1547,  0.0714,\n",
              "                        0.1445,  0.0250],\n",
              "                      [ 0.0281, -0.1902, -0.1605, -0.1133,  0.1787,  0.1006, -0.1053,  0.1143,\n",
              "                       -0.2415,  0.2174],\n",
              "                      [ 0.0386,  0.0244,  0.1877,  0.0071,  0.2849, -0.0574,  0.0275,  0.1121,\n",
              "                        0.0426, -0.1801],\n",
              "                      [ 0.0083, -0.0654,  0.0756,  0.0439,  0.0812, -0.1807, -0.2128, -0.0198,\n",
              "                        0.2100, -0.1630],\n",
              "                      [ 0.0874, -0.2396,  0.2269, -0.2751,  0.2140,  0.2590,  0.2130, -0.2461,\n",
              "                        0.2158, -0.3159],\n",
              "                      [ 0.3113,  0.0404,  0.2791, -0.0917,  0.1436,  0.1791,  0.1013, -0.2810,\n",
              "                        0.2075,  0.2612],\n",
              "                      [-0.2312,  0.2844, -0.0429,  0.0278, -0.0278,  0.1367,  0.0843,  0.0133,\n",
              "                       -0.3105, -0.1591],\n",
              "                      [-0.3013,  0.0426, -0.2227, -0.2822, -0.2668,  0.1322,  0.0124, -0.2346,\n",
              "                       -0.1766, -0.0293]])),\n",
              "             ('bias',\n",
              "              tensor([-0.1973, -0.0102,  0.2695,  0.1173,  0.0232, -0.1871,  0.0222, -0.3103,\n",
              "                      -0.2469,  0.0259]))])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int8_layer =int8_layer.to(0)\n",
        "int8_state = int8_layer.state_dict()\n",
        "int8_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09qP3ZlPOczt",
        "outputId": "61798d41-4580-4572-b26a-1fa6f6e7c681"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[ 123,   94,   11,   12,   45,  127,   36,  -92,   90,  -59],\n",
              "                      [ -27,  124,  -63,   94,  -53,  120,   65,  127,   37,  -60],\n",
              "                      [ -18,  -68, -125,  127,  119,    5,   71,   33,   66,   11],\n",
              "                      [  15, -100,  -84,  -60,   94,   53,  -55,   60, -127,  114],\n",
              "                      [  17,   11,   84,    3,  127,  -26,   12,   50,   19,  -80],\n",
              "                      [   5,  -39,   45,   26,   48, -108, -127,  -12,  125,  -97],\n",
              "                      [  35,  -96,   91, -111,   86,  104,   86,  -99,   87, -127],\n",
              "                      [ 127,   16,  114,  -37,   59,   73,   41, -115,   85,  107],\n",
              "                      [ -95,  116,  -18,   11,  -11,   56,   34,    5, -127,  -65],\n",
              "                      [-127,   18,  -94, -119, -112,   56,    5,  -99,  -74,  -12]],\n",
              "                     device='cuda:0', dtype=torch.int8)),\n",
              "             ('bias',\n",
              "              tensor([-0.1973, -0.0102,  0.2695,  0.1173,  0.0232, -0.1871,  0.0222, -0.3103,\n",
              "                      -0.2469,  0.0259], device='cuda:0')),\n",
              "             ('SCB',\n",
              "              tensor([0.2937, 0.2510, 0.2769, 0.2415, 0.2849, 0.2128, 0.3159, 0.3113, 0.3105,\n",
              "                      0.3013], device='cuda:0', dtype=torch.float32)),\n",
              "             ('weight_format', tensor(0, dtype=torch.uint8))])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
        "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type = \"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "m-TJcCYZRIRd"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_q4 = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-350m\", device_map = \"cuda:0\",\n",
        "    quantization_config=nf4_config\n",
        ")\n",
        "\n",
        "print(model_q4.get_memory_footprint()/1e6, get_parm_dtypes(model_q4.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFgW9fOlWPGC",
        "outputId": "fe260954-84e3-4f04-8f4e-ac978672d1bc"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "207.835136 [(torch.float16, 242), (torch.uint8, 146)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model_q4(**batch)\n",
        "out.loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_lDwiSEW0_W",
        "outputId": "ab2eecf5-e622-48a8-a0b7-4f457bf1e4ec"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.4492, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dec_layer = model_q4.model.decoder.layers[0]\n",
        "dec_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiOYiUBtXX-K",
        "outputId": "1219e3e2-efa5-40c9-e0d7-ebc52bf130ac"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTDecoderLayer(\n",
              "  (self_attn): OPTSdpaAttention(\n",
              "    (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "  )\n",
              "  (activation_fn): ReLU()\n",
              "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
              "  (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
              "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q4_layer = dec_layer.self_attn.k_proj\n",
        "q4_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-8BGDcoXg-D",
        "outputId": "dc64695e-31c5-4a46-e803-97046b1720a6"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear4bit(in_features=1024, out_features=1024, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q4_layer.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzuo_8fDXqqZ",
        "outputId": "231941cd-cc06-4c35-aec6-e8b2e6d75dc6"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[ 32],\n",
              "                      [ 29],\n",
              "                      [208],\n",
              "                      ...,\n",
              "                      [ 66],\n",
              "                      [ 34],\n",
              "                      [172]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('bias',\n",
              "              tensor([-0.0134,  0.0082,  0.0161,  ..., -0.0242, -0.0150,  0.0203],\n",
              "                     device='cuda:0')),\n",
              "             ('weight.absmax',\n",
              "              tensor([255, 255,   0,  ...,   0,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('weight.nested_absmax',\n",
              "              tensor([0.0077, 0.0142, 0.0153, 0.0138, 0.0399, 0.0409, 0.0417, 0.0426, 0.0053,\n",
              "                      0.0053, 0.0053, 0.0053, 0.0051, 0.0051, 0.0051, 0.0053, 0.0195, 0.0269,\n",
              "                      0.0223, 0.0195, 0.0053, 0.0053, 0.0054, 0.0054, 0.0317, 0.0315, 0.0306,\n",
              "                      0.0320, 0.0262, 0.0265, 0.0230, 0.0296, 0.0051, 0.0065, 0.0050, 0.0050,\n",
              "                      0.0051, 0.0053, 0.0050, 0.0051, 0.0056, 0.0056, 0.0056, 0.0056, 0.0428,\n",
              "                      0.0410, 0.0405, 0.0422, 0.0397, 0.0402, 0.0394, 0.0414, 0.0204, 0.0106,\n",
              "                      0.0140, 0.0078, 0.0406, 0.0349, 0.0359, 0.0418, 0.0503, 0.0499, 0.0470,\n",
              "                      0.0458], device='cuda:0', dtype=torch.float32)),\n",
              "             ('weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  49,  54,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  49,\n",
              "                       49,  57,  57,  56,  49,  55,  50,  49,  48,  52,  51,  53,  56,  54,\n",
              "                       55,  51, 125], dtype=torch.uint8))])"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FP4 VS NF4 LAYERS"
      ],
      "metadata": {
        "id": "_Egwp3T3YdLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_in  = 10\n",
        "n_out = 10\n",
        "torch.manual_seed(11)\n",
        "fp16_layer = nn.Linear(n_in, n_out)\n",
        "fp16_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7lZikS-Xu1r",
        "outputId": "2da2fcb6-aaa6-4255-e09a-bdd1f7970311"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=10, out_features=10, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp4_layer = LinearFP4(n_in, n_out)\n",
        "fp4_layer.load_state_dict(fp16_layer.state_dict())\n",
        "\n",
        "nf4_model = LinearNF4(n_in, n_out)\n",
        "nf4_model.load_state_dict(fp16_layer.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hay7neoLYp2r",
        "outputId": "f93b6ef7-1cfc-408b-bb97-32b7d6058c4a"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp4_layer = LinearFP4(n_in, n_out)\n",
        "fp4_layer.load_state_dict(fp16_layer.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9ktSYUhZHxt",
        "outputId": "d9be317b-b6ed-4a95-a88a-287f4361641b"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nf4_model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tu4Gl-WZLQ_",
        "outputId": "20860840-9089-4854-bbb8-c95b20435609"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[ 0.2844,  0.2170,  0.0247,  0.0281,  0.1041,  0.2937,  0.0831, -0.2136,\n",
              "                        0.2078, -0.1361],\n",
              "                      [-0.0537,  0.2445, -0.1245,  0.1865, -0.1038,  0.2362,  0.1284,  0.2510,\n",
              "                        0.0729, -0.1195],\n",
              "                      [-0.0383, -0.1476, -0.2729,  0.2769,  0.2600,  0.0114,  0.1547,  0.0714,\n",
              "                        0.1445,  0.0250],\n",
              "                      [ 0.0281, -0.1902, -0.1605, -0.1133,  0.1787,  0.1006, -0.1053,  0.1143,\n",
              "                       -0.2415,  0.2174],\n",
              "                      [ 0.0386,  0.0244,  0.1877,  0.0071,  0.2849, -0.0574,  0.0275,  0.1121,\n",
              "                        0.0426, -0.1801],\n",
              "                      [ 0.0083, -0.0654,  0.0756,  0.0439,  0.0812, -0.1807, -0.2128, -0.0198,\n",
              "                        0.2100, -0.1630],\n",
              "                      [ 0.0874, -0.2396,  0.2269, -0.2751,  0.2140,  0.2590,  0.2130, -0.2461,\n",
              "                        0.2158, -0.3159],\n",
              "                      [ 0.3113,  0.0404,  0.2791, -0.0917,  0.1436,  0.1791,  0.1013, -0.2810,\n",
              "                        0.2075,  0.2612],\n",
              "                      [-0.2312,  0.2844, -0.0429,  0.0278, -0.0278,  0.1367,  0.0843,  0.0133,\n",
              "                       -0.3105, -0.1591],\n",
              "                      [-0.3013,  0.0426, -0.2227, -0.2822, -0.2668,  0.1322,  0.0124, -0.2346,\n",
              "                       -0.1766, -0.0293]])),\n",
              "             ('bias',\n",
              "              tensor([-0.1973, -0.0102,  0.2695,  0.1173,  0.0232, -0.1871,  0.0222, -0.3103,\n",
              "                      -0.2469,  0.0259]))])"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp4_layer = fp4_layer.to(0)\n",
        "fp4_state = fp4_layer.state_dict()\n",
        "fp4_state['weight.quant_map'], fp4_state['weight'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_7cQIdOZRhm",
        "outputId": "7ca86cd7-772d-451f-9eaa-a8a3830e3443"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0.0000,  0.0052,  0.6665,  1.0000,  0.3333,  0.5000,  0.1666,  0.2500,\n",
              "          0.0000, -0.0052, -0.6665, -1.0000, -0.3333, -0.5000, -0.1666, -0.2500],\n",
              "        device='cuda:0'),\n",
              " torch.Size([50, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nf4_model = nf4_model.to(0)\n",
        "nf4_state = nf4_model.state_dict()\n",
        "nf4_state['weight.quant_map'], nf4_state['weight'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD1Pa2GCZt3x",
        "outputId": "6b1984f6-8b7e-426b-d1b9-d105117cf835"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "          0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "        device='cuda:0'),\n",
              " torch.Size([50, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
        "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
        "nf4_config  =BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_quant_type = \"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype = compute_dtype\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-350m\", device_map = \"cuda:0\", torch_dtype=torch.float32,\n",
        "    quantization_config  = nf4_config\n",
        ")"
      ],
      "metadata": {
        "id": "1aGTa9fPaqFh"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyVV47ANbp0J",
        "outputId": "6a3ee160-fb75-44cb-b54a-2edc99bfdd04"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('model.decoder.embed_tokens.weight',\n",
              "              tensor([[-0.0353,  0.0629, -0.0628,  ..., -0.0625,  0.0188,  0.0313],\n",
              "                      [ 0.0213,  0.0379, -0.0625,  ..., -0.0625, -0.0167,  0.0313],\n",
              "                      [-0.0484, -0.0648,  0.0690,  ...,  0.0656, -0.0626, -0.0485],\n",
              "                      ...,\n",
              "                      [ 0.0723,  0.0312, -0.0634,  ..., -0.0625, -0.0053, -0.0755],\n",
              "                      [ 0.0596, -0.0695, -0.0626,  ...,  0.0736, -0.0040,  0.0409],\n",
              "                      [-0.0237,  0.0327, -0.0636,  ..., -0.0625, -0.0248,  0.0315]],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.embed_positions.weight',\n",
              "              tensor([[-0.0066, -0.0121, -0.0097,  ..., -0.0013,  0.0037, -0.0047],\n",
              "                      [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "                      [ 0.0211, -0.0379, -0.0188,  ...,  0.0145,  0.0212,  0.0219],\n",
              "                      ...,\n",
              "                      [-0.0023, -0.0504, -0.0124,  ..., -0.0127,  0.0157, -0.0058],\n",
              "                      [-0.0034, -0.0514, -0.0112,  ..., -0.0151,  0.0123, -0.0059],\n",
              "                      [-0.0015, -0.0507, -0.0100,  ..., -0.0152,  0.0131, -0.0040]],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.project_out.weight',\n",
              "              tensor([[156],\n",
              "                      [ 78],\n",
              "                      [ 17],\n",
              "                      ...,\n",
              "                      [216],\n",
              "                      [169],\n",
              "                      [161]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.project_out.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.project_out.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.project_out.weight.nested_absmax',\n",
              "              tensor([0.0075, 0.0200, 0.0087, 0.0050, 0.0059, 0.0173, 0.0108, 0.0051, 0.0025,\n",
              "                      0.0152, 0.0053, 0.0085, 0.0064, 0.0025, 0.0112, 0.0061, 0.0044, 0.0058,\n",
              "                      0.0047, 0.0085, 0.0054, 0.0150, 0.0103, 0.0065, 0.0061, 0.0033, 0.0121,\n",
              "                      0.0123, 0.0067, 0.0150, 0.0066, 0.0083], device='cuda:0',\n",
              "                     dtype=torch.float32)),\n",
              "             ('model.decoder.project_out.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.project_out.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  53,\n",
              "                       49,  50,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110, 101,\n",
              "                      115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122, 101,\n",
              "                       34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108, 111,\n",
              "                       97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101, 100,\n",
              "                       95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  49,  50,\n",
              "                       53,  50,  53,  56,  51,  50,  54,  53,  51,  48,  52,  53,  54,  53,\n",
              "                       52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.project_in.weight',\n",
              "              tensor([[241],\n",
              "                      [ 90],\n",
              "                      [155],\n",
              "                      ...,\n",
              "                      [ 36],\n",
              "                      [234],\n",
              "                      [ 98]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.project_in.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ..., 255, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.project_in.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.project_in.weight.nested_absmax',\n",
              "              tensor([0.0290, 0.0285, 0.0275, 0.0408, 0.0324, 0.0300, 0.0377, 0.0344, 0.0273,\n",
              "                      0.0328, 0.0282, 0.0289, 0.0367, 0.0308, 0.0237, 0.0310, 0.0324, 0.0327,\n",
              "                      0.0308, 0.0292, 0.0305, 0.0285, 0.0282, 0.0286, 0.0333, 0.0333, 0.0308,\n",
              "                      0.0382, 0.0294, 0.0302, 0.0356, 0.0294], device='cuda:0',\n",
              "                     dtype=torch.float32)),\n",
              "             ('model.decoder.project_in.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.project_in.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  53,  49,  50,  93,  44,  32,  34, 110, 101,\n",
              "                      115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122, 101,\n",
              "                       34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108, 111,\n",
              "                       97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101, 100,\n",
              "                       95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  49,  49,\n",
              "                       51,  56,  51,  49,  56,  55,  55,  55,  48,  56,  52,  51,  53,  48,\n",
              "                       54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn.k_proj.weight',\n",
              "              tensor([[ 32],\n",
              "                      [ 29],\n",
              "                      [208],\n",
              "                      ...,\n",
              "                      [ 66],\n",
              "                      [ 34],\n",
              "                      [172]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn.k_proj.bias',\n",
              "              tensor([-0.0134,  0.0082,  0.0161,  ..., -0.0242, -0.0150,  0.0203],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.self_attn.k_proj.weight.absmax',\n",
              "              tensor([255, 255,   0,  ...,   0,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.0.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0077, 0.0142, 0.0153, 0.0138, 0.0399, 0.0409, 0.0417, 0.0426, 0.0053,\n",
              "                      0.0053, 0.0053, 0.0053, 0.0051, 0.0051, 0.0051, 0.0053, 0.0195, 0.0269,\n",
              "                      0.0223, 0.0195, 0.0053, 0.0053, 0.0054, 0.0054, 0.0317, 0.0315, 0.0306,\n",
              "                      0.0320, 0.0262, 0.0265, 0.0230, 0.0296, 0.0051, 0.0065, 0.0050, 0.0050,\n",
              "                      0.0051, 0.0053, 0.0050, 0.0051, 0.0056, 0.0056, 0.0056, 0.0056, 0.0428,\n",
              "                      0.0410, 0.0405, 0.0422, 0.0397, 0.0402, 0.0394, 0.0414, 0.0204, 0.0106,\n",
              "                      0.0140, 0.0078, 0.0406, 0.0349, 0.0359, 0.0418, 0.0503, 0.0499, 0.0470,\n",
              "                      0.0458], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.0.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  49,\n",
              "                       49,  57,  57,  56,  49,  55,  50,  49,  48,  52,  51,  53,  56,  54,\n",
              "                       55,  51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn.v_proj.weight',\n",
              "              tensor([[167],\n",
              "                      [ 40],\n",
              "                      [200],\n",
              "                      ...,\n",
              "                      [165],\n",
              "                      [202],\n",
              "                      [ 12]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn.v_proj.bias',\n",
              "              tensor([-0.0025, -0.0201,  0.0107,  ...,  0.0066, -0.0204, -0.0107],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.self_attn.v_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.0.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0275, 0.0227, 0.0221, 0.0255, 0.0204, 0.0302, 0.0261, 0.0312, 0.0229,\n",
              "                      0.0275, 0.0225, 0.0247, 0.0196, 0.0190, 0.0207, 0.0242, 0.0205, 0.0199,\n",
              "                      0.0247, 0.0206, 0.0285, 0.0198, 0.0300, 0.0280, 0.0352, 0.0297, 0.0359,\n",
              "                      0.0318, 0.0340, 0.0310, 0.0337, 0.0206, 0.0222, 0.0269, 0.0212, 0.0228,\n",
              "                      0.0299, 0.0238, 0.0346, 0.0288, 0.0352, 0.0297, 0.0319, 0.0271, 0.0349,\n",
              "                      0.0324, 0.0367, 0.0318, 0.0213, 0.0241, 0.0357, 0.0253, 0.0366, 0.0244,\n",
              "                      0.0351, 0.0270, 0.0223, 0.0254, 0.0264, 0.0215, 0.0310, 0.0238, 0.0348,\n",
              "                      0.0394], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.0.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       52,  56,  53,  48,  55,  54,  49,  50,  49,  57,  56,  53,  57,  49,\n",
              "                       50,  51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn.q_proj.weight',\n",
              "              tensor([[ 34],\n",
              "                      [111],\n",
              "                      [162],\n",
              "                      ...,\n",
              "                      [ 42],\n",
              "                      [180],\n",
              "                      [188]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn.q_proj.bias',\n",
              "              tensor([ 0.1249, -0.1249, -0.1249,  ...,  0.1251, -0.1252,  0.1250],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.self_attn.q_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ...,   0, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.0.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0057, 0.0127, 0.0162, 0.0060, 0.0407, 0.0415, 0.0429, 0.0412, 0.0044,\n",
              "                      0.0044, 0.0044, 0.0041, 0.0034, 0.0034, 0.0034, 0.0035, 0.0171, 0.0191,\n",
              "                      0.0177, 0.0152, 0.0091, 0.0096, 0.0033, 0.0034, 0.0099, 0.0108, 0.0274,\n",
              "                      0.0211, 0.0263, 0.0226, 0.0275, 0.0215, 0.0253, 0.0321, 0.0263, 0.0257,\n",
              "                      0.0029, 0.0029, 0.0031, 0.0031, 0.0033, 0.0031, 0.0033, 0.0033, 0.0275,\n",
              "                      0.0227, 0.0252, 0.0264, 0.0372, 0.0353, 0.0357, 0.0409, 0.0209, 0.0168,\n",
              "                      0.0195, 0.0237, 0.0268, 0.0242, 0.0383, 0.0354, 0.0513, 0.0502, 0.0522,\n",
              "                      0.0517], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.0.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  49,\n",
              "                       50,  50,  51,  53,  56,  49,  50,  48,  57,  55,  55,  56,  55,  56,\n",
              "                       53,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn.out_proj.weight',\n",
              "              tensor([[188],\n",
              "                      [214],\n",
              "                      [ 88],\n",
              "                      ...,\n",
              "                      [136],\n",
              "                      [ 38],\n",
              "                      [ 38]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn.out_proj.bias',\n",
              "              tensor([ 0.0058, -0.0029, -0.0008,  ...,  0.0078,  0.0021,  0.0062],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.self_attn.out_proj.weight.absmax',\n",
              "              tensor([255,   0, 255,  ...,   0,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.0.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0325, 0.0359, 0.0355, 0.0579, 0.0372, 0.0629, 0.0942, 0.0878, 0.0944,\n",
              "                      0.0274, 0.0398, 0.0337, 0.0412, 0.0623, 0.0566, 0.0388, 0.0325, 0.0573,\n",
              "                      0.0275, 0.0973, 0.0311, 0.0555, 0.0406, 0.0348, 0.0762, 0.0682, 0.0904,\n",
              "                      0.0840, 0.0468, 0.0453, 0.0306, 0.0385, 0.0372, 0.0475, 0.0581, 0.0376,\n",
              "                      0.0409, 0.0718, 0.0845, 0.0397, 0.0312, 0.0320, 0.0320, 0.0413, 0.0342,\n",
              "                      0.0582, 0.0323, 0.0288, 0.0460, 0.0300, 0.0571, 0.0387, 0.0496, 0.0395,\n",
              "                      0.0336, 0.0315, 0.0343, 0.0961, 0.0333, 0.0405, 0.0302, 0.0334, 0.0428,\n",
              "                      0.0502], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.0.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       51,  50,  49,  55,  52,  52,  51,  48,  55,  56,  55,  53,  54,  51,\n",
              "                       51,  50,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.self_attn_layer_norm.bias',\n",
              "              tensor([ 0.0514, -0.0400,  0.0672,  ..., -0.0154, -0.0246,  0.0627],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.fc1.weight',\n",
              "              tensor([[ 55],\n",
              "                      [ 43],\n",
              "                      [ 77],\n",
              "                      ...,\n",
              "                      [203],\n",
              "                      [157],\n",
              "                      [235]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.fc1.bias',\n",
              "              tensor([-0.0208, -0.0083, -0.0540,  ..., -0.0057, -0.0307, -0.0313],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.fc1.weight.absmax',\n",
              "              tensor([255,   0, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.0.fc1.weight.nested_absmax',\n",
              "              tensor([0.0449, 0.0459, 0.0376, 0.0368, 0.0290, 0.0451, 0.0427, 0.0446, 0.0278,\n",
              "                      0.0372, 0.0359, 0.0460, 0.0353, 0.0460, 0.0410, 0.0443, 0.0387, 0.0459,\n",
              "                      0.0430, 0.0335, 0.0449, 0.0409, 0.0289, 0.0388, 0.0371, 0.0446, 0.0362,\n",
              "                      0.0369, 0.0382, 0.0383, 0.0378, 0.0381, 0.0458, 0.0460, 0.0453, 0.0411,\n",
              "                      0.0368, 0.0433, 0.0332, 0.0421, 0.0415, 0.0396, 0.0355, 0.0354, 0.0460,\n",
              "                      0.0458, 0.0346, 0.0442, 0.0426, 0.0388, 0.0438, 0.0459, 0.0395, 0.0290,\n",
              "                      0.0366, 0.0368, 0.0421, 0.0403, 0.0328, 0.0401, 0.0311, 0.0437, 0.0457,\n",
              "                      0.0416, 0.0458, 0.0321, 0.0403, 0.0344, 0.0460, 0.0391, 0.0455, 0.0363,\n",
              "                      0.0442, 0.0333, 0.0374, 0.0426, 0.0432, 0.0308, 0.0439, 0.0311, 0.0400,\n",
              "                      0.0459, 0.0453, 0.0409, 0.0431, 0.0365, 0.0461, 0.0412, 0.0414, 0.0420,\n",
              "                      0.0456, 0.0299, 0.0386, 0.0412, 0.0413, 0.0362, 0.0446, 0.0386, 0.0440,\n",
              "                      0.0348, 0.0351, 0.0377, 0.0459, 0.0416, 0.0443, 0.0406, 0.0452, 0.0371,\n",
              "                      0.0450, 0.0448, 0.0457, 0.0374, 0.0400, 0.0387, 0.0339, 0.0434, 0.0440,\n",
              "                      0.0362, 0.0435, 0.0341, 0.0393, 0.0348, 0.0407, 0.0414, 0.0386, 0.0346,\n",
              "                      0.0458, 0.0264, 0.0399, 0.0459, 0.0424, 0.0327, 0.0431, 0.0375, 0.0414,\n",
              "                      0.0430, 0.0354, 0.0297, 0.0377, 0.0405, 0.0436, 0.0443, 0.0349, 0.0440,\n",
              "                      0.0387, 0.0398, 0.0304, 0.0459, 0.0366, 0.0403, 0.0455, 0.0424, 0.0428,\n",
              "                      0.0316, 0.0323, 0.0408, 0.0388, 0.0376, 0.0295, 0.0404, 0.0398, 0.0362,\n",
              "                      0.0459, 0.0301, 0.0385, 0.0292, 0.0335, 0.0451, 0.0390, 0.0443, 0.0284,\n",
              "                      0.0455, 0.0332, 0.0376, 0.0429, 0.0370, 0.0375, 0.0431, 0.0347, 0.0337,\n",
              "                      0.0344, 0.0458, 0.0381, 0.0405, 0.0384, 0.0375, 0.0388, 0.0389, 0.0406,\n",
              "                      0.0377, 0.0376, 0.0406, 0.0456, 0.0364, 0.0395, 0.0441, 0.0389, 0.0450,\n",
              "                      0.0352, 0.0413, 0.0421, 0.0409, 0.0304, 0.0377, 0.0434, 0.0336, 0.0411,\n",
              "                      0.0365, 0.0452, 0.0459, 0.0301, 0.0435, 0.0421, 0.0318, 0.0406, 0.0454,\n",
              "                      0.0451, 0.0410, 0.0418, 0.0432, 0.0439, 0.0418, 0.0293, 0.0374, 0.0449,\n",
              "                      0.0416, 0.0427, 0.0381, 0.0388, 0.0389, 0.0374, 0.0357, 0.0415, 0.0343,\n",
              "                      0.0458, 0.0385, 0.0423, 0.0365, 0.0442, 0.0351, 0.0457, 0.0404, 0.0418,\n",
              "                      0.0423, 0.0390, 0.0353, 0.0290, 0.0427, 0.0393, 0.0383, 0.0459, 0.0411,\n",
              "                      0.0385, 0.0395, 0.0316, 0.0399], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.0.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  57,  48,  49,  52,  55,  55,  56,  49,  51,  55,  50,  48,  55,\n",
              "                       48,  51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.fc2.weight',\n",
              "              tensor([[ 35],\n",
              "                      [135],\n",
              "                      [114],\n",
              "                      ...,\n",
              "                      [ 85],\n",
              "                      [107],\n",
              "                      [ 36]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.fc2.bias',\n",
              "              tensor([-0.0129,  0.0251, -0.0123,  ..., -0.0051,  0.0089, -0.0130],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.fc2.weight.absmax',\n",
              "              tensor([255,   0,   0,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.0.fc2.weight.nested_absmax',\n",
              "              tensor([0.0311, 0.0262, 0.0284, 0.0397, 0.0415, 0.0417, 0.0346, 0.0294, 0.0349,\n",
              "                      0.0283, 0.0355, 0.0336, 0.0336, 0.0257, 0.0426, 0.0446, 0.0472, 0.0258,\n",
              "                      0.0341, 0.0274, 0.0424, 0.0388, 0.0472, 0.0480, 0.0775, 0.0361, 0.0377,\n",
              "                      0.0324, 0.0371, 0.0355, 0.0416, 0.0422, 0.0302, 0.0498, 0.0657, 0.0333,\n",
              "                      0.0254, 0.0436, 0.0356, 0.0436, 0.0481, 0.0242, 0.0367, 0.0381, 0.0341,\n",
              "                      0.0341, 0.0353, 0.0421, 0.0377, 0.0384, 0.0417, 0.0363, 0.0508, 0.0319,\n",
              "                      0.0374, 0.0336, 0.0273, 0.0399, 0.0410, 0.0418, 0.0401, 0.0341, 0.0374,\n",
              "                      0.0357, 0.0325, 0.0318, 0.0439, 0.0392, 0.0496, 0.0355, 0.0425, 0.0464,\n",
              "                      0.0411, 0.0349, 0.0316, 0.0320, 0.0618, 0.0333, 0.0478, 0.0390, 0.0308,\n",
              "                      0.0468, 0.0383, 0.0241, 0.0280, 0.0305, 0.0427, 0.0469, 0.0272, 0.0313,\n",
              "                      0.0454, 0.0331, 0.0314, 0.0576, 0.0217, 0.0343, 0.0320, 0.0271, 0.0290,\n",
              "                      0.0462, 0.0351, 0.0520, 0.0340, 0.0324, 0.0289, 0.0313, 0.0328, 0.0574,\n",
              "                      0.0251, 0.0479, 0.0350, 0.0290, 0.0532, 0.0315, 0.0305, 0.0375, 0.0409,\n",
              "                      0.0349, 0.0250, 0.0353, 0.0376, 0.0515, 0.0356, 0.0361, 0.0258, 0.0387,\n",
              "                      0.0345, 0.0356, 0.0433, 0.0286, 0.0420, 0.0465, 0.0253, 0.0399, 0.0444,\n",
              "                      0.0312, 0.0340, 0.0386, 0.0382, 0.0381, 0.0313, 0.0286, 0.0378, 0.0417,\n",
              "                      0.0334, 0.0258, 0.0289, 0.0381, 0.0313, 0.0444, 0.0363, 0.0480, 0.0236,\n",
              "                      0.0461, 0.0264, 0.0262, 0.0367, 0.0356, 0.0328, 0.0257, 0.0271, 0.0302,\n",
              "                      0.0329, 0.0324, 0.0510, 0.0292, 0.0231, 0.0309, 0.0274, 0.0259, 0.0277,\n",
              "                      0.0359, 0.0395, 0.0380, 0.0397, 0.0297, 0.0267, 0.0388, 0.0304, 0.0428,\n",
              "                      0.0478, 0.0316, 0.0483, 0.0291, 0.0339, 0.0332, 0.0295, 0.0281, 0.0364,\n",
              "                      0.0289, 0.0361, 0.0247, 0.0394, 0.0455, 0.0368, 0.0274, 0.0303, 0.0367,\n",
              "                      0.0359, 0.0421, 0.0381, 0.0259, 0.0434, 0.0345, 0.0310, 0.0373, 0.0441,\n",
              "                      0.0400, 0.0444, 0.0344, 0.0347, 0.0480, 0.0320, 0.0432, 0.0235, 0.0478,\n",
              "                      0.0287, 0.0330, 0.0380, 0.0329, 0.0366, 0.0280, 0.0375, 0.0250, 0.0373,\n",
              "                      0.0402, 0.0366, 0.0272, 0.0319, 0.0602, 0.0284, 0.0563, 0.0277, 0.0349,\n",
              "                      0.0291, 0.0265, 0.0402, 0.0325, 0.0250, 0.0334, 0.0432, 0.0351, 0.0345,\n",
              "                      0.0262, 0.0349, 0.0370, 0.0364, 0.0392, 0.0313, 0.0278, 0.0369, 0.0236,\n",
              "                      0.0253, 0.0326, 0.0300, 0.0306], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.0.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  55,  51,  50,  48,  49,  56,  50,  57,  49,  57,  53,  48,  50,\n",
              "                       50,  54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.0.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.0.final_layer_norm.bias',\n",
              "              tensor([ 0.0309, -0.0313,  0.0177,  ...,  0.0244,  0.0309,  0.0582],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.self_attn.k_proj.weight',\n",
              "              tensor([[161],\n",
              "                      [135],\n",
              "                      [212],\n",
              "                      ...,\n",
              "                      [107],\n",
              "                      [150],\n",
              "                      [ 90]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0245,  0.0173,  0.0157,  ...,  0.0260,  0.0021, -0.0274],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.1.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0442, 0.0420, 0.0442, 0.0437, 0.0405, 0.0387, 0.0442, 0.0310, 0.0451,\n",
              "                      0.0451, 0.0464, 0.0457, 0.0441, 0.0442, 0.0442, 0.0440, 0.0450, 0.0453,\n",
              "                      0.0454, 0.0458, 0.0443, 0.0446, 0.0442, 0.0447, 0.0432, 0.0430, 0.0400,\n",
              "                      0.0289, 0.0438, 0.0435, 0.0376, 0.0443, 0.0419, 0.0363, 0.0436, 0.0432,\n",
              "                      0.0446, 0.0448, 0.0443, 0.0446, 0.0375, 0.0373, 0.0421, 0.0368, 0.0343,\n",
              "                      0.0332, 0.0394, 0.0316, 0.0320, 0.0275, 0.0343, 0.0338, 0.0445, 0.0443,\n",
              "                      0.0442, 0.0447, 0.0407, 0.0396, 0.0396, 0.0357, 0.0442, 0.0442, 0.0441,\n",
              "                      0.0445], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.1.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  48,  55,  55,  56,  55,  49,  55,  57,  57,  52,  54,  56,  57,\n",
              "                       57,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.self_attn.v_proj.weight',\n",
              "              tensor([[244],\n",
              "                      [210],\n",
              "                      [134],\n",
              "                      ...,\n",
              "                      [ 92],\n",
              "                      [237],\n",
              "                      [196]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.self_attn.v_proj.bias',\n",
              "              tensor([-0.0072,  0.0117, -0.0260,  ..., -0.0053,  0.0168,  0.0078],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.self_attn.v_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.1.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0359, 0.0319, 0.0260, 0.0365, 0.0475, 0.0375, 0.0319, 0.0340, 0.0292,\n",
              "                      0.0244, 0.0364, 0.0248, 0.0209, 0.0371, 0.0419, 0.0356, 0.0287, 0.0364,\n",
              "                      0.0296, 0.0314, 0.0441, 0.0360, 0.0359, 0.0394, 0.0367, 0.0253, 0.0357,\n",
              "                      0.0459, 0.0289, 0.0344, 0.0303, 0.0384, 0.0285, 0.0298, 0.0295, 0.0340,\n",
              "                      0.0237, 0.0348, 0.0289, 0.0354, 0.0259, 0.0258, 0.0255, 0.0257, 0.0350,\n",
              "                      0.0405, 0.0376, 0.0485, 0.0347, 0.0387, 0.0476, 0.0284, 0.0506, 0.0372,\n",
              "                      0.0326, 0.0307, 0.0244, 0.0265, 0.0246, 0.0242, 0.0240, 0.0298, 0.0327,\n",
              "                      0.0257], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.1.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       53,  56,  53,  52,  50,  56,  55,  55,  52,  51,  53,  54,  56,  52,\n",
              "                       50,  48,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.self_attn.q_proj.weight',\n",
              "              tensor([[212],\n",
              "                      [ 20],\n",
              "                      [164],\n",
              "                      ...,\n",
              "                      [151],\n",
              "                      [ 37],\n",
              "                      [130]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.self_attn.q_proj.bias',\n",
              "              tensor([ 0.0364,  0.0143, -0.0242,  ..., -0.0287,  0.0288, -0.0087],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.self_attn.q_proj.weight.absmax',\n",
              "              tensor([255,   2,   0,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.1.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0415, 0.0344, 0.0446, 0.0320, 0.0458, 0.0457, 0.0419, 0.0456, 0.0459,\n",
              "                      0.0450, 0.0459, 0.0459, 0.0430, 0.0375, 0.0299, 0.0351, 0.0459, 0.0459,\n",
              "                      0.0459, 0.0465, 0.0434, 0.0387, 0.0415, 0.0442, 0.0443, 0.0394, 0.0402,\n",
              "                      0.0291, 0.0452, 0.0453, 0.0457, 0.0457, 0.0462, 0.0457, 0.0459, 0.0421,\n",
              "                      0.0405, 0.0469, 0.0453, 0.0458, 0.0401, 0.0380, 0.0401, 0.0315, 0.0288,\n",
              "                      0.0338, 0.0321, 0.0456, 0.0437, 0.0409, 0.0309, 0.0307, 0.0459, 0.0455,\n",
              "                      0.0454, 0.0459, 0.0396, 0.0422, 0.0412, 0.0413, 0.0420, 0.0432, 0.0404,\n",
              "                      0.0441], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.1.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  57,  48,  55,  48,  55,  57,  57,  48,  53,  50,  55,  49,  53,\n",
              "                       51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.self_attn.out_proj.weight',\n",
              "              tensor([[ 33],\n",
              "                      [165],\n",
              "                      [ 76],\n",
              "                      ...,\n",
              "                      [130],\n",
              "                      [124],\n",
              "                      [ 93]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.self_attn.out_proj.bias',\n",
              "              tensor([ 0.0317,  0.0439,  0.0376,  ...,  0.0451, -0.0313,  0.0346],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.self_attn.out_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.1.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0289, 0.0273, 0.0307, 0.0370, 0.0289, 0.0760, 0.0809, 0.0691, 0.0778,\n",
              "                      0.0255, 0.0258, 0.0405, 0.0344, 0.0453, 0.0307, 0.0405, 0.0333, 0.0479,\n",
              "                      0.0310, 0.0811, 0.0293, 0.0789, 0.0288, 0.0383, 0.0618, 0.0418, 0.0786,\n",
              "                      0.0788, 0.0688, 0.0343, 0.0319, 0.0309, 0.0331, 0.0326, 0.0351, 0.0257,\n",
              "                      0.0293, 0.0539, 0.0789, 0.0234, 0.0295, 0.0360, 0.0343, 0.0349, 0.0263,\n",
              "                      0.0682, 0.0405, 0.0275, 0.0310, 0.0583, 0.0358, 0.0258, 0.0351, 0.0277,\n",
              "                      0.0406, 0.0313, 0.0439, 0.0799, 0.0299, 0.0337, 0.0389, 0.0335, 0.0378,\n",
              "                      0.0378], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.1.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       52,  54,  48,  53,  48,  51,  56,  52,  54,  52,  48,  54,  57,  51,\n",
              "                       54,  54,  53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.self_attn_layer_norm.bias',\n",
              "              tensor([ 0.0321, -0.0413,  0.0502,  ...,  0.0121, -0.0222,  0.0565],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.fc1.weight',\n",
              "              tensor([[ 92],\n",
              "                      [ 49],\n",
              "                      [122],\n",
              "                      ...,\n",
              "                      [ 71],\n",
              "                      [ 94],\n",
              "                      [ 47]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.fc1.bias',\n",
              "              tensor([-0.0209, -0.0236, -0.0279,  ..., -0.0262, -0.0080, -0.0321],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.fc1.weight.absmax',\n",
              "              tensor([255, 255, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.1.fc1.weight.nested_absmax',\n",
              "              tensor([0.0444, 0.0439, 0.0437, 0.0385, 0.0350, 0.0405, 0.0399, 0.0427, 0.0362,\n",
              "                      0.0348, 0.0439, 0.0448, 0.0431, 0.0433, 0.0419, 0.0438, 0.0386, 0.0333,\n",
              "                      0.0291, 0.0373, 0.0407, 0.0358, 0.0363, 0.0352, 0.0375, 0.0411, 0.0438,\n",
              "                      0.0413, 0.0359, 0.0401, 0.0440, 0.0394, 0.0294, 0.0433, 0.0346, 0.0427,\n",
              "                      0.0440, 0.0412, 0.0436, 0.0445, 0.0438, 0.0393, 0.0360, 0.0393, 0.0447,\n",
              "                      0.0440, 0.0431, 0.0386, 0.0333, 0.0434, 0.0442, 0.0401, 0.0414, 0.0433,\n",
              "                      0.0382, 0.0410, 0.0431, 0.0302, 0.0436, 0.0375, 0.0408, 0.0418, 0.0404,\n",
              "                      0.0406, 0.0422, 0.0348, 0.0331, 0.0416, 0.0371, 0.0356, 0.0335, 0.0415,\n",
              "                      0.0444, 0.0442, 0.0403, 0.0437, 0.0324, 0.0413, 0.0408, 0.0427, 0.0349,\n",
              "                      0.0344, 0.0378, 0.0429, 0.0447, 0.0434, 0.0448, 0.0432, 0.0443, 0.0432,\n",
              "                      0.0391, 0.0400, 0.0341, 0.0354, 0.0414, 0.0410, 0.0340, 0.0401, 0.0418,\n",
              "                      0.0447, 0.0402, 0.0445, 0.0418, 0.0437, 0.0438, 0.0333, 0.0382, 0.0405,\n",
              "                      0.0415, 0.0380, 0.0411, 0.0391, 0.0416, 0.0448, 0.0416, 0.0392, 0.0306,\n",
              "                      0.0399, 0.0354, 0.0410, 0.0411, 0.0421, 0.0304, 0.0365, 0.0435, 0.0429,\n",
              "                      0.0371, 0.0414, 0.0414, 0.0275, 0.0355, 0.0404, 0.0443, 0.0447, 0.0441,\n",
              "                      0.0436, 0.0428, 0.0418, 0.0314, 0.0386, 0.0341, 0.0380, 0.0413, 0.0369,\n",
              "                      0.0381, 0.0439, 0.0396, 0.0449, 0.0369, 0.0430, 0.0446, 0.0448, 0.0441,\n",
              "                      0.0448, 0.0419, 0.0418, 0.0397, 0.0309, 0.0438, 0.0378, 0.0436, 0.0440,\n",
              "                      0.0395, 0.0436, 0.0438, 0.0407, 0.0443, 0.0414, 0.0412, 0.0432, 0.0403,\n",
              "                      0.0384, 0.0360, 0.0417, 0.0443, 0.0418, 0.0418, 0.0395, 0.0342, 0.0438,\n",
              "                      0.0423, 0.0427, 0.0445, 0.0437, 0.0431, 0.0380, 0.0432, 0.0405, 0.0435,\n",
              "                      0.0436, 0.0423, 0.0436, 0.0405, 0.0402, 0.0384, 0.0366, 0.0444, 0.0427,\n",
              "                      0.0449, 0.0314, 0.0459, 0.0389, 0.0373, 0.0448, 0.0351, 0.0317, 0.0429,\n",
              "                      0.0350, 0.0441, 0.0330, 0.0435, 0.0437, 0.0366, 0.0416, 0.0452, 0.0360,\n",
              "                      0.0340, 0.0440, 0.0444, 0.0402, 0.0394, 0.0374, 0.0350, 0.0407, 0.0321,\n",
              "                      0.0364, 0.0435, 0.0367, 0.0322, 0.0411, 0.0416, 0.0382, 0.0407, 0.0436,\n",
              "                      0.0441, 0.0430, 0.0438, 0.0386, 0.0396, 0.0444, 0.0355, 0.0367, 0.0388,\n",
              "                      0.0435, 0.0432, 0.0439, 0.0420, 0.0421, 0.0446, 0.0369, 0.0319, 0.0354,\n",
              "                      0.0436, 0.0310, 0.0391, 0.0457], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.1.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  48,  49,  57,  53,  51,  48,  48,  50,  56,  49,  48,  52,  55,\n",
              "                       56,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.fc2.weight',\n",
              "              tensor([[216],\n",
              "                      [221],\n",
              "                      [146],\n",
              "                      ...,\n",
              "                      [132],\n",
              "                      [ 25],\n",
              "                      [ 98]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.fc2.bias',\n",
              "              tensor([-0.0149, -0.0168, -0.0002,  ...,  0.0342, -0.0079, -0.0452],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.fc2.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.1.fc2.weight.nested_absmax',\n",
              "              tensor([0.0388, 0.0301, 0.0259, 0.0290, 0.0312, 0.0333, 0.0259, 0.0360, 0.0323,\n",
              "                      0.0311, 0.0279, 0.0391, 0.0360, 0.0255, 0.0423, 0.0322, 0.0253, 0.0345,\n",
              "                      0.0289, 0.0449, 0.0424, 0.0359, 0.0308, 0.0517, 0.1828, 0.0339, 0.0342,\n",
              "                      0.0397, 0.0316, 0.0282, 0.0277, 0.0578, 0.0346, 0.0577, 0.0649, 0.0295,\n",
              "                      0.0558, 0.0360, 0.0257, 0.0295, 0.0290, 0.0257, 0.0413, 0.0331, 0.0391,\n",
              "                      0.0284, 0.0328, 0.0369, 0.0393, 0.0454, 0.0328, 0.0293, 0.0315, 0.0405,\n",
              "                      0.0291, 0.0295, 0.0335, 0.0244, 0.0468, 0.0361, 0.0317, 0.0308, 0.0290,\n",
              "                      0.0568, 0.0274, 0.0288, 0.0262, 0.0279, 0.0342, 0.0385, 0.0457, 0.0337,\n",
              "                      0.0317, 0.0366, 0.0352, 0.0327, 0.0640, 0.0322, 0.0451, 0.0276, 0.0560,\n",
              "                      0.0422, 0.0435, 0.0467, 0.0230, 0.0289, 0.0556, 0.0312, 0.0243, 0.0320,\n",
              "                      0.0294, 0.0312, 0.0413, 0.0281, 0.0435, 0.0355, 0.0339, 0.0380, 0.0225,\n",
              "                      0.0538, 0.0412, 0.0286, 0.0290, 0.0268, 0.0328, 0.0284, 0.0300, 0.0600,\n",
              "                      0.0253, 0.0579, 0.0322, 0.0395, 0.0282, 0.0325, 0.0359, 0.0322, 0.0449,\n",
              "                      0.0475, 0.0386, 0.0333, 0.0290, 0.0272, 0.0460, 0.0393, 0.0256, 0.0293,\n",
              "                      0.0301, 0.0295, 0.0334, 0.0290, 0.0358, 0.0406, 0.0402, 0.0386, 0.0337,\n",
              "                      0.0306, 0.0335, 0.0363, 0.0347, 0.0362, 0.0386, 0.0295, 0.0300, 0.0372,\n",
              "                      0.0582, 0.0351, 0.0384, 0.0577, 0.0322, 0.0578, 0.0295, 0.0441, 0.0381,\n",
              "                      0.0532, 0.0468, 0.0387, 0.0315, 0.0392, 0.0416, 0.0347, 0.0318, 0.0431,\n",
              "                      0.0309, 0.0342, 0.0357, 0.0419, 0.0312, 0.0306, 0.0252, 0.0347, 0.0439,\n",
              "                      0.0314, 0.0258, 0.0436, 0.0281, 0.0278, 0.0251, 0.0283, 0.0464, 0.0375,\n",
              "                      0.0439, 0.0250, 0.0373, 0.0438, 0.0283, 0.0296, 0.0350, 0.0378, 0.0355,\n",
              "                      0.0251, 0.0250, 0.0299, 0.0300, 0.0275, 0.0349, 0.0563, 0.0342, 0.0339,\n",
              "                      0.0395, 0.0430, 0.0281, 0.0358, 0.0298, 0.0434, 0.0312, 0.0361, 0.0413,\n",
              "                      0.0480, 0.0259, 0.0425, 0.0258, 0.0317, 0.0271, 0.0367, 0.0366, 0.0285,\n",
              "                      0.0336, 0.0353, 0.0265, 0.0334, 0.0294, 0.0311, 0.0359, 0.0390, 0.0384,\n",
              "                      0.0348, 0.0334, 0.0407, 0.0300, 0.0695, 0.0395, 0.0382, 0.0332, 0.0301,\n",
              "                      0.0254, 0.0279, 0.0278, 0.0391, 0.0299, 0.0411, 0.0295, 0.0306, 0.0494,\n",
              "                      0.0393, 0.0355, 0.0290, 0.0375, 0.0449, 0.0389, 0.0418, 0.0420, 0.0291,\n",
              "                      0.0276, 0.0428, 0.0237, 0.0341], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.1.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  55,  50,  48,  54,  50,  57,  51,  51,  52,  52,  52,  57,  55,\n",
              "                       54,  56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.1.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.1.final_layer_norm.bias',\n",
              "              tensor([ 0.0109,  0.0109, -0.0413,  ...,  0.0143,  0.0498, -0.0090],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.self_attn.k_proj.weight',\n",
              "              tensor([[120],\n",
              "                      [172],\n",
              "                      [133],\n",
              "                      ...,\n",
              "                      [101],\n",
              "                      [136],\n",
              "                      [ 81]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0191, -0.0223,  0.0111,  ...,  0.0246, -0.0106,  0.0157],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.2.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0346, 0.0362, 0.0369, 0.0364, 0.0363, 0.0361, 0.0366, 0.0370, 0.0368,\n",
              "                      0.0364, 0.0372, 0.0364, 0.0586, 0.0496, 0.0497, 0.0537, 0.0407, 0.0356,\n",
              "                      0.0348, 0.0332, 0.0363, 0.0370, 0.0378, 0.0359, 0.0351, 0.0361, 0.0304,\n",
              "                      0.0353, 0.0362, 0.0358, 0.0351, 0.0359, 0.0364, 0.0368, 0.0364, 0.0373,\n",
              "                      0.0363, 0.0364, 0.0360, 0.0369, 0.0359, 0.0372, 0.0364, 0.0360, 0.0348,\n",
              "                      0.0361, 0.0359, 0.0340, 0.0364, 0.0361, 0.0367, 0.0363, 0.0363, 0.0363,\n",
              "                      0.0361, 0.0363, 0.0264, 0.0268, 0.0331, 0.0318, 0.0405, 0.0369, 0.0375,\n",
              "                      0.0378], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.2.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  56,  54,  57,  49,  53,  52,  55,  53,  49,  51,  48,  48,  56,\n",
              "                       49,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.self_attn.v_proj.weight',\n",
              "              tensor([[ 69],\n",
              "                      [ 33],\n",
              "                      [ 44],\n",
              "                      ...,\n",
              "                      [148],\n",
              "                      [ 58],\n",
              "                      [ 17]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.self_attn.v_proj.bias',\n",
              "              tensor([-0.0172,  0.0045, -0.0080,  ...,  0.0046, -0.0264,  0.0039],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.self_attn.v_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.2.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0283, 0.0267, 0.0314, 0.0271, 0.0366, 0.0300, 0.0288, 0.0347, 0.0306,\n",
              "                      0.0314, 0.0399, 0.0380, 0.0366, 0.0388, 0.0504, 0.0364, 0.0326, 0.0298,\n",
              "                      0.0278, 0.0305, 0.0388, 0.0303, 0.0391, 0.0407, 0.0383, 0.0387, 0.0324,\n",
              "                      0.0263, 0.0375, 0.0383, 0.0357, 0.0502, 0.0324, 0.0380, 0.0356, 0.0382,\n",
              "                      0.0258, 0.0280, 0.0339, 0.0259, 0.0325, 0.0454, 0.0364, 0.0388, 0.0252,\n",
              "                      0.0405, 0.0264, 0.0308, 0.0485, 0.0303, 0.0215, 0.0334, 0.0325, 0.0431,\n",
              "                      0.0234, 0.0516, 0.0367, 0.0515, 0.0580, 0.0496, 0.0263, 0.0389, 0.0414,\n",
              "                      0.0335], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.2.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  53,  52,  50,  52,  49,  53,  57,  49,  54,  57,  49,  57,  55,\n",
              "                       48,  56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.self_attn.q_proj.weight',\n",
              "              tensor([[212],\n",
              "                      [ 85],\n",
              "                      [202],\n",
              "                      ...,\n",
              "                      [200],\n",
              "                      [ 50],\n",
              "                      [ 41]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.self_attn.q_proj.bias',\n",
              "              tensor([-0.0392, -0.0071,  0.0285,  ...,  0.0598, -0.0205, -0.0034],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.self_attn.q_proj.weight.absmax',\n",
              "              tensor([255,   0,   0,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.2.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0327, 0.0350, 0.0312, 0.0333, 0.0342, 0.0349, 0.0344, 0.0349, 0.0355,\n",
              "                      0.0352, 0.0354, 0.0355, 0.0558, 0.0469, 0.0468, 0.0442, 0.0356, 0.0341,\n",
              "                      0.0337, 0.0301, 0.0351, 0.0353, 0.0346, 0.0346, 0.0341, 0.0338, 0.0346,\n",
              "                      0.0327, 0.0347, 0.0350, 0.0339, 0.0356, 0.0355, 0.0360, 0.0356, 0.0372,\n",
              "                      0.0333, 0.0347, 0.0338, 0.0322, 0.0358, 0.0350, 0.0352, 0.0355, 0.0341,\n",
              "                      0.0334, 0.0366, 0.0313, 0.0350, 0.0353, 0.0354, 0.0352, 0.0352, 0.0359,\n",
              "                      0.0360, 0.0354, 0.0344, 0.0344, 0.0319, 0.0288, 0.0355, 0.0352, 0.0351,\n",
              "                      0.0352], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.2.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  57,  56,  52,  50,  56,  53,  53,  57,  51,  48,  51,  50,  56,\n",
              "                       51,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.self_attn.out_proj.weight',\n",
              "              tensor([[227],\n",
              "                      [ 75],\n",
              "                      [218],\n",
              "                      ...,\n",
              "                      [229],\n",
              "                      [167],\n",
              "                      [ 91]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.self_attn.out_proj.bias',\n",
              "              tensor([ 0.0016,  0.0287,  0.0095,  ...,  0.0321, -0.0299,  0.0061],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.self_attn.out_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.2.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0292, 0.0365, 0.0362, 0.0377, 0.0353, 0.0745, 0.0814, 0.0747, 0.0770,\n",
              "                      0.0319, 0.0261, 0.0263, 0.0410, 0.0316, 0.0404, 0.0332, 0.0312, 0.0346,\n",
              "                      0.0258, 0.0775, 0.0403, 0.0603, 0.0254, 0.0258, 0.0742, 0.0632, 0.0749,\n",
              "                      0.0748, 0.0279, 0.0241, 0.0299, 0.0408, 0.0516, 0.0371, 0.0526, 0.0306,\n",
              "                      0.0272, 0.0409, 0.0469, 0.0311, 0.0253, 0.0415, 0.0342, 0.0266, 0.0276,\n",
              "                      0.0616, 0.0404, 0.0443, 0.0269, 0.0456, 0.0324, 0.0582, 0.0238, 0.0317,\n",
              "                      0.0281, 0.0299, 0.0280, 0.0760, 0.0275, 0.0398, 0.0440, 0.0279, 0.0349,\n",
              "                      0.0289], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.2.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       53,  48,  51,  50,  53,  56,  51,  54,  57,  56,  54,  51,  48,  51,\n",
              "                       51,  51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.self_attn_layer_norm.bias',\n",
              "              tensor([ 0.0063, -0.0246, -0.0050,  ..., -0.0098,  0.0300,  0.0621],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.fc1.weight',\n",
              "              tensor([[ 26],\n",
              "                      [210],\n",
              "                      [231],\n",
              "                      ...,\n",
              "                      [123],\n",
              "                      [ 78],\n",
              "                      [147]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.fc1.bias',\n",
              "              tensor([-0.0278,  0.0035, -0.0086,  ...,  0.0059, -0.0165, -0.0196],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.fc1.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.2.fc1.weight.nested_absmax',\n",
              "              tensor([0.0285, 0.0413, 0.0406, 0.0421, 0.0376, 0.0382, 0.0402, 0.0288, 0.0442,\n",
              "                      0.0373, 0.0432, 0.0354, 0.0444, 0.0446, 0.0355, 0.0436, 0.0351, 0.0429,\n",
              "                      0.0435, 0.0340, 0.0377, 0.0323, 0.0305, 0.0443, 0.0366, 0.0417, 0.0341,\n",
              "                      0.0431, 0.0347, 0.0418, 0.0420, 0.0383, 0.0398, 0.0361, 0.0333, 0.0431,\n",
              "                      0.0377, 0.0415, 0.0356, 0.0421, 0.0442, 0.0373, 0.0308, 0.0377, 0.0321,\n",
              "                      0.0352, 0.0418, 0.0373, 0.0409, 0.0407, 0.0384, 0.0408, 0.0371, 0.0425,\n",
              "                      0.0348, 0.0398, 0.0358, 0.0445, 0.0445, 0.0455, 0.0442, 0.0370, 0.0355,\n",
              "                      0.0322, 0.0447, 0.0372, 0.0375, 0.0434, 0.0430, 0.0328, 0.0435, 0.0439,\n",
              "                      0.0412, 0.0447, 0.0381, 0.0335, 0.0373, 0.0426, 0.0423, 0.0437, 0.0438,\n",
              "                      0.0410, 0.0442, 0.0429, 0.0373, 0.0371, 0.0437, 0.0352, 0.0376, 0.0360,\n",
              "                      0.0442, 0.0356, 0.0399, 0.0359, 0.0342, 0.0388, 0.0388, 0.0423, 0.0433,\n",
              "                      0.0388, 0.0413, 0.0440, 0.0437, 0.0389, 0.0400, 0.0328, 0.0425, 0.0362,\n",
              "                      0.0395, 0.0430, 0.0351, 0.0391, 0.0437, 0.0380, 0.0420, 0.0346, 0.0447,\n",
              "                      0.0384, 0.0420, 0.0415, 0.0379, 0.0447, 0.0377, 0.0349, 0.0443, 0.0429,\n",
              "                      0.0445, 0.0420, 0.0441, 0.0344, 0.0437, 0.0430, 0.0290, 0.0373, 0.0424,\n",
              "                      0.0430, 0.0440, 0.0447, 0.0427, 0.0434, 0.0418, 0.0426, 0.0436, 0.0399,\n",
              "                      0.0390, 0.0354, 0.0440, 0.0429, 0.0327, 0.0439, 0.0378, 0.0384, 0.0446,\n",
              "                      0.0346, 0.0336, 0.0446, 0.0452, 0.0398, 0.0376, 0.0442, 0.0451, 0.0395,\n",
              "                      0.0384, 0.0360, 0.0423, 0.0388, 0.0319, 0.0379, 0.0415, 0.0438, 0.0373,\n",
              "                      0.0399, 0.0382, 0.0344, 0.0434, 0.0435, 0.0452, 0.0324, 0.0376, 0.0378,\n",
              "                      0.0414, 0.0417, 0.0423, 0.0430, 0.0404, 0.0447, 0.0446, 0.0370, 0.0421,\n",
              "                      0.0407, 0.0384, 0.0415, 0.0419, 0.0299, 0.0447, 0.0442, 0.0337, 0.0437,\n",
              "                      0.0407, 0.0444, 0.0415, 0.0337, 0.0316, 0.0420, 0.0406, 0.0435, 0.0421,\n",
              "                      0.0297, 0.0401, 0.0421, 0.0442, 0.0357, 0.0411, 0.0429, 0.0443, 0.0431,\n",
              "                      0.0359, 0.0445, 0.0320, 0.0428, 0.0439, 0.0355, 0.0436, 0.0313, 0.0340,\n",
              "                      0.0313, 0.0341, 0.0334, 0.0385, 0.0405, 0.0399, 0.0440, 0.0445, 0.0445,\n",
              "                      0.0421, 0.0445, 0.0362, 0.0381, 0.0341, 0.0351, 0.0395, 0.0335, 0.0370,\n",
              "                      0.0442, 0.0342, 0.0417, 0.0392, 0.0420, 0.0445, 0.0400, 0.0347, 0.0403,\n",
              "                      0.0399, 0.0405, 0.0429, 0.0327], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.2.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  48,  50,  57,  52,  54,  54,  56,  54,  55,  52,  52,  54,  57,\n",
              "                      125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.fc2.weight',\n",
              "              tensor([[163],\n",
              "                      [129],\n",
              "                      [120],\n",
              "                      ...,\n",
              "                      [ 35],\n",
              "                      [ 87],\n",
              "                      [227]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.fc2.bias',\n",
              "              tensor([-0.0080,  0.0141,  0.0397,  ...,  0.0411, -0.0537, -0.0141],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.fc2.weight.absmax',\n",
              "              tensor([255, 255,   0,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.2.fc2.weight.nested_absmax',\n",
              "              tensor([0.0395, 0.0413, 0.0319, 0.0408, 0.0344, 0.0424, 0.0413, 0.0402, 0.0386,\n",
              "                      0.0344, 0.0232, 0.0366, 0.0335, 0.0334, 0.0469, 0.0397, 0.0248, 0.0367,\n",
              "                      0.0365, 0.0274, 0.0305, 0.0254, 0.0369, 0.0356, 0.1843, 0.0433, 0.0399,\n",
              "                      0.0266, 0.0377, 0.0365, 0.0293, 0.0593, 0.0269, 0.0575, 0.0615, 0.0424,\n",
              "                      0.0408, 0.0309, 0.0505, 0.0493, 0.0362, 0.0430, 0.0415, 0.0313, 0.0289,\n",
              "                      0.0275, 0.0446, 0.0353, 0.0355, 0.0533, 0.0492, 0.0279, 0.0287, 0.0383,\n",
              "                      0.0456, 0.0399, 0.0306, 0.0336, 0.0344, 0.0484, 0.0588, 0.0361, 0.0327,\n",
              "                      0.0514, 0.0300, 0.0380, 0.0364, 0.0439, 0.0427, 0.0290, 0.0498, 0.0279,\n",
              "                      0.0337, 0.0416, 0.0278, 0.0305, 0.0637, 0.0323, 0.0348, 0.0279, 0.0426,\n",
              "                      0.0264, 0.0284, 0.0416, 0.0456, 0.0296, 0.0585, 0.0334, 0.0305, 0.0493,\n",
              "                      0.0325, 0.0505, 0.0274, 0.0329, 0.0308, 0.0480, 0.0258, 0.0264, 0.0590,\n",
              "                      0.0585, 0.0364, 0.0280, 0.0518, 0.0398, 0.0352, 0.0289, 0.0496, 0.0598,\n",
              "                      0.0382, 0.0594, 0.0312, 0.0349, 0.0373, 0.0290, 0.0286, 0.0333, 0.0348,\n",
              "                      0.0509, 0.0254, 0.0364, 0.0387, 0.0345, 0.0342, 0.0372, 0.0385, 0.0383,\n",
              "                      0.0414, 0.0494, 0.0305, 0.0269, 0.0366, 0.0485, 0.0326, 0.0318, 0.0275,\n",
              "                      0.0295, 0.0270, 0.0283, 0.0319, 0.0460, 0.0317, 0.0549, 0.0347, 0.0284,\n",
              "                      0.0596, 0.0242, 0.0378, 0.0566, 0.0315, 0.0592, 0.0237, 0.0336, 0.0369,\n",
              "                      0.0464, 0.0245, 0.0229, 0.0304, 0.0273, 0.0320, 0.0289, 0.0281, 0.0400,\n",
              "                      0.0406, 0.0429, 0.0567, 0.0514, 0.0280, 0.0462, 0.0423, 0.0300, 0.0279,\n",
              "                      0.0261, 0.0404, 0.0374, 0.0341, 0.0358, 0.0373, 0.0294, 0.0298, 0.0451,\n",
              "                      0.0370, 0.0349, 0.0320, 0.0474, 0.0283, 0.0386, 0.0555, 0.0533, 0.0303,\n",
              "                      0.0517, 0.0442, 0.0237, 0.0279, 0.0300, 0.0395, 0.0425, 0.0378, 0.0313,\n",
              "                      0.0374, 0.0486, 0.0312, 0.0394, 0.0516, 0.0276, 0.0249, 0.0340, 0.0336,\n",
              "                      0.0335, 0.0591, 0.0532, 0.0269, 0.0347, 0.0323, 0.0370, 0.0309, 0.0383,\n",
              "                      0.0373, 0.0317, 0.0286, 0.0280, 0.0397, 0.0278, 0.0316, 0.0280, 0.0265,\n",
              "                      0.0245, 0.0381, 0.0341, 0.0317, 0.0637, 0.0334, 0.0274, 0.0391, 0.0322,\n",
              "                      0.0350, 0.0544, 0.0310, 0.0339, 0.0328, 0.0405, 0.0436, 0.0245, 0.0328,\n",
              "                      0.0256, 0.0414, 0.0305, 0.0483, 0.0397, 0.0393, 0.0367, 0.0382, 0.0409,\n",
              "                      0.0392, 0.0562, 0.0271, 0.0500], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.2.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  53,  54,  55,  54,  54,  49,  52,  54,  52,  50,  49,  52,  51,\n",
              "                       50,  53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.2.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.2.final_layer_norm.bias',\n",
              "              tensor([ 0.0191,  0.0262, -0.0424,  ...,  0.0111,  0.0482, -0.0467],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.self_attn.k_proj.weight',\n",
              "              tensor([[152],\n",
              "                      [ 99],\n",
              "                      [133],\n",
              "                      ...,\n",
              "                      [246],\n",
              "                      [214],\n",
              "                      [115]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.self_attn.k_proj.bias',\n",
              "              tensor([-0.0072, -0.0280,  0.0167,  ..., -0.0219, -0.0159, -0.0267],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ..., 255, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.3.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0347, 0.0349, 0.0351, 0.0351, 0.0350, 0.0350, 0.0350, 0.0354, 0.0356,\n",
              "                      0.0350, 0.0351, 0.0359, 0.0523, 0.0523, 0.0515, 0.0465, 0.0348, 0.0332,\n",
              "                      0.0345, 0.0351, 0.0590, 0.0569, 0.0537, 0.0506, 0.0430, 0.0358, 0.0354,\n",
              "                      0.0354, 0.0357, 0.0350, 0.0350, 0.0349, 0.0353, 0.0350, 0.0350, 0.0349,\n",
              "                      0.0341, 0.0335, 0.0351, 0.0349, 0.0342, 0.0352, 0.0348, 0.0357, 0.0344,\n",
              "                      0.0350, 0.0341, 0.0317, 0.0356, 0.0354, 0.0353, 0.0348, 0.0350, 0.0348,\n",
              "                      0.0351, 0.0353, 0.0365, 0.0353, 0.0365, 0.0363, 0.0356, 0.0351, 0.0351,\n",
              "                      0.0337], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.3.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       57,  48,  48,  52,  56,  54,  52,  48,  57,  54,  54,  52,  49,  53,\n",
              "                       52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.self_attn.v_proj.weight',\n",
              "              tensor([[120],\n",
              "                      [170],\n",
              "                      [149],\n",
              "                      ...,\n",
              "                      [ 98],\n",
              "                      [ 82],\n",
              "                      [122]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.self_attn.v_proj.bias',\n",
              "              tensor([-0.0002,  0.0143,  0.0219,  ..., -0.0196, -0.0050, -0.0104],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.self_attn.v_proj.weight.absmax',\n",
              "              tensor([255, 255,   0,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.3.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0360, 0.0517, 0.0331, 0.0325, 0.0262, 0.0453, 0.0403, 0.0276, 0.0243,\n",
              "                      0.0251, 0.0348, 0.0424, 0.0327, 0.0333, 0.0431, 0.0330, 0.0292, 0.0328,\n",
              "                      0.0379, 0.0237, 0.0382, 0.0472, 0.0529, 0.0496, 0.0355, 0.0265, 0.0272,\n",
              "                      0.0331, 0.0274, 0.0235, 0.0257, 0.0252, 0.0283, 0.0384, 0.0275, 0.0389,\n",
              "                      0.0473, 0.0418, 0.0332, 0.0330, 0.0352, 0.0320, 0.0273, 0.0424, 0.0347,\n",
              "                      0.0474, 0.0399, 0.0361, 0.0370, 0.0439, 0.0517, 0.0421, 0.0414, 0.0264,\n",
              "                      0.0332, 0.0493, 0.0414, 0.0219, 0.0381, 0.0373, 0.0407, 0.0453, 0.0361,\n",
              "                      0.0311], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.3.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  56,  55,  50,  56,  54,  57,  50,  56,  50,  57,  54,  48,  56,\n",
              "                       57,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.self_attn.q_proj.weight',\n",
              "              tensor([[ 41],\n",
              "                      [ 22],\n",
              "                      [233],\n",
              "                      ...,\n",
              "                      [ 37],\n",
              "                      [ 84],\n",
              "                      [ 67]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.self_attn.q_proj.bias',\n",
              "              tensor([ 0.0269,  0.0149, -0.0116,  ...,  0.0069,  0.0148,  0.0122],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.self_attn.q_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ..., 255,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.3.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0321, 0.0326, 0.0323, 0.0322, 0.0324, 0.0326, 0.0328, 0.0326, 0.0322,\n",
              "                      0.0323, 0.0322, 0.0338, 0.0484, 0.0584, 0.0481, 0.0475, 0.0335, 0.0323,\n",
              "                      0.0323, 0.0333, 0.0550, 0.0578, 0.0579, 0.0589, 0.0337, 0.0347, 0.0339,\n",
              "                      0.0339, 0.0324, 0.0332, 0.0332, 0.0327, 0.0323, 0.0326, 0.0319, 0.0334,\n",
              "                      0.0332, 0.0324, 0.0324, 0.0321, 0.0335, 0.0332, 0.0323, 0.0319, 0.0323,\n",
              "                      0.0323, 0.0323, 0.0323, 0.0326, 0.0339, 0.0328, 0.0329, 0.0330, 0.0327,\n",
              "                      0.0327, 0.0328, 0.0329, 0.0332, 0.0328, 0.0326, 0.0326, 0.0324, 0.0323,\n",
              "                      0.0329], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.3.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       57,  50,  54,  56,  51,  56,  53,  49,  55,  49,  56,  57,  48,  50,\n",
              "                       53,  57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.self_attn.out_proj.weight',\n",
              "              tensor([[117],\n",
              "                      [104],\n",
              "                      [107],\n",
              "                      ...,\n",
              "                      [151],\n",
              "                      [120],\n",
              "                      [ 21]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.self_attn.out_proj.bias',\n",
              "              tensor([-0.0048,  0.0077,  0.0094,  ...,  0.0139, -0.0196,  0.0010],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.self_attn.out_proj.weight.absmax',\n",
              "              tensor([  0,   3,   0,  ...,   0,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.3.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0328, 0.0622, 0.0336, 0.0292, 0.0294, 0.0706, 0.1224, 0.0709, 0.0702,\n",
              "                      0.0235, 0.0299, 0.0313, 0.0330, 0.0258, 0.0325, 0.0297, 0.0283, 0.0376,\n",
              "                      0.0271, 0.0750, 0.0380, 0.0700, 0.0365, 0.0283, 0.0703, 0.0476, 0.0711,\n",
              "                      0.0694, 0.0427, 0.0291, 0.0297, 0.0286, 0.0296, 0.0242, 0.0355, 0.0343,\n",
              "                      0.0251, 0.0308, 0.0690, 0.0327, 0.0342, 0.0415, 0.0324, 0.0337, 0.0256,\n",
              "                      0.0693, 0.0390, 0.0335, 0.0293, 0.0385, 0.0288, 0.0376, 0.0313, 0.0311,\n",
              "                      0.0302, 0.0289, 0.0302, 0.0722, 0.0323, 0.0338, 0.0271, 0.0510, 0.0256,\n",
              "                      0.0313], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.3.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       53,  52,  48,  48,  49,  50,  57,  48,  51,  53,  49,  49,  53,  50,\n",
              "                       52,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.self_attn_layer_norm.bias',\n",
              "              tensor([ 0.0394,  0.0267, -0.0440,  ..., -0.0150,  0.0453,  0.0198],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.fc1.weight',\n",
              "              tensor([[ 50],\n",
              "                      [132],\n",
              "                      [155],\n",
              "                      ...,\n",
              "                      [123],\n",
              "                      [ 28],\n",
              "                      [ 56]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.fc1.bias',\n",
              "              tensor([-0.0258, -0.0210, -0.0328,  ..., -0.0019, -0.0069, -0.0411],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.fc1.weight.absmax',\n",
              "              tensor([255,   0,   0,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.3.fc1.weight.nested_absmax',\n",
              "              tensor([0.0306, 0.0415, 0.0359, 0.0441, 0.0433, 0.0299, 0.0379, 0.0430, 0.0447,\n",
              "                      0.0388, 0.0423, 0.0449, 0.0426, 0.0292, 0.0365, 0.0445, 0.0448, 0.0366,\n",
              "                      0.0338, 0.0437, 0.0450, 0.0431, 0.0447, 0.0418, 0.0306, 0.0447, 0.0335,\n",
              "                      0.0443, 0.0443, 0.0385, 0.0440, 0.0310, 0.0428, 0.0443, 0.0427, 0.0368,\n",
              "                      0.0410, 0.0343, 0.0361, 0.0374, 0.0377, 0.0391, 0.0325, 0.0386, 0.0435,\n",
              "                      0.0333, 0.0447, 0.0393, 0.0435, 0.0429, 0.0446, 0.0408, 0.0353, 0.0339,\n",
              "                      0.0382, 0.0384, 0.0434, 0.0375, 0.0411, 0.0391, 0.0385, 0.0443, 0.0447,\n",
              "                      0.0438, 0.0439, 0.0440, 0.0352, 0.0419, 0.0302, 0.0442, 0.0449, 0.0400,\n",
              "                      0.0421, 0.0448, 0.0398, 0.0408, 0.0446, 0.0347, 0.0448, 0.0386, 0.0413,\n",
              "                      0.0357, 0.0449, 0.0324, 0.0446, 0.0382, 0.0383, 0.0434, 0.0415, 0.0389,\n",
              "                      0.0427, 0.0443, 0.0435, 0.0330, 0.0343, 0.0426, 0.0433, 0.0299, 0.0450,\n",
              "                      0.0390, 0.0391, 0.0419, 0.0347, 0.0435, 0.0444, 0.0451, 0.0297, 0.0415,\n",
              "                      0.0441, 0.0350, 0.0441, 0.0405, 0.0448, 0.0440, 0.0447, 0.0448, 0.0404,\n",
              "                      0.0391, 0.0416, 0.0440, 0.0380, 0.0420, 0.0435, 0.0421, 0.0410, 0.0372,\n",
              "                      0.0429, 0.0441, 0.0411, 0.0397, 0.0448, 0.0444, 0.0444, 0.0435, 0.0300,\n",
              "                      0.0391, 0.0450, 0.0391, 0.0447, 0.0380, 0.0254, 0.0440, 0.0447, 0.0421,\n",
              "                      0.0388, 0.0346, 0.0433, 0.0399, 0.0432, 0.0437, 0.0383, 0.0346, 0.0407,\n",
              "                      0.0341, 0.0342, 0.0446, 0.0330, 0.0440, 0.0449, 0.0396, 0.0391, 0.0437,\n",
              "                      0.0446, 0.0346, 0.0449, 0.0443, 0.0394, 0.0346, 0.0403, 0.0400, 0.0448,\n",
              "                      0.0418, 0.0381, 0.0414, 0.0394, 0.0316, 0.0446, 0.0381, 0.0394, 0.0384,\n",
              "                      0.0382, 0.0413, 0.0433, 0.0438, 0.0333, 0.0444, 0.0332, 0.0446, 0.0419,\n",
              "                      0.0402, 0.0375, 0.0447, 0.0446, 0.0344, 0.0412, 0.0446, 0.0411, 0.0440,\n",
              "                      0.0341, 0.0409, 0.0437, 0.0447, 0.0440, 0.0308, 0.0437, 0.0422, 0.0438,\n",
              "                      0.0440, 0.0410, 0.0438, 0.0332, 0.0427, 0.0393, 0.0444, 0.0432, 0.0326,\n",
              "                      0.0412, 0.0381, 0.0369, 0.0441, 0.0418, 0.0360, 0.0292, 0.0435, 0.0390,\n",
              "                      0.0447, 0.0441, 0.0412, 0.0436, 0.0408, 0.0435, 0.0438, 0.0380, 0.0449,\n",
              "                      0.0317, 0.0442, 0.0394, 0.0437, 0.0352, 0.0353, 0.0435, 0.0418, 0.0314,\n",
              "                      0.0429, 0.0356, 0.0446, 0.0404, 0.0428, 0.0448, 0.0429, 0.0393, 0.0438,\n",
              "                      0.0382, 0.0426, 0.0408, 0.0338], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.3.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  48,  49,  50,  56,  57,  51,  55,  57,  53,  57,  54,  55,  49,\n",
              "                       48,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.fc2.weight',\n",
              "              tensor([[122],\n",
              "                      [231],\n",
              "                      [202],\n",
              "                      ...,\n",
              "                      [167],\n",
              "                      [ 64],\n",
              "                      [149]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.fc2.bias',\n",
              "              tensor([-0.0033,  0.0187,  0.0197,  ...,  0.0247, -0.0625,  0.0046],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.fc2.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.3.fc2.weight.nested_absmax',\n",
              "              tensor([0.0589, 0.0328, 0.0257, 0.0429, 0.0402, 0.0384, 0.0596, 0.0319, 0.0458,\n",
              "                      0.0374, 0.0311, 0.0412, 0.0350, 0.0382, 0.0415, 0.0571, 0.0222, 0.0422,\n",
              "                      0.0466, 0.0398, 0.0597, 0.0368, 0.0457, 0.0534, 0.1847, 0.0502, 0.0457,\n",
              "                      0.0281, 0.0446, 0.0244, 0.0314, 0.0595, 0.0580, 0.0511, 0.0446, 0.0321,\n",
              "                      0.0314, 0.0300, 0.0303, 0.0432, 0.0521, 0.0402, 0.0405, 0.0593, 0.0398,\n",
              "                      0.0289, 0.0238, 0.0378, 0.0382, 0.0413, 0.0430, 0.0530, 0.0231, 0.0364,\n",
              "                      0.0590, 0.0368, 0.0378, 0.0327, 0.0487, 0.0261, 0.0297, 0.0269, 0.0448,\n",
              "                      0.0380, 0.0305, 0.0585, 0.0415, 0.0446, 0.0418, 0.0280, 0.0340, 0.0306,\n",
              "                      0.0414, 0.0380, 0.0330, 0.0374, 0.0623, 0.0250, 0.0313, 0.0532, 0.0330,\n",
              "                      0.0285, 0.0568, 0.0399, 0.0578, 0.0499, 0.0596, 0.0302, 0.0233, 0.0519,\n",
              "                      0.0351, 0.0461, 0.0419, 0.0315, 0.0283, 0.0329, 0.0512, 0.0281, 0.0458,\n",
              "                      0.0596, 0.0565, 0.0349, 0.0585, 0.0297, 0.0457, 0.0473, 0.0282, 0.0597,\n",
              "                      0.0358, 0.0596, 0.0419, 0.0318, 0.0270, 0.0570, 0.0346, 0.0255, 0.0309,\n",
              "                      0.0430, 0.0361, 0.0366, 0.0326, 0.0366, 0.0559, 0.0357, 0.0314, 0.0369,\n",
              "                      0.0461, 0.0382, 0.0296, 0.0415, 0.0438, 0.0343, 0.0254, 0.0597, 0.0457,\n",
              "                      0.0454, 0.0546, 0.0435, 0.0569, 0.0381, 0.0416, 0.0373, 0.0355, 0.0314,\n",
              "                      0.0592, 0.0364, 0.0260, 0.0462, 0.0457, 0.0597, 0.0377, 0.0462, 0.0305,\n",
              "                      0.0519, 0.0383, 0.0314, 0.0253, 0.0513, 0.0457, 0.0450, 0.0339, 0.0479,\n",
              "                      0.0306, 0.0326, 0.0396, 0.0461, 0.0317, 0.0281, 0.0322, 0.0382, 0.0498,\n",
              "                      0.0347, 0.0336, 0.0566, 0.0432, 0.0306, 0.0376, 0.0507, 0.0455, 0.0435,\n",
              "                      0.0446, 0.0281, 0.0366, 0.0377, 0.0357, 0.0389, 0.0429, 0.0404, 0.0329,\n",
              "                      0.0514, 0.0250, 0.0225, 0.0280, 0.0447, 0.0278, 0.0416, 0.0275, 0.0468,\n",
              "                      0.0290, 0.0254, 0.0342, 0.0390, 0.0567, 0.0427, 0.0524, 0.0319, 0.0367,\n",
              "                      0.0423, 0.0471, 0.0270, 0.0349, 0.0265, 0.0371, 0.0243, 0.0328, 0.0368,\n",
              "                      0.0313, 0.0384, 0.0359, 0.0595, 0.0287, 0.0534, 0.0452, 0.0306, 0.0342,\n",
              "                      0.0350, 0.0355, 0.0341, 0.0242, 0.0626, 0.0433, 0.0587, 0.0265, 0.0392,\n",
              "                      0.0313, 0.0556, 0.0336, 0.0549, 0.0283, 0.0594, 0.0402, 0.0327, 0.0452,\n",
              "                      0.0475, 0.0411, 0.0559, 0.0441, 0.0532, 0.0494, 0.0259, 0.0377, 0.0526,\n",
              "                      0.0239, 0.0531, 0.0446, 0.0402], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.3.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  53,  50,  57,  55,  57,  49,  54,  53,  51,  49,  53,  54,  50,\n",
              "                       56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.3.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.3.final_layer_norm.bias',\n",
              "              tensor([-0.0075, -0.0115,  0.0282,  ...,  0.0114,  0.0235, -0.0290],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.self_attn.k_proj.weight',\n",
              "              tensor([[139],\n",
              "                      [ 31],\n",
              "                      [238],\n",
              "                      ...,\n",
              "                      [ 23],\n",
              "                      [241],\n",
              "                      [136]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0154,  0.0165,  0.0196,  ...,  0.0016, -0.0156, -0.0213],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ..., 255, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.4.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0319, 0.0339, 0.0344, 0.0333, 0.0345, 0.0356, 0.0360, 0.0353, 0.0610,\n",
              "                      0.0613, 0.0577, 0.0620, 0.0344, 0.0298, 0.0348, 0.0344, 0.0351, 0.0392,\n",
              "                      0.0356, 0.0372, 0.0341, 0.0344, 0.0330, 0.0337, 0.0345, 0.0362, 0.0347,\n",
              "                      0.0345, 0.0337, 0.0335, 0.0330, 0.0344, 0.0407, 0.0422, 0.0413, 0.0365,\n",
              "                      0.0336, 0.0351, 0.0344, 0.0342, 0.0347, 0.0348, 0.0348, 0.0348, 0.0334,\n",
              "                      0.0344, 0.0344, 0.0341, 0.0344, 0.0347, 0.0342, 0.0342, 0.0337, 0.0340,\n",
              "                      0.0343, 0.0333, 0.0367, 0.0359, 0.0362, 0.0356, 0.0343, 0.0344, 0.0339,\n",
              "                      0.0344], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.4.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       57,  48,  53,  56,  51,  50,  57,  52,  54,  51,  48,  48,  53,  48,\n",
              "                       54,  54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.self_attn.v_proj.weight',\n",
              "              tensor([[ 88],\n",
              "                      [134],\n",
              "                      [117],\n",
              "                      ...,\n",
              "                      [ 51],\n",
              "                      [145],\n",
              "                      [ 30]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.self_attn.v_proj.bias',\n",
              "              tensor([ 0.0264, -0.0072, -0.0052,  ...,  0.0100, -0.0011, -0.0154],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.self_attn.v_proj.weight.absmax',\n",
              "              tensor([255, 255,   0,  ..., 255,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.4.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0265, 0.0270, 0.0224, 0.0297, 0.0284, 0.0270, 0.0274, 0.0400, 0.0408,\n",
              "                      0.0439, 0.0427, 0.0339, 0.0292, 0.0295, 0.0462, 0.0283, 0.0273, 0.0426,\n",
              "                      0.0261, 0.0331, 0.0421, 0.0417, 0.0391, 0.0433, 0.0544, 0.0387, 0.0338,\n",
              "                      0.0368, 0.0323, 0.0340, 0.0356, 0.0354, 0.0468, 0.0461, 0.0477, 0.0408,\n",
              "                      0.0382, 0.0354, 0.0317, 0.0285, 0.0277, 0.0320, 0.0306, 0.0262, 0.0276,\n",
              "                      0.0384, 0.0285, 0.0221, 0.0354, 0.0370, 0.0345, 0.0350, 0.0373, 0.0339,\n",
              "                      0.0381, 0.0309, 0.0287, 0.0348, 0.0299, 0.0277, 0.0377, 0.0360, 0.0565,\n",
              "                      0.0430], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.4.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  54,  49,  56,  51,  55,  48,  56,  54,  48,  56,  49,  53,  48,\n",
              "                       52,  56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.self_attn.q_proj.weight',\n",
              "              tensor([[ 82],\n",
              "                      [183],\n",
              "                      [ 26],\n",
              "                      ...,\n",
              "                      [170],\n",
              "                      [173],\n",
              "                      [214]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.self_attn.q_proj.bias',\n",
              "              tensor([-0.0218,  0.0070, -0.0100,  ..., -0.0032, -0.0252,  0.0217],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.self_attn.q_proj.weight.absmax',\n",
              "              tensor([255,   0, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.4.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0322, 0.0299, 0.0322, 0.0357, 0.0344, 0.0331, 0.0333, 0.0328, 0.0559,\n",
              "                      0.0561, 0.0589, 0.0570, 0.0325, 0.0351, 0.0333, 0.0326, 0.0318, 0.0404,\n",
              "                      0.0404, 0.0339, 0.0333, 0.0324, 0.0323, 0.0335, 0.0328, 0.0331, 0.0323,\n",
              "                      0.0323, 0.0314, 0.0329, 0.0306, 0.0331, 0.0429, 0.0431, 0.0434, 0.0415,\n",
              "                      0.0325, 0.0329, 0.0328, 0.0328, 0.0332, 0.0348, 0.0333, 0.0336, 0.0327,\n",
              "                      0.0324, 0.0327, 0.0331, 0.0329, 0.0329, 0.0327, 0.0342, 0.0323, 0.0322,\n",
              "                      0.0332, 0.0345, 0.0323, 0.0325, 0.0323, 0.0325, 0.0325, 0.0328, 0.0327,\n",
              "                      0.0337], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.4.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       57,  50,  53,  51,  57,  50,  54,  53,  55,  53,  49,  56,  51,  56,\n",
              "                       54,  56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.self_attn.out_proj.weight',\n",
              "              tensor([[ 86],\n",
              "                      [139],\n",
              "                      [216],\n",
              "                      ...,\n",
              "                      [138],\n",
              "                      [147],\n",
              "                      [133]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.self_attn.out_proj.bias',\n",
              "              tensor([-0.0359,  0.0588,  0.0111,  ...,  0.0470, -0.0100, -0.0010],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.self_attn.out_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.4.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0271, 0.0418, 0.0266, 0.0326, 0.0269, 0.0623, 0.0774, 0.0713, 0.0718,\n",
              "                      0.0256, 0.0312, 0.0286, 0.0233, 0.0295, 0.0324, 0.0256, 0.0288, 0.0493,\n",
              "                      0.0388, 0.0746, 0.0277, 0.0682, 0.0374, 0.0268, 0.0716, 0.0625, 0.0714,\n",
              "                      0.0714, 0.0438, 0.0379, 0.0334, 0.0228, 0.0268, 0.0401, 0.0323, 0.0266,\n",
              "                      0.0382, 0.0360, 0.0551, 0.0326, 0.0406, 0.0321, 0.0359, 0.0355, 0.0278,\n",
              "                      0.0692, 0.0295, 0.0300, 0.0317, 0.0402, 0.0234, 0.0379, 0.0246, 0.0305,\n",
              "                      0.0301, 0.0390, 0.0344, 0.0729, 0.0307, 0.0345, 0.0308, 0.0298, 0.0261,\n",
              "                      0.0375], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.4.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       53,  51,  51,  49,  48,  57,  51,  48,  55,  50,  56,  57,  49,  50,\n",
              "                       51,  53,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.self_attn_layer_norm.bias',\n",
              "              tensor([ 0.0244,  0.0327, -0.0288,  ...,  0.0115,  0.0628, -0.0033],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.fc1.weight',\n",
              "              tensor([[150],\n",
              "                      [ 98],\n",
              "                      [153],\n",
              "                      ...,\n",
              "                      [100],\n",
              "                      [ 37],\n",
              "                      [179]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.fc1.bias',\n",
              "              tensor([-0.0144, -0.0431, -0.0185,  ..., -0.0167, -0.0010, -0.0298],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.fc1.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ..., 255,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.4.fc1.weight.nested_absmax',\n",
              "              tensor([0.0353, 0.0414, 0.0387, 0.0358, 0.0405, 0.0448, 0.0451, 0.0357, 0.0434,\n",
              "                      0.0352, 0.0382, 0.0321, 0.0266, 0.0454, 0.0440, 0.0401, 0.0450, 0.0446,\n",
              "                      0.0342, 0.0342, 0.0445, 0.0446, 0.0449, 0.0384, 0.0406, 0.0326, 0.0425,\n",
              "                      0.0379, 0.0393, 0.0304, 0.0436, 0.0378, 0.0525, 0.0426, 0.0397, 0.0451,\n",
              "                      0.0399, 0.0443, 0.0444, 0.0398, 0.0326, 0.0440, 0.0449, 0.0415, 0.0329,\n",
              "                      0.0278, 0.0445, 0.0450, 0.0439, 0.0335, 0.0337, 0.0449, 0.0378, 0.0437,\n",
              "                      0.0437, 0.0348, 0.0440, 0.0426, 0.0360, 0.0449, 0.0343, 0.0388, 0.0379,\n",
              "                      0.0397, 0.0379, 0.0382, 0.0440, 0.0421, 0.0298, 0.0390, 0.0448, 0.0437,\n",
              "                      0.0419, 0.0406, 0.0408, 0.0356, 0.0438, 0.0418, 0.0443, 0.0393, 0.0389,\n",
              "                      0.0365, 0.0535, 0.0422, 0.0422, 0.0449, 0.0449, 0.0320, 0.0425, 0.0437,\n",
              "                      0.0412, 0.0442, 0.0423, 0.0384, 0.0435, 0.0350, 0.0409, 0.0436, 0.0406,\n",
              "                      0.0408, 0.0423, 0.0353, 0.0371, 0.0423, 0.0445, 0.0445, 0.0460, 0.0375,\n",
              "                      0.0429, 0.0316, 0.0364, 0.0438, 0.0423, 0.0402, 0.0389, 0.0417, 0.0415,\n",
              "                      0.0431, 0.0390, 0.0387, 0.0361, 0.0447, 0.0440, 0.0445, 0.0406, 0.0437,\n",
              "                      0.0367, 0.0392, 0.0431, 0.0334, 0.0440, 0.0273, 0.0378, 0.0418, 0.0407,\n",
              "                      0.0370, 0.0373, 0.0449, 0.0348, 0.0425, 0.0398, 0.0425, 0.0412, 0.0387,\n",
              "                      0.0450, 0.0350, 0.0397, 0.0427, 0.0428, 0.0449, 0.0445, 0.0352, 0.0397,\n",
              "                      0.0436, 0.0387, 0.0439, 0.0410, 0.0420, 0.0431, 0.0434, 0.0375, 0.0412,\n",
              "                      0.0343, 0.0431, 0.0383, 0.0319, 0.0474, 0.0417, 0.0448, 0.0448, 0.0449,\n",
              "                      0.0398, 0.0412, 0.0390, 0.0440, 0.0312, 0.0393, 0.0375, 0.0430, 0.0394,\n",
              "                      0.0406, 0.0439, 0.0360, 0.0405, 0.0324, 0.0354, 0.0448, 0.0379, 0.0442,\n",
              "                      0.0425, 0.0447, 0.0415, 0.0447, 0.0450, 0.0447, 0.0438, 0.0441, 0.0447,\n",
              "                      0.0418, 0.0414, 0.0449, 0.0389, 0.0375, 0.0336, 0.0449, 0.0400, 0.0436,\n",
              "                      0.0403, 0.0447, 0.0445, 0.0375, 0.0385, 0.0410, 0.0454, 0.0450, 0.0380,\n",
              "                      0.0352, 0.0384, 0.0400, 0.0387, 0.0414, 0.0362, 0.0437, 0.0437, 0.0404,\n",
              "                      0.0330, 0.0368, 0.0260, 0.0343, 0.0439, 0.0388, 0.0442, 0.0424, 0.0367,\n",
              "                      0.0433, 0.0354, 0.0368, 0.0430, 0.0450, 0.0439, 0.0392, 0.0326, 0.0434,\n",
              "                      0.0447, 0.0343, 0.0440, 0.0401, 0.0423, 0.0450, 0.0428, 0.0395, 0.0442,\n",
              "                      0.0439, 0.0407, 0.0428, 0.0395], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.4.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  48,  49,  48,  48,  49,  55,  56,  55,  49,  56,  53,  54,  54,\n",
              "                       57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.fc2.weight',\n",
              "              tensor([[195],\n",
              "                      [103],\n",
              "                      [198],\n",
              "                      ...,\n",
              "                      [ 49],\n",
              "                      [137],\n",
              "                      [  1]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.fc2.bias',\n",
              "              tensor([-0.0262,  0.0229, -0.0200,  ..., -0.0072, -0.0634,  0.0159],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.fc2.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.4.fc2.weight.nested_absmax',\n",
              "              tensor([0.0592, 0.0438, 0.0253, 0.0437, 0.0365, 0.0517, 0.0422, 0.0362, 0.0292,\n",
              "                      0.0222, 0.0281, 0.0431, 0.0351, 0.0517, 0.0293, 0.0431, 0.0447, 0.0413,\n",
              "                      0.0518, 0.0291, 0.0599, 0.0492, 0.0360, 0.0459, 0.1851, 0.0413, 0.0443,\n",
              "                      0.0303, 0.0353, 0.0316, 0.0596, 0.0601, 0.0400, 0.0601, 0.0599, 0.0439,\n",
              "                      0.0318, 0.0406, 0.0483, 0.0330, 0.0407, 0.0372, 0.0351, 0.0357, 0.0418,\n",
              "                      0.0276, 0.0326, 0.0361, 0.0398, 0.0273, 0.0459, 0.0359, 0.0329, 0.0327,\n",
              "                      0.0328, 0.0411, 0.0400, 0.0283, 0.0340, 0.0600, 0.0548, 0.0306, 0.0567,\n",
              "                      0.0473, 0.0283, 0.0403, 0.0376, 0.0428, 0.0326, 0.0354, 0.0602, 0.0601,\n",
              "                      0.0377, 0.0304, 0.0411, 0.0302, 0.0631, 0.0382, 0.0601, 0.0590, 0.0353,\n",
              "                      0.0354, 0.0346, 0.0366, 0.0433, 0.0599, 0.0600, 0.0331, 0.0263, 0.0337,\n",
              "                      0.0309, 0.0327, 0.0538, 0.0354, 0.0396, 0.0309, 0.0356, 0.0312, 0.0334,\n",
              "                      0.0601, 0.0298, 0.0600, 0.0279, 0.0310, 0.0340, 0.0325, 0.0381, 0.0603,\n",
              "                      0.0267, 0.0600, 0.0338, 0.0280, 0.0489, 0.0523, 0.0600, 0.0600, 0.0395,\n",
              "                      0.0418, 0.0410, 0.0249, 0.0417, 0.0432, 0.0428, 0.0342, 0.0333, 0.0481,\n",
              "                      0.0517, 0.0536, 0.0594, 0.0434, 0.0382, 0.0324, 0.0481, 0.0349, 0.0384,\n",
              "                      0.0299, 0.0597, 0.0538, 0.0334, 0.0584, 0.0426, 0.0403, 0.0514, 0.0267,\n",
              "                      0.0481, 0.0260, 0.0357, 0.0321, 0.0491, 0.0532, 0.0382, 0.0341, 0.0301,\n",
              "                      0.0601, 0.0489, 0.0324, 0.0364, 0.0445, 0.0383, 0.0447, 0.0444, 0.0322,\n",
              "                      0.0306, 0.0350, 0.0315, 0.0378, 0.0373, 0.0299, 0.0301, 0.0345, 0.0386,\n",
              "                      0.0367, 0.0286, 0.0361, 0.0467, 0.0444, 0.0593, 0.0342, 0.0415, 0.0385,\n",
              "                      0.0601, 0.0269, 0.0294, 0.0401, 0.0357, 0.0411, 0.0296, 0.0326, 0.0465,\n",
              "                      0.0327, 0.0345, 0.0257, 0.0298, 0.0310, 0.0582, 0.0431, 0.0312, 0.0456,\n",
              "                      0.0595, 0.0354, 0.0429, 0.0458, 0.0411, 0.0539, 0.0490, 0.0508, 0.0379,\n",
              "                      0.0528, 0.0424, 0.0286, 0.0437, 0.0342, 0.0345, 0.0293, 0.0358, 0.0569,\n",
              "                      0.0424, 0.0248, 0.0236, 0.0601, 0.0322, 0.0422, 0.0339, 0.0379, 0.0257,\n",
              "                      0.0578, 0.0429, 0.0383, 0.0229, 0.0660, 0.0305, 0.0601, 0.0399, 0.0386,\n",
              "                      0.0433, 0.0277, 0.0287, 0.0291, 0.0299, 0.0533, 0.0345, 0.0326, 0.0525,\n",
              "                      0.0274, 0.0326, 0.0511, 0.0245, 0.0600, 0.0426, 0.0364, 0.0353, 0.0321,\n",
              "                      0.0304, 0.0299, 0.0298, 0.0348], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.4.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  52,  57,  48,  49,  56,  55,  51,  52,  54,  57,  51,  53,  50,\n",
              "                       55,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.4.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.4.final_layer_norm.bias',\n",
              "              tensor([-0.0226, -0.0022,  0.0303,  ..., -0.0098, -0.0262, -0.0278],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.self_attn.k_proj.weight',\n",
              "              tensor([[72],\n",
              "                      [91],\n",
              "                      [87],\n",
              "                      ...,\n",
              "                      [73],\n",
              "                      [20],\n",
              "                      [31]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0151, -0.0247,  0.0421,  ..., -0.0254, -0.0141, -0.0178],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.5.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0409, 0.0537, 0.0449, 0.0389, 0.0456, 0.0457, 0.0456, 0.0454, 0.0518,\n",
              "                      0.0509, 0.0509, 0.0482, 0.0450, 0.0431, 0.0422, 0.0451, 0.0447, 0.0451,\n",
              "                      0.0451, 0.0451, 0.0514, 0.0531, 0.0516, 0.0463, 0.0452, 0.0451, 0.0448,\n",
              "                      0.0442, 0.0422, 0.0453, 0.0431, 0.0448, 0.0363, 0.0386, 0.0413, 0.0350,\n",
              "                      0.0463, 0.0464, 0.0462, 0.0464, 0.0511, 0.0484, 0.0450, 0.0489, 0.0456,\n",
              "                      0.0450, 0.0451, 0.0453, 0.0452, 0.0450, 0.0451, 0.0452, 0.0385, 0.0402,\n",
              "                      0.0422, 0.0439, 0.0448, 0.0427, 0.0449, 0.0414, 0.0454, 0.0457, 0.0448,\n",
              "                      0.0457], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.5.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  57,  57,  50,  50,  48,  53,  48,  50,  51,  55,  54,  53,  53,\n",
              "                       54,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.self_attn.v_proj.weight',\n",
              "              tensor([[211],\n",
              "                      [150],\n",
              "                      [167],\n",
              "                      ...,\n",
              "                      [123],\n",
              "                      [ 20],\n",
              "                      [ 63]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.self_attn.v_proj.bias',\n",
              "              tensor([-0.0133,  0.0066,  0.0013,  ...,  0.0153, -0.0039, -0.0260],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.self_attn.v_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.5.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0394, 0.0354, 0.0368, 0.0269, 0.0335, 0.0265, 0.0556, 0.0404, 0.0299,\n",
              "                      0.0329, 0.0385, 0.0323, 0.0408, 0.0349, 0.0368, 0.0429, 0.0321, 0.0419,\n",
              "                      0.0534, 0.0430, 0.0234, 0.0469, 0.0346, 0.0305, 0.0338, 0.0404, 0.0351,\n",
              "                      0.0385, 0.0362, 0.0278, 0.0323, 0.0390, 0.0374, 0.0345, 0.0312, 0.0324,\n",
              "                      0.0375, 0.0353, 0.0377, 0.0361, 0.0272, 0.0537, 0.0253, 0.0301, 0.0333,\n",
              "                      0.0377, 0.0384, 0.0284, 0.0337, 0.0476, 0.0407, 0.0417, 0.0497, 0.0414,\n",
              "                      0.0468, 0.0383, 0.0290, 0.0258, 0.0243, 0.0319, 0.0499, 0.0435, 0.0374,\n",
              "                      0.0449], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.5.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  52,  53,  52,  51,  54,  52,  57,  53,  53,  52,  50,  53,  50,\n",
              "                       54,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.self_attn.q_proj.weight',\n",
              "              tensor([[109],\n",
              "                      [121],\n",
              "                      [ 65],\n",
              "                      ...,\n",
              "                      [109],\n",
              "                      [  1],\n",
              "                      [ 50]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.self_attn.q_proj.bias',\n",
              "              tensor([ 0.0690,  0.0615,  0.0736,  ..., -0.0271, -0.0277, -0.0207],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.self_attn.q_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.5.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0465, 0.0481, 0.0447, 0.0494, 0.0408, 0.0360, 0.0315, 0.0306, 0.0533,\n",
              "                      0.0490, 0.0491, 0.0527, 0.0426, 0.0409, 0.0403, 0.0409, 0.0427, 0.0437,\n",
              "                      0.0431, 0.0432, 0.0459, 0.0506, 0.0427, 0.0466, 0.0440, 0.0429, 0.0428,\n",
              "                      0.0432, 0.0417, 0.0411, 0.0338, 0.0422, 0.0414, 0.0413, 0.0388, 0.0401,\n",
              "                      0.0428, 0.0427, 0.0428, 0.0433, 0.0514, 0.0457, 0.0446, 0.0509, 0.0429,\n",
              "                      0.0416, 0.0428, 0.0424, 0.0425, 0.0434, 0.0434, 0.0426, 0.0429, 0.0426,\n",
              "                      0.0422, 0.0413, 0.0424, 0.0429, 0.0413, 0.0425, 0.0432, 0.0431, 0.0446,\n",
              "                      0.0433], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.5.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  50,  51,  49,  48,  56,  50,  53,  53,  56,  54,  51,  49,  56,\n",
              "                       57,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.self_attn.out_proj.weight',\n",
              "              tensor([[167],\n",
              "                      [ 77],\n",
              "                      [252],\n",
              "                      ...,\n",
              "                      [ 64],\n",
              "                      [141],\n",
              "                      [161]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.self_attn.out_proj.bias',\n",
              "              tensor([-0.0196, -0.0021,  0.0084,  ...,  0.0067, -0.0302,  0.0215],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.self_attn.out_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ..., 255,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.5.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0345, 0.0349, 0.0274, 0.0302, 0.0293, 0.0738, 0.1323, 0.0744, 0.0749,\n",
              "                      0.0238, 0.0321, 0.0306, 0.0248, 0.0255, 0.0341, 0.0244, 0.0257, 0.0555,\n",
              "                      0.0351, 0.0763, 0.0355, 0.0752, 0.0364, 0.0317, 0.0576, 0.0490, 0.0627,\n",
              "                      0.0537, 0.0356, 0.0265, 0.0313, 0.0317, 0.0282, 0.0296, 0.0413, 0.0273,\n",
              "                      0.0338, 0.0482, 0.0420, 0.0315, 0.0295, 0.0342, 0.0451, 0.0335, 0.0261,\n",
              "                      0.0747, 0.0326, 0.0242, 0.0261, 0.0290, 0.0419, 0.0393, 0.0374, 0.0339,\n",
              "                      0.0317, 0.0354, 0.0278, 0.0752, 0.0365, 0.0250, 0.0248, 0.0298, 0.0274,\n",
              "                      0.0308], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.5.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       52,  57,  56,  51,  49,  48,  57,  50,  51,  53,  55,  54,  51,  53,\n",
              "                       53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.self_attn_layer_norm.bias',\n",
              "              tensor([-0.0043,  0.0037,  0.0123,  ...,  0.0029,  0.0447, -0.0303],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.fc1.weight',\n",
              "              tensor([[197],\n",
              "                      [246],\n",
              "                      [ 66],\n",
              "                      ...,\n",
              "                      [173],\n",
              "                      [122],\n",
              "                      [179]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.fc1.bias',\n",
              "              tensor([-0.0235, -0.0249, -0.0160,  ..., -0.0467, -0.0204,  0.0211],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.fc1.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.5.fc1.weight.nested_absmax',\n",
              "              tensor([0.0440, 0.0422, 0.0390, 0.0451, 0.0365, 0.0368, 0.0329, 0.0361, 0.0319,\n",
              "                      0.0444, 0.0380, 0.0426, 0.0499, 0.0426, 0.0444, 0.0409, 0.0430, 0.0358,\n",
              "                      0.0440, 0.0433, 0.0410, 0.0443, 0.0449, 0.0404, 0.0444, 0.0446, 0.0427,\n",
              "                      0.0379, 0.0447, 0.0427, 0.0445, 0.0310, 0.0446, 0.0414, 0.0426, 0.0425,\n",
              "                      0.0444, 0.0387, 0.0383, 0.0447, 0.0371, 0.0415, 0.0437, 0.0442, 0.0428,\n",
              "                      0.0414, 0.0412, 0.0346, 0.0382, 0.0446, 0.0424, 0.0399, 0.0394, 0.0325,\n",
              "                      0.0375, 0.0358, 0.0454, 0.0444, 0.0444, 0.0444, 0.0438, 0.0439, 0.0414,\n",
              "                      0.0371, 0.0390, 0.0434, 0.0437, 0.0360, 0.0410, 0.0371, 0.0446, 0.0446,\n",
              "                      0.0422, 0.0394, 0.0375, 0.0365, 0.0402, 0.0432, 0.0441, 0.0444, 0.0448,\n",
              "                      0.0344, 0.0438, 0.0390, 0.0444, 0.0436, 0.0462, 0.0400, 0.0412, 0.0425,\n",
              "                      0.0440, 0.0425, 0.0396, 0.0445, 0.0408, 0.0379, 0.0444, 0.0407, 0.0435,\n",
              "                      0.0434, 0.0377, 0.0446, 0.0429, 0.0485, 0.0453, 0.0433, 0.0444, 0.0437,\n",
              "                      0.0442, 0.0441, 0.0430, 0.0379, 0.0415, 0.0458, 0.0443, 0.0408, 0.0429,\n",
              "                      0.0401, 0.0441, 0.0444, 0.0429, 0.0437, 0.0421, 0.0371, 0.0382, 0.0435,\n",
              "                      0.0351, 0.0413, 0.0424, 0.0428, 0.0381, 0.0438, 0.0372, 0.0422, 0.0423,\n",
              "                      0.0443, 0.0430, 0.0409, 0.0444, 0.0349, 0.0430, 0.0447, 0.0433, 0.0441,\n",
              "                      0.0440, 0.0439, 0.0433, 0.0417, 0.0387, 0.0447, 0.0405, 0.0435, 0.0410,\n",
              "                      0.0447, 0.0385, 0.0384, 0.0379, 0.0447, 0.0410, 0.0432, 0.0446, 0.0370,\n",
              "                      0.0352, 0.0448, 0.0311, 0.0397, 0.0400, 0.0372, 0.0430, 0.0383, 0.0416,\n",
              "                      0.0415, 0.0441, 0.0436, 0.0433, 0.0382, 0.0432, 0.0397, 0.0356, 0.0418,\n",
              "                      0.0375, 0.0437, 0.0403, 0.0372, 0.0320, 0.0394, 0.0374, 0.0418, 0.0371,\n",
              "                      0.0381, 0.0429, 0.0386, 0.0424, 0.0344, 0.0447, 0.0447, 0.0446, 0.0396,\n",
              "                      0.0391, 0.0410, 0.0437, 0.0418, 0.0428, 0.0418, 0.0440, 0.0413, 0.0378,\n",
              "                      0.0358, 0.0393, 0.0430, 0.0349, 0.0446, 0.0425, 0.0444, 0.0424, 0.0412,\n",
              "                      0.0341, 0.0435, 0.0444, 0.0363, 0.0260, 0.0343, 0.0403, 0.0531, 0.0335,\n",
              "                      0.0343, 0.0409, 0.0426, 0.0451, 0.0382, 0.0415, 0.0413, 0.0435, 0.0437,\n",
              "                      0.0293, 0.0430, 0.0445, 0.0447, 0.0437, 0.0438, 0.0441, 0.0447, 0.0447,\n",
              "                      0.0446, 0.0447, 0.0358, 0.0445, 0.0430, 0.0419, 0.0399, 0.0438, 0.0391,\n",
              "                      0.0438, 0.0391, 0.0412, 0.0410], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.5.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  48,  51,  48,  57,  50,  49,  50,  50,  48,  55,  55,  57,  52,\n",
              "                       49,  57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.fc2.weight',\n",
              "              tensor([[ 90],\n",
              "                      [ 75],\n",
              "                      [146],\n",
              "                      ...,\n",
              "                      [ 77],\n",
              "                      [178],\n",
              "                      [ 76]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.fc2.bias',\n",
              "              tensor([-0.0130,  0.0018, -0.0031,  ...,  0.0107, -0.0502,  0.0026],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.fc2.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ..., 255,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.5.fc2.weight.nested_absmax',\n",
              "              tensor([0.0456, 0.0419, 0.0520, 0.0508, 0.0313, 0.0385, 0.0576, 0.0292, 0.0404,\n",
              "                      0.0444, 0.0585, 0.0383, 0.0439, 0.0346, 0.0558, 0.0394, 0.0416, 0.0361,\n",
              "                      0.0402, 0.0390, 0.0564, 0.0327, 0.0584, 0.0353, 0.1835, 0.0501, 0.0573,\n",
              "                      0.0303, 0.0468, 0.0418, 0.0398, 0.0521, 0.0348, 0.0588, 0.0585, 0.0485,\n",
              "                      0.0532, 0.0353, 0.0432, 0.0431, 0.0401, 0.0418, 0.0564, 0.0471, 0.0322,\n",
              "                      0.0380, 0.0581, 0.0560, 0.0387, 0.0331, 0.0581, 0.0578, 0.0270, 0.0433,\n",
              "                      0.0559, 0.0582, 0.0480, 0.0581, 0.0514, 0.0444, 0.0384, 0.0585, 0.0336,\n",
              "                      0.0561, 0.0562, 0.0540, 0.0508, 0.0474, 0.0582, 0.0582, 0.0513, 0.0554,\n",
              "                      0.0414, 0.0339, 0.0583, 0.0585, 0.0598, 0.0463, 0.0585, 0.0524, 0.0502,\n",
              "                      0.0585, 0.0585, 0.0571, 0.0538, 0.0331, 0.0545, 0.0452, 0.0396, 0.0361,\n",
              "                      0.0584, 0.0451, 0.0472, 0.0317, 0.0346, 0.0468, 0.0584, 0.0286, 0.0572,\n",
              "                      0.0587, 0.0371, 0.0462, 0.0315, 0.0551, 0.0473, 0.0353, 0.0416, 0.0589,\n",
              "                      0.0517, 0.0590, 0.0535, 0.0257, 0.0441, 0.0413, 0.0525, 0.0585, 0.0577,\n",
              "                      0.0536, 0.0281, 0.0300, 0.0582, 0.0480, 0.0329, 0.0397, 0.0562, 0.0444,\n",
              "                      0.0555, 0.0569, 0.0585, 0.0508, 0.0278, 0.0490, 0.0325, 0.0310, 0.0578,\n",
              "                      0.0352, 0.0300, 0.0428, 0.0319, 0.0513, 0.0585, 0.0536, 0.0389, 0.0572,\n",
              "                      0.0560, 0.0580, 0.0349, 0.0363, 0.0439, 0.0490, 0.0429, 0.0575, 0.0506,\n",
              "                      0.0491, 0.0585, 0.0396, 0.0314, 0.0444, 0.0349, 0.0301, 0.0421, 0.0526,\n",
              "                      0.0499, 0.0577, 0.0485, 0.0483, 0.0516, 0.0404, 0.0433, 0.0444, 0.0573,\n",
              "                      0.0267, 0.0350, 0.0292, 0.0541, 0.0581, 0.0291, 0.0534, 0.0536, 0.0519,\n",
              "                      0.0473, 0.0497, 0.0356, 0.0402, 0.0346, 0.0423, 0.0386, 0.0460, 0.0369,\n",
              "                      0.0550, 0.0357, 0.0407, 0.0584, 0.0336, 0.0529, 0.0451, 0.0295, 0.0517,\n",
              "                      0.0549, 0.0344, 0.0521, 0.0495, 0.0278, 0.0487, 0.0433, 0.0366, 0.0292,\n",
              "                      0.0318, 0.0351, 0.0592, 0.0451, 0.0433, 0.0536, 0.0581, 0.0351, 0.0472,\n",
              "                      0.0573, 0.0429, 0.0578, 0.0585, 0.0573, 0.0410, 0.0483, 0.0394, 0.0471,\n",
              "                      0.0452, 0.0338, 0.0517, 0.0574, 0.0596, 0.0579, 0.0362, 0.0585, 0.0435,\n",
              "                      0.0585, 0.0555, 0.0438, 0.0443, 0.0303, 0.0451, 0.0504, 0.0319, 0.0398,\n",
              "                      0.0566, 0.0383, 0.0377, 0.0313, 0.0574, 0.0349, 0.0389, 0.0280, 0.0471,\n",
              "                      0.0583, 0.0491, 0.0436, 0.0569], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.5.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  54,  52,  54,  48,  49,  57,  50,  50,  48,  51,  53,  50,  49,\n",
              "                       55,  51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.5.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.5.final_layer_norm.bias',\n",
              "              tensor([-0.0143, -0.0044,  0.0134,  ..., -0.0267, -0.0147,  0.0066],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.self_attn.k_proj.weight',\n",
              "              tensor([[162],\n",
              "                      [107],\n",
              "                      [249],\n",
              "                      ...,\n",
              "                      [ 88],\n",
              "                      [ 76],\n",
              "                      [142]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0252,  0.0120,  0.0327,  ..., -0.0117,  0.0157,  0.0186],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.self_attn.k_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ...,   0, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.6.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0438, 0.0449, 0.0438, 0.0450, 0.0466, 0.0431, 0.0499, 0.0462, 0.0411,\n",
              "                      0.0409, 0.0405, 0.0405, 0.0491, 0.0492, 0.0448, 0.0508, 0.0442, 0.0473,\n",
              "                      0.0476, 0.0442, 0.0501, 0.0497, 0.0474, 0.0513, 0.0481, 0.0497, 0.0502,\n",
              "                      0.0489, 0.0426, 0.0423, 0.0438, 0.0428, 0.0426, 0.0438, 0.0450, 0.0434,\n",
              "                      0.0450, 0.0437, 0.0439, 0.0433, 0.0412, 0.0416, 0.0412, 0.0418, 0.0400,\n",
              "                      0.0428, 0.0446, 0.0483, 0.0562, 0.0504, 0.0539, 0.0575, 0.0414, 0.0429,\n",
              "                      0.0415, 0.0420, 0.0483, 0.0539, 0.0511, 0.0519, 0.0431, 0.0443, 0.0431,\n",
              "                      0.0436], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.6.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  52,  57,  56,  51,  51,  53,  54,  50,  57,  55,  48,  49,  54,\n",
              "                       49,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.self_attn.v_proj.weight',\n",
              "              tensor([[ 95],\n",
              "                      [225],\n",
              "                      [158],\n",
              "                      ...,\n",
              "                      [134],\n",
              "                      [ 38],\n",
              "                      [145]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.self_attn.v_proj.bias',\n",
              "              tensor([-0.0247, -0.0177, -0.0221,  ..., -0.0037, -0.0151, -0.0016],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.self_attn.v_proj.weight.absmax',\n",
              "              tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.6.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0272, 0.0270, 0.0290, 0.0290, 0.0326, 0.0306, 0.0383, 0.0429, 0.0371,\n",
              "                      0.0260, 0.0249, 0.0315, 0.0361, 0.0483, 0.0398, 0.0411, 0.0422, 0.0462,\n",
              "                      0.0379, 0.0468, 0.0335, 0.0364, 0.0463, 0.0404, 0.0493, 0.0585, 0.0512,\n",
              "                      0.0417, 0.0244, 0.0244, 0.0271, 0.0231, 0.0310, 0.0211, 0.0290, 0.0245,\n",
              "                      0.0322, 0.0266, 0.0220, 0.0278, 0.0248, 0.0265, 0.0228, 0.0220, 0.0393,\n",
              "                      0.0390, 0.0290, 0.0335, 0.0312, 0.0335, 0.0341, 0.0270, 0.0291, 0.0278,\n",
              "                      0.0402, 0.0350, 0.0408, 0.0482, 0.0390, 0.0542, 0.0331, 0.0292, 0.0328,\n",
              "                      0.0317], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.6.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  48,  52,  55,  55,  54,  51,  54,  55,  53,  52,  53,  49,  50,\n",
              "                       55,  57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.self_attn.q_proj.weight',\n",
              "              tensor([[ 75],\n",
              "                      [190],\n",
              "                      [ 60],\n",
              "                      ...,\n",
              "                      [232],\n",
              "                      [  8],\n",
              "                      [163]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.self_attn.q_proj.bias',\n",
              "              tensor([ 0.0389,  0.0784,  0.0352,  ..., -0.0407,  0.0009,  0.0567],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.self_attn.q_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.6.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0406, 0.0400, 0.0399, 0.0409, 0.0403, 0.0404, 0.0401, 0.0405, 0.0272,\n",
              "                      0.0288, 0.0317, 0.0398, 0.0519, 0.0508, 0.0420, 0.0417, 0.0376, 0.0409,\n",
              "                      0.0445, 0.0409, 0.0482, 0.0493, 0.0527, 0.0520, 0.0545, 0.0490, 0.0556,\n",
              "                      0.0507, 0.0398, 0.0398, 0.0398, 0.0401, 0.0403, 0.0409, 0.0421, 0.0415,\n",
              "                      0.0405, 0.0413, 0.0400, 0.0406, 0.0286, 0.0334, 0.0389, 0.0400, 0.0400,\n",
              "                      0.0396, 0.0390, 0.0398, 0.0514, 0.0551, 0.0495, 0.0543, 0.0410, 0.0420,\n",
              "                      0.0423, 0.0424, 0.0478, 0.0494, 0.0490, 0.0493, 0.0398, 0.0399, 0.0400,\n",
              "                      0.0403], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.6.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  53,  49,  49,  53,  56,  52,  57,  57,  55,  49,  55,  55,  49,\n",
              "                       50,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.self_attn.out_proj.weight',\n",
              "              tensor([[ 78],\n",
              "                      [ 41],\n",
              "                      [113],\n",
              "                      ...,\n",
              "                      [244],\n",
              "                      [184],\n",
              "                      [ 86]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.self_attn.out_proj.bias',\n",
              "              tensor([ 0.0126, -0.0047, -0.0473,  ...,  0.0300, -0.0375,  0.0406],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.self_attn.out_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.6.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0247, 0.0316, 0.0296, 0.0311, 0.0304, 0.0688, 0.0868, 0.0766, 0.0765,\n",
              "                      0.0301, 0.0303, 0.0332, 0.0349, 0.0272, 0.0410, 0.0321, 0.0237, 0.0446,\n",
              "                      0.0276, 0.0780, 0.0323, 0.0619, 0.0318, 0.0256, 0.0725, 0.0562, 0.0776,\n",
              "                      0.0774, 0.0327, 0.0312, 0.0331, 0.0408, 0.0340, 0.0245, 0.0354, 0.0277,\n",
              "                      0.0268, 0.0565, 0.0594, 0.0287, 0.0348, 0.0293, 0.0359, 0.0357, 0.0295,\n",
              "                      0.0777, 0.0297, 0.0313, 0.0368, 0.0307, 0.0324, 0.0260, 0.0384, 0.0432,\n",
              "                      0.0260, 0.0313, 0.0267, 0.0752, 0.0323, 0.0365, 0.0312, 0.0484, 0.0258,\n",
              "                      0.0264], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.6.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       52,  55,  50,  55,  50,  52,  56,  49,  48,  50,  52,  50,  54,  53,\n",
              "                       50,  57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.self_attn_layer_norm.bias',\n",
              "              tensor([-0.0115,  0.0057,  0.0131,  ..., -0.0086,  0.0506, -0.0260],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.fc1.weight',\n",
              "              tensor([[212],\n",
              "                      [ 26],\n",
              "                      [253],\n",
              "                      ...,\n",
              "                      [192],\n",
              "                      [221],\n",
              "                      [ 78]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.fc1.bias',\n",
              "              tensor([-0.0038, -0.0246,  0.0214,  ..., -0.0118, -0.0249, -0.0119],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.fc1.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.6.fc1.weight.nested_absmax',\n",
              "              tensor([0.0440, 0.0439, 0.0462, 0.0424, 0.0441, 0.0442, 0.0396, 0.0442, 0.0429,\n",
              "                      0.0438, 0.0389, 0.0441, 0.0426, 0.0428, 0.0442, 0.0375, 0.0435, 0.0424,\n",
              "                      0.0443, 0.0421, 0.0438, 0.0444, 0.0438, 0.0352, 0.0403, 0.0442, 0.0412,\n",
              "                      0.0354, 0.0441, 0.0405, 0.0443, 0.0338, 0.0343, 0.0416, 0.0381, 0.0325,\n",
              "                      0.0440, 0.0441, 0.0436, 0.0366, 0.0449, 0.0414, 0.0422, 0.0399, 0.0399,\n",
              "                      0.0391, 0.0441, 0.0429, 0.0431, 0.0435, 0.0352, 0.0387, 0.0446, 0.0431,\n",
              "                      0.0383, 0.0396, 0.0438, 0.0437, 0.0441, 0.0403, 0.0424, 0.0427, 0.0447,\n",
              "                      0.0367, 0.0369, 0.0438, 0.0320, 0.0442, 0.0432, 0.0422, 0.0438, 0.0328,\n",
              "                      0.0430, 0.0442, 0.0402, 0.0427, 0.0349, 0.0383, 0.0438, 0.0419, 0.0407,\n",
              "                      0.0387, 0.0436, 0.0415, 0.0364, 0.0422, 0.0436, 0.0435, 0.0369, 0.0403,\n",
              "                      0.0436, 0.0385, 0.0414, 0.0396, 0.0428, 0.0440, 0.0388, 0.0296, 0.0436,\n",
              "                      0.0364, 0.0344, 0.0441, 0.0427, 0.0434, 0.0436, 0.0426, 0.0390, 0.0441,\n",
              "                      0.0383, 0.0442, 0.0411, 0.0444, 0.0411, 0.0380, 0.0416, 0.0437, 0.0435,\n",
              "                      0.0428, 0.0429, 0.0435, 0.0427, 0.0377, 0.0307, 0.0336, 0.0442, 0.0443,\n",
              "                      0.0400, 0.0370, 0.0441, 0.0425, 0.0442, 0.0434, 0.0442, 0.0356, 0.0408,\n",
              "                      0.0442, 0.0436, 0.0441, 0.0359, 0.0437, 0.0449, 0.0441, 0.0399, 0.0432,\n",
              "                      0.0446, 0.0435, 0.0426, 0.0378, 0.0433, 0.0406, 0.0442, 0.0409, 0.0411,\n",
              "                      0.0440, 0.0417, 0.0367, 0.0416, 0.0397, 0.0433, 0.0386, 0.0442, 0.0338,\n",
              "                      0.0441, 0.0313, 0.0441, 0.0430, 0.0399, 0.0421, 0.0440, 0.0439, 0.0322,\n",
              "                      0.0439, 0.0407, 0.0423, 0.0443, 0.0447, 0.0411, 0.0435, 0.0410, 0.0385,\n",
              "                      0.0351, 0.0430, 0.0411, 0.0440, 0.0384, 0.0431, 0.0411, 0.0433, 0.0426,\n",
              "                      0.0388, 0.0426, 0.0427, 0.0438, 0.0442, 0.0416, 0.0382, 0.0380, 0.0371,\n",
              "                      0.0410, 0.0436, 0.0341, 0.0429, 0.0294, 0.0401, 0.0439, 0.0336, 0.0397,\n",
              "                      0.0407, 0.0430, 0.0435, 0.0441, 0.0406, 0.0386, 0.0427, 0.0419, 0.0430,\n",
              "                      0.0435, 0.0491, 0.0350, 0.0399, 0.0412, 0.0356, 0.0435, 0.0425, 0.0449,\n",
              "                      0.0401, 0.0424, 0.0435, 0.0397, 0.0468, 0.0446, 0.0427, 0.0438, 0.0402,\n",
              "                      0.0439, 0.0364, 0.0439, 0.0441, 0.0441, 0.0474, 0.0431, 0.0318, 0.0405,\n",
              "                      0.0386, 0.0443, 0.0430, 0.0434, 0.0425, 0.0441, 0.0338, 0.0398, 0.0385,\n",
              "                      0.0313, 0.0424, 0.0369, 0.0442], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.6.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  48,  56,  49,  49,  50,  54,  50,  49,  51,  48,  55,  51,  55,\n",
              "                       51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.fc2.weight',\n",
              "              tensor([[109],\n",
              "                      [ 87],\n",
              "                      [171],\n",
              "                      ...,\n",
              "                      [151],\n",
              "                      [ 62],\n",
              "                      [ 27]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.fc2.bias',\n",
              "              tensor([-0.0136, -0.0033,  0.0110,  ...,  0.0082, -0.0350,  0.0192],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.fc2.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.6.fc2.weight.nested_absmax',\n",
              "              tensor([0.0562, 0.0493, 0.0401, 0.0358, 0.0473, 0.0387, 0.0566, 0.0444, 0.0527,\n",
              "                      0.0312, 0.0389, 0.0553, 0.0336, 0.0471, 0.0563, 0.0555, 0.0579, 0.0565,\n",
              "                      0.0543, 0.0519, 0.0544, 0.0527, 0.0513, 0.0296, 0.1816, 0.0444, 0.0515,\n",
              "                      0.0569, 0.0350, 0.0378, 0.0428, 0.0563, 0.0442, 0.0540, 0.0480, 0.0523,\n",
              "                      0.0535, 0.0568, 0.0417, 0.0495, 0.0499, 0.0257, 0.0422, 0.0368, 0.0552,\n",
              "                      0.0563, 0.0567, 0.0465, 0.0542, 0.0558, 0.0549, 0.0563, 0.0339, 0.0529,\n",
              "                      0.0406, 0.0560, 0.0530, 0.0396, 0.0469, 0.0568, 0.0356, 0.0566, 0.0224,\n",
              "                      0.0565, 0.0455, 0.0548, 0.0493, 0.0406, 0.0532, 0.0566, 0.0561, 0.0444,\n",
              "                      0.0394, 0.0561, 0.0542, 0.0474, 0.0583, 0.0542, 0.0567, 0.0557, 0.0562,\n",
              "                      0.0539, 0.0519, 0.0539, 0.0552, 0.0553, 0.0497, 0.0268, 0.0358, 0.0572,\n",
              "                      0.0553, 0.0569, 0.0564, 0.0449, 0.0422, 0.0403, 0.0562, 0.0563, 0.0491,\n",
              "                      0.0573, 0.0526, 0.0566, 0.0497, 0.0510, 0.0480, 0.0563, 0.0497, 0.0567,\n",
              "                      0.0421, 0.0538, 0.0405, 0.0507, 0.0555, 0.0556, 0.0472, 0.0498, 0.0488,\n",
              "                      0.0422, 0.0563, 0.0284, 0.0459, 0.0412, 0.0560, 0.0493, 0.0556, 0.0428,\n",
              "                      0.0373, 0.0549, 0.0463, 0.0564, 0.0543, 0.0433, 0.0343, 0.0541, 0.0510,\n",
              "                      0.0556, 0.0496, 0.0562, 0.0564, 0.0500, 0.0393, 0.0519, 0.0564, 0.0573,\n",
              "                      0.0568, 0.0560, 0.0411, 0.0346, 0.0317, 0.0563, 0.0513, 0.0389, 0.0563,\n",
              "                      0.0477, 0.0454, 0.0559, 0.0437, 0.0414, 0.0566, 0.0539, 0.0439, 0.0365,\n",
              "                      0.0539, 0.0298, 0.0470, 0.0387, 0.0466, 0.0363, 0.0558, 0.0377, 0.0575,\n",
              "                      0.0564, 0.0560, 0.0563, 0.0560, 0.0304, 0.0467, 0.0351, 0.0564, 0.0560,\n",
              "                      0.0466, 0.0566, 0.0483, 0.0499, 0.0436, 0.0448, 0.0457, 0.0492, 0.0561,\n",
              "                      0.0510, 0.0329, 0.0335, 0.0575, 0.0492, 0.0377, 0.0533, 0.0275, 0.0564,\n",
              "                      0.0522, 0.0397, 0.0536, 0.0561, 0.0541, 0.0565, 0.0433, 0.0321, 0.0558,\n",
              "                      0.0448, 0.0563, 0.0573, 0.0559, 0.0439, 0.0566, 0.0382, 0.0518, 0.0573,\n",
              "                      0.0421, 0.0411, 0.0480, 0.0362, 0.0362, 0.0450, 0.0422, 0.0568, 0.0444,\n",
              "                      0.0341, 0.0531, 0.0301, 0.0566, 0.0573, 0.0330, 0.0428, 0.0514, 0.0560,\n",
              "                      0.0326, 0.0511, 0.0533, 0.0491, 0.0475, 0.0459, 0.0401, 0.0345, 0.0422,\n",
              "                      0.0547, 0.0345, 0.0480, 0.0562, 0.0522, 0.0432, 0.0552, 0.0561, 0.0470,\n",
              "                      0.0314, 0.0453, 0.0469, 0.0436], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.6.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  56,  52,  51,  53,  49,  57,  50,  49,  48,  56,  49,  53,  52,\n",
              "                       51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.6.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.6.final_layer_norm.bias',\n",
              "              tensor([-0.0043,  0.0095, -0.0075,  ..., -0.0156, -0.0359,  0.0300],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.self_attn.k_proj.weight',\n",
              "              tensor([[145],\n",
              "                      [ 74],\n",
              "                      [225],\n",
              "                      ...,\n",
              "                      [134],\n",
              "                      [147],\n",
              "                      [116]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0207,  0.0168,  0.0303,  ...,  0.0280,  0.0307, -0.0306],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.self_attn.k_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.7.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0412, 0.0441, 0.0418, 0.0430, 0.0427, 0.0405, 0.0457, 0.0410, 0.0471,\n",
              "                      0.0480, 0.0435, 0.0496, 0.0473, 0.0503, 0.0527, 0.0529, 0.0379, 0.0375,\n",
              "                      0.0466, 0.0379, 0.0470, 0.0482, 0.0498, 0.0487, 0.0591, 0.0587, 0.0591,\n",
              "                      0.0615, 0.0386, 0.0388, 0.0381, 0.0426, 0.0431, 0.0441, 0.0433, 0.0447,\n",
              "                      0.0381, 0.0428, 0.0377, 0.0434, 0.0458, 0.0482, 0.0464, 0.0388, 0.0412,\n",
              "                      0.0432, 0.0427, 0.0407, 0.0444, 0.0437, 0.0447, 0.0444, 0.0456, 0.0507,\n",
              "                      0.0296, 0.0440, 0.0407, 0.0408, 0.0427, 0.0424, 0.0376, 0.0376, 0.0460,\n",
              "                      0.0420], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.7.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  55,  54,  51,  48,  53,  53,  53,  48,  51,  51,  54,  56,  51,\n",
              "                       55,  56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.self_attn.v_proj.weight',\n",
              "              tensor([[181],\n",
              "                      [133],\n",
              "                      [181],\n",
              "                      ...,\n",
              "                      [237],\n",
              "                      [153],\n",
              "                      [130]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.self_attn.v_proj.bias',\n",
              "              tensor([-0.0022, -0.0010, -0.0070,  ...,  0.0183,  0.0089,  0.0038],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.self_attn.v_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.7.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0265, 0.0300, 0.0287, 0.0328, 0.0300, 0.0305, 0.0333, 0.0277, 0.0471,\n",
              "                      0.0445, 0.0486, 0.0429, 0.0272, 0.0348, 0.0402, 0.0274, 0.0303, 0.0277,\n",
              "                      0.0409, 0.0351, 0.0511, 0.0351, 0.0559, 0.0459, 0.0517, 0.0491, 0.0446,\n",
              "                      0.0412, 0.0377, 0.0396, 0.0426, 0.0533, 0.0423, 0.0385, 0.0360, 0.0366,\n",
              "                      0.0359, 0.0381, 0.0426, 0.0336, 0.0362, 0.0543, 0.0493, 0.0551, 0.0284,\n",
              "                      0.0233, 0.0261, 0.0251, 0.0348, 0.0345, 0.0353, 0.0377, 0.0445, 0.0528,\n",
              "                      0.0393, 0.0335, 0.0215, 0.0286, 0.0223, 0.0367, 0.0258, 0.0425, 0.0257,\n",
              "                      0.0352], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.7.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  50,  51,  52,  57,  56,  52,  56,  52,  52,  57,  50,  51,  48,\n",
              "                       49,  57,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.self_attn.q_proj.weight',\n",
              "              tensor([[ 68],\n",
              "                      [172],\n",
              "                      [121],\n",
              "                      ...,\n",
              "                      [206],\n",
              "                      [ 54],\n",
              "                      [115]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.self_attn.q_proj.bias',\n",
              "              tensor([-0.0359, -0.0408, -0.0218,  ..., -0.0390, -0.0543, -0.0379],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.self_attn.q_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.7.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0394, 0.0393, 0.0406, 0.0398, 0.0390, 0.0394, 0.0387, 0.0393, 0.0488,\n",
              "                      0.0445, 0.0481, 0.0460, 0.0394, 0.0418, 0.0410, 0.0413, 0.0374, 0.0364,\n",
              "                      0.0375, 0.0372, 0.0408, 0.0399, 0.0475, 0.0385, 0.0579, 0.0639, 0.0614,\n",
              "                      0.0671, 0.0379, 0.0376, 0.0375, 0.0379, 0.0406, 0.0378, 0.0373, 0.0387,\n",
              "                      0.0376, 0.0373, 0.0375, 0.0376, 0.0397, 0.0372, 0.0427, 0.0471, 0.0367,\n",
              "                      0.0376, 0.0383, 0.0394, 0.0396, 0.0417, 0.0403, 0.0398, 0.0398, 0.0412,\n",
              "                      0.0366, 0.0492, 0.0379, 0.0378, 0.0373, 0.0378, 0.0346, 0.0366, 0.0378,\n",
              "                      0.0377], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.7.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  55,  52,  50,  53,  54,  54,  52,  48,  54,  55,  50,  54,  56,\n",
              "                       51,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.self_attn.out_proj.weight',\n",
              "              tensor([[ 87],\n",
              "                      [118],\n",
              "                      [ 68],\n",
              "                      ...,\n",
              "                      [135],\n",
              "                      [ 34],\n",
              "                      [123]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.self_attn.out_proj.bias',\n",
              "              tensor([-0.0041, -0.0124, -0.0261,  ...,  0.0205, -0.0147,  0.0258],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.self_attn.out_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ..., 255,   0,   1], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.7.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0301, 0.0293, 0.0335, 0.0362, 0.0434, 0.0464, 0.0814, 0.0471, 0.0664,\n",
              "                      0.0398, 0.0351, 0.0411, 0.0388, 0.0621, 0.0362, 0.0271, 0.0292, 0.0395,\n",
              "                      0.0375, 0.0752, 0.0334, 0.0574, 0.0258, 0.0293, 0.0717, 0.0413, 0.0697,\n",
              "                      0.0733, 0.0691, 0.0369, 0.0273, 0.0323, 0.0279, 0.0306, 0.0381, 0.0324,\n",
              "                      0.0419, 0.0287, 0.0421, 0.0373, 0.0338, 0.0409, 0.0468, 0.0410, 0.0438,\n",
              "                      0.0393, 0.0390, 0.0317, 0.0461, 0.0351, 0.0262, 0.0399, 0.0403, 0.0510,\n",
              "                      0.0345, 0.0277, 0.0360, 0.0631, 0.0330, 0.0438, 0.0381, 0.0305, 0.0293,\n",
              "                      0.0404], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.7.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       53,  48,  50,  48,  51,  49,  51,  55,  48,  57,  57,  55,  52,  50,\n",
              "                       56,  57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.self_attn_layer_norm.bias',\n",
              "              tensor([-0.0125,  0.0127,  0.0139,  ..., -0.0174,  0.0437, -0.0188],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.fc1.weight',\n",
              "              tensor([[201],\n",
              "                      [181],\n",
              "                      [117],\n",
              "                      ...,\n",
              "                      [163],\n",
              "                      [220],\n",
              "                      [101]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.fc1.bias',\n",
              "              tensor([-0.0205, -0.0095, -0.0104,  ..., -0.0440, -0.0147, -0.0305],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.fc1.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ..., 255,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.7.fc1.weight.nested_absmax',\n",
              "              tensor([0.0399, 0.0436, 0.0340, 0.0434, 0.0436, 0.0434, 0.0442, 0.0396, 0.0414,\n",
              "                      0.0433, 0.0435, 0.0336, 0.0430, 0.0401, 0.0410, 0.0398, 0.0436, 0.0382,\n",
              "                      0.0382, 0.0394, 0.0410, 0.0436, 0.0431, 0.0462, 0.0403, 0.0343, 0.0497,\n",
              "                      0.0343, 0.0439, 0.0354, 0.0416, 0.0436, 0.0395, 0.0444, 0.0403, 0.0398,\n",
              "                      0.0424, 0.0430, 0.0434, 0.0386, 0.0424, 0.0429, 0.0430, 0.0437, 0.0325,\n",
              "                      0.0392, 0.0381, 0.0423, 0.0408, 0.0348, 0.0405, 0.0439, 0.0420, 0.0436,\n",
              "                      0.0392, 0.0436, 0.0437, 0.0424, 0.0405, 0.0439, 0.0436, 0.0433, 0.0344,\n",
              "                      0.0400, 0.0434, 0.0423, 0.0423, 0.0439, 0.0434, 0.0429, 0.0422, 0.0430,\n",
              "                      0.0436, 0.0433, 0.0441, 0.0372, 0.0360, 0.0436, 0.0390, 0.0437, 0.0426,\n",
              "                      0.0437, 0.0434, 0.0396, 0.0441, 0.0408, 0.0431, 0.0431, 0.0399, 0.0394,\n",
              "                      0.0369, 0.0432, 0.0422, 0.0426, 0.0435, 0.0414, 0.0426, 0.0437, 0.0418,\n",
              "                      0.0435, 0.0429, 0.0376, 0.0437, 0.0430, 0.0417, 0.0430, 0.0400, 0.0435,\n",
              "                      0.0438, 0.0417, 0.0439, 0.0410, 0.0416, 0.0421, 0.0420, 0.0403, 0.0435,\n",
              "                      0.0434, 0.0382, 0.0384, 0.0428, 0.0426, 0.0420, 0.0434, 0.0433, 0.0436,\n",
              "                      0.0434, 0.0437, 0.0423, 0.0432, 0.0427, 0.0431, 0.0354, 0.0420, 0.0454,\n",
              "                      0.0430, 0.0431, 0.0428, 0.0435, 0.0436, 0.0432, 0.0432, 0.0436, 0.0432,\n",
              "                      0.0435, 0.0431, 0.0433, 0.0422, 0.0437, 0.0414, 0.0436, 0.0412, 0.0422,\n",
              "                      0.0431, 0.0358, 0.0420, 0.0456, 0.0422, 0.0424, 0.0430, 0.0414, 0.0421,\n",
              "                      0.0437, 0.0370, 0.0427, 0.0429, 0.0439, 0.0428, 0.0343, 0.0435, 0.0436,\n",
              "                      0.0411, 0.0432, 0.0438, 0.0425, 0.0436, 0.0428, 0.0426, 0.0336, 0.0417,\n",
              "                      0.0431, 0.0436, 0.0420, 0.0433, 0.0437, 0.0390, 0.0419, 0.0423, 0.0332,\n",
              "                      0.0428, 0.0441, 0.0427, 0.0432, 0.0414, 0.0398, 0.0422, 0.0424, 0.0428,\n",
              "                      0.0438, 0.0371, 0.0435, 0.0433, 0.0432, 0.0439, 0.0416, 0.0432, 0.0360,\n",
              "                      0.0322, 0.0437, 0.0423, 0.0424, 0.0375, 0.0427, 0.0434, 0.0433, 0.0434,\n",
              "                      0.0397, 0.0434, 0.0424, 0.0431, 0.0411, 0.0420, 0.0338, 0.0435, 0.0435,\n",
              "                      0.0436, 0.0437, 0.0423, 0.0412, 0.0431, 0.0451, 0.0418, 0.0447, 0.0415,\n",
              "                      0.0367, 0.0426, 0.0432, 0.0433, 0.0422, 0.0418, 0.0401, 0.0393, 0.0401,\n",
              "                      0.0326, 0.0445, 0.0414, 0.0391, 0.0436, 0.0436, 0.0419, 0.0370, 0.0383,\n",
              "                      0.0437, 0.0427, 0.0434, 0.0386], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.7.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  49,  52,  51,  52,  57,  56,  48,  48,  51,  52,  56,  50,  56,\n",
              "                       49,  57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.fc2.weight',\n",
              "              tensor([[248],\n",
              "                      [184],\n",
              "                      [ 60],\n",
              "                      ...,\n",
              "                      [118],\n",
              "                      [ 43],\n",
              "                      [147]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.fc2.bias',\n",
              "              tensor([-0.0073, -0.0083, -0.0106,  ...,  0.0242, -0.0218,  0.0422],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.fc2.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.7.fc2.weight.nested_absmax',\n",
              "              tensor([0.0537, 0.0350, 0.0536, 0.0494, 0.0538, 0.0506, 0.0493, 0.0537, 0.0536,\n",
              "                      0.0317, 0.0465, 0.0535, 0.0538, 0.0499, 0.0592, 0.0538, 0.0526, 0.0548,\n",
              "                      0.0520, 0.0507, 0.0515, 0.0536, 0.0516, 0.0538, 0.1788, 0.0538, 0.0532,\n",
              "                      0.0512, 0.0523, 0.0549, 0.0539, 0.0584, 0.0447, 0.0579, 0.0575, 0.0546,\n",
              "                      0.0535, 0.0550, 0.0537, 0.0538, 0.0538, 0.0458, 0.0535, 0.0502, 0.0533,\n",
              "                      0.0564, 0.0547, 0.0558, 0.0543, 0.0537, 0.0403, 0.0449, 0.0528, 0.0541,\n",
              "                      0.0488, 0.0533, 0.0497, 0.0538, 0.0357, 0.0544, 0.0527, 0.0539, 0.0526,\n",
              "                      0.0516, 0.0549, 0.0575, 0.0542, 0.0538, 0.0526, 0.0549, 0.0538, 0.0554,\n",
              "                      0.0538, 0.0538, 0.0439, 0.0530, 0.0565, 0.0397, 0.0539, 0.0537, 0.0555,\n",
              "                      0.0536, 0.0548, 0.0571, 0.0520, 0.0499, 0.0538, 0.0344, 0.0509, 0.0566,\n",
              "                      0.0490, 0.0424, 0.0508, 0.0536, 0.0508, 0.0453, 0.0535, 0.0510, 0.0535,\n",
              "                      0.0557, 0.0538, 0.0554, 0.0432, 0.0334, 0.0507, 0.0490, 0.0469, 0.0577,\n",
              "                      0.0548, 0.0573, 0.0452, 0.0522, 0.0538, 0.0539, 0.0543, 0.0441, 0.0575,\n",
              "                      0.0527, 0.0536, 0.0538, 0.0529, 0.0413, 0.0561, 0.0546, 0.0538, 0.0538,\n",
              "                      0.0516, 0.0509, 0.0548, 0.0433, 0.0523, 0.0455, 0.0461, 0.0538, 0.0524,\n",
              "                      0.0549, 0.0538, 0.0494, 0.0553, 0.0541, 0.0535, 0.0502, 0.0513, 0.0519,\n",
              "                      0.0532, 0.0524, 0.0498, 0.0538, 0.0535, 0.0538, 0.0464, 0.0506, 0.0536,\n",
              "                      0.0541, 0.0528, 0.0512, 0.0538, 0.0327, 0.0522, 0.0542, 0.0607, 0.0561,\n",
              "                      0.0362, 0.0502, 0.0525, 0.0547, 0.0541, 0.0541, 0.0532, 0.0455, 0.0546,\n",
              "                      0.0503, 0.0524, 0.0537, 0.0496, 0.0530, 0.0424, 0.0450, 0.0460, 0.0536,\n",
              "                      0.0536, 0.0537, 0.0531, 0.0488, 0.0476, 0.0541, 0.0522, 0.0408, 0.0532,\n",
              "                      0.0522, 0.0531, 0.0513, 0.0539, 0.0511, 0.0533, 0.0548, 0.0535, 0.0533,\n",
              "                      0.0513, 0.0542, 0.0555, 0.0518, 0.0536, 0.0544, 0.0519, 0.0376, 0.0514,\n",
              "                      0.0458, 0.0534, 0.0559, 0.0553, 0.0532, 0.0539, 0.0473, 0.0477, 0.0542,\n",
              "                      0.0466, 0.0519, 0.0511, 0.0534, 0.0467, 0.0449, 0.0538, 0.0503, 0.0533,\n",
              "                      0.0538, 0.0522, 0.0490, 0.0524, 0.0584, 0.0441, 0.0552, 0.0541, 0.0539,\n",
              "                      0.0360, 0.0536, 0.0568, 0.0430, 0.0528, 0.0491, 0.0508, 0.0528, 0.0549,\n",
              "                      0.0447, 0.0536, 0.0541, 0.0536, 0.0416, 0.0533, 0.0522, 0.0538, 0.0509,\n",
              "                      0.0486, 0.0510, 0.0517, 0.0530], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.7.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  49,  49,  55,  51,  57,  57,  53,  55,  51,  51,  50,  54,  49,\n",
              "                       49,  49, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.7.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.7.final_layer_norm.bias',\n",
              "              tensor([-0.0130, -0.0139, -0.0195,  ..., -0.0203, -0.0223,  0.0176],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.self_attn.k_proj.weight',\n",
              "              tensor([[149],\n",
              "                      [250],\n",
              "                      [166],\n",
              "                      ...,\n",
              "                      [114],\n",
              "                      [103],\n",
              "                      [ 93]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.self_attn.k_proj.bias',\n",
              "              tensor([-0.0318, -0.0118,  0.0341,  ..., -0.0005, -0.0003, -0.0253],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.8.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0464, 0.0464, 0.0445, 0.0538, 0.0345, 0.0333, 0.0358, 0.0338, 0.0593,\n",
              "                      0.0624, 0.0602, 0.0608, 0.0528, 0.0370, 0.0360, 0.0342, 0.0340, 0.0330,\n",
              "                      0.0335, 0.0344, 0.0350, 0.0366, 0.0378, 0.0371, 0.0371, 0.0364, 0.0371,\n",
              "                      0.0377, 0.0340, 0.0347, 0.0341, 0.0342, 0.0350, 0.0374, 0.0423, 0.0350,\n",
              "                      0.0472, 0.0543, 0.0519, 0.0575, 0.0346, 0.0335, 0.0362, 0.0378, 0.0351,\n",
              "                      0.0344, 0.0346, 0.0340, 0.0322, 0.0349, 0.0341, 0.0339, 0.0336, 0.0363,\n",
              "                      0.0346, 0.0324, 0.0410, 0.0399, 0.0418, 0.0379, 0.0411, 0.0430, 0.0344,\n",
              "                      0.0384], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.8.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       57,  52,  49,  55,  48,  52,  53,  49,  49,  54,  52,  50,  52,  53,\n",
              "                       54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.self_attn.v_proj.weight',\n",
              "              tensor([[ 22],\n",
              "                      [ 38],\n",
              "                      [155],\n",
              "                      ...,\n",
              "                      [ 70],\n",
              "                      [ 94],\n",
              "                      [177]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.self_attn.v_proj.bias',\n",
              "              tensor([ 0.0218, -0.0059, -0.0046,  ...,  0.0003,  0.0209, -0.0003],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.self_attn.v_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.8.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0399, 0.0456, 0.0431, 0.0364, 0.0389, 0.0433, 0.0334, 0.0358, 0.0465,\n",
              "                      0.0354, 0.0392, 0.0428, 0.0580, 0.0562, 0.0476, 0.0530, 0.0383, 0.0302,\n",
              "                      0.0338, 0.0324, 0.0361, 0.0365, 0.0356, 0.0361, 0.0346, 0.0466, 0.0240,\n",
              "                      0.0207, 0.0320, 0.0230, 0.0255, 0.0284, 0.0364, 0.0337, 0.0435, 0.0480,\n",
              "                      0.0442, 0.0616, 0.0490, 0.0505, 0.0440, 0.0385, 0.0301, 0.0397, 0.0368,\n",
              "                      0.0314, 0.0360, 0.0412, 0.0236, 0.0252, 0.0251, 0.0273, 0.0255, 0.0260,\n",
              "                      0.0358, 0.0350, 0.0354, 0.0618, 0.0458, 0.0530, 0.0341, 0.0513, 0.0364,\n",
              "                      0.0449], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.8.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  50,  53,  55,  52,  55,  56,  56,  57,  50,  56,  48,  51,  49,\n",
              "                       57,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.self_attn.q_proj.weight',\n",
              "              tensor([[ 62],\n",
              "                      [114],\n",
              "                      [ 74],\n",
              "                      ...,\n",
              "                      [158],\n",
              "                      [ 23],\n",
              "                      [ 51]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.self_attn.q_proj.bias',\n",
              "              tensor([-0.0288,  0.0439,  0.0167,  ..., -0.0039, -0.0297, -0.0110],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.self_attn.q_proj.weight.absmax',\n",
              "              tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.8.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0369, 0.0338, 0.0359, 0.0407, 0.0309, 0.0329, 0.0313, 0.0320, 0.0566,\n",
              "                      0.0589, 0.0534, 0.0575, 0.0416, 0.0316, 0.0402, 0.0404, 0.0353, 0.0361,\n",
              "                      0.0335, 0.0344, 0.0341, 0.0347, 0.0344, 0.0352, 0.0317, 0.0327, 0.0322,\n",
              "                      0.0358, 0.0312, 0.0311, 0.0339, 0.0335, 0.0335, 0.0314, 0.0364, 0.0308,\n",
              "                      0.0590, 0.0547, 0.0497, 0.0547, 0.0332, 0.0354, 0.0314, 0.0302, 0.0331,\n",
              "                      0.0371, 0.0336, 0.0349, 0.0328, 0.0321, 0.0318, 0.0338, 0.0317, 0.0325,\n",
              "                      0.0308, 0.0296, 0.0330, 0.0359, 0.0391, 0.0382, 0.0331, 0.0323, 0.0324,\n",
              "                      0.0310], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.8.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       57,  52,  48,  52,  57,  55,  53,  49,  55,  53,  56,  53,  55,  53,\n",
              "                       52,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.self_attn.out_proj.weight',\n",
              "              tensor([[104],\n",
              "                      [ 87],\n",
              "                      [130],\n",
              "                      ...,\n",
              "                      [ 67],\n",
              "                      [ 20],\n",
              "                      [ 25]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.self_attn.out_proj.bias',\n",
              "              tensor([ 0.0336, -0.0124, -0.0354,  ...,  0.0335, -0.0526,  0.0371],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.self_attn.out_proj.weight.absmax',\n",
              "              tensor([255,   0,   0,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.8.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0310, 0.0291, 0.0310, 0.0404, 0.0311, 0.0404, 0.0807, 0.0481, 0.0456,\n",
              "                      0.0332, 0.0310, 0.0318, 0.0327, 0.0301, 0.0426, 0.0376, 0.0330, 0.0333,\n",
              "                      0.0466, 0.0730, 0.0296, 0.0395, 0.0332, 0.0320, 0.0455, 0.0414, 0.0441,\n",
              "                      0.0434, 0.0434, 0.0377, 0.0367, 0.0371, 0.0363, 0.0324, 0.0344, 0.0330,\n",
              "                      0.0352, 0.0313, 0.0453, 0.0371, 0.0291, 0.0606, 0.0378, 0.0326, 0.0265,\n",
              "                      0.0481, 0.0351, 0.0282, 0.0313, 0.0578, 0.0391, 0.0326, 0.0289, 0.0646,\n",
              "                      0.0384, 0.0439, 0.0409, 0.0578, 0.0403, 0.0417, 0.0661, 0.0423, 0.0313,\n",
              "                      0.0311], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.8.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       53,  49,  57,  55,  49,  49,  48,  55,  55,  50,  49,  51,  50,  56,\n",
              "                       55,  51,  53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.self_attn_layer_norm.bias',\n",
              "              tensor([-0.0474, -0.0039,  0.0187,  ..., -0.0530,  0.0618, -0.0077],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.fc1.weight',\n",
              "              tensor([[137],\n",
              "                      [194],\n",
              "                      [246],\n",
              "                      ...,\n",
              "                      [167],\n",
              "                      [ 59],\n",
              "                      [167]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.fc1.bias',\n",
              "              tensor([-0.0272, -0.0168, -0.0273,  ..., -0.0130,  0.0089, -0.0001],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.fc1.weight.absmax',\n",
              "              tensor([255,   0,   0,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.8.fc1.weight.nested_absmax',\n",
              "              tensor([0.0469, 0.0453, 0.0389, 0.0399, 0.0381, 0.0421, 0.0403, 0.0368, 0.0453,\n",
              "                      0.0441, 0.0468, 0.0463, 0.0453, 0.0449, 0.0460, 0.0481, 0.0466, 0.0464,\n",
              "                      0.0436, 0.0439, 0.0458, 0.0394, 0.0436, 0.0432, 0.0420, 0.0377, 0.0466,\n",
              "                      0.0461, 0.0464, 0.0466, 0.0453, 0.0470, 0.0449, 0.0469, 0.0413, 0.0465,\n",
              "                      0.0442, 0.0448, 0.0450, 0.0373, 0.0469, 0.0403, 0.0316, 0.0411, 0.0456,\n",
              "                      0.0428, 0.0444, 0.0460, 0.0425, 0.0367, 0.0323, 0.0421, 0.0351, 0.0450,\n",
              "                      0.0348, 0.0447, 0.0452, 0.0442, 0.0474, 0.0439, 0.0465, 0.0419, 0.0449,\n",
              "                      0.0441, 0.0310, 0.0392, 0.0321, 0.0322, 0.0462, 0.0350, 0.0466, 0.0470,\n",
              "                      0.0360, 0.0414, 0.0464, 0.0472, 0.0422, 0.0454, 0.0433, 0.0452, 0.0448,\n",
              "                      0.0433, 0.0447, 0.0419, 0.0475, 0.0439, 0.0468, 0.0437, 0.0466, 0.0468,\n",
              "                      0.0375, 0.0449, 0.0410, 0.0428, 0.0450, 0.0459, 0.0466, 0.0455, 0.0391,\n",
              "                      0.0466, 0.0400, 0.0452, 0.0400, 0.0391, 0.0470, 0.0410, 0.0478, 0.0466,\n",
              "                      0.0466, 0.0391, 0.0422, 0.0468, 0.0347, 0.0321, 0.0458, 0.0427, 0.0466,\n",
              "                      0.0470, 0.0472, 0.0449, 0.0327, 0.0466, 0.0387, 0.0469, 0.0439, 0.0466,\n",
              "                      0.0373, 0.0468, 0.0481, 0.0466, 0.0477, 0.0462, 0.0423, 0.0422, 0.0422,\n",
              "                      0.0465, 0.0460, 0.0439, 0.0359, 0.0468, 0.0354, 0.0453, 0.0413, 0.0437,\n",
              "                      0.0460, 0.0427, 0.0422, 0.0414, 0.0337, 0.0453, 0.0446, 0.0461, 0.0463,\n",
              "                      0.0374, 0.0430, 0.0466, 0.0358, 0.0466, 0.0421, 0.0450, 0.0448, 0.0453,\n",
              "                      0.0464, 0.0455, 0.0395, 0.0463, 0.0444, 0.0460, 0.0422, 0.0446, 0.0464,\n",
              "                      0.0424, 0.0335, 0.0463, 0.0387, 0.0464, 0.0395, 0.0460, 0.0441, 0.0459,\n",
              "                      0.0393, 0.0475, 0.0406, 0.0435, 0.0432, 0.0466, 0.0455, 0.0369, 0.0376,\n",
              "                      0.0435, 0.0413, 0.0463, 0.0384, 0.0376, 0.0466, 0.0470, 0.0478, 0.0340,\n",
              "                      0.0425, 0.0468, 0.0458, 0.0455, 0.0414, 0.0453, 0.0327, 0.0366, 0.0455,\n",
              "                      0.0468, 0.0441, 0.0447, 0.0408, 0.0391, 0.0439, 0.0446, 0.0461, 0.0444,\n",
              "                      0.0434, 0.0397, 0.0454, 0.0468, 0.0470, 0.0436, 0.0370, 0.0466, 0.0445,\n",
              "                      0.0472, 0.0366, 0.0414, 0.0450, 0.0480, 0.0363, 0.0450, 0.0494, 0.0376,\n",
              "                      0.0430, 0.0424, 0.0442, 0.0459, 0.0399, 0.0373, 0.0405, 0.0458, 0.0424,\n",
              "                      0.0445, 0.0447, 0.0372, 0.0438, 0.0377, 0.0327, 0.0367, 0.0455, 0.0483,\n",
              "                      0.0448, 0.0465, 0.0397, 0.0435], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.8.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  56,  51,  55,  49,  56,  50,  50,  56,  51,  52,  48,  49,  52,\n",
              "                       56,  57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.fc2.weight',\n",
              "              tensor([[199],\n",
              "                      [113],\n",
              "                      [ 45],\n",
              "                      ...,\n",
              "                      [ 43],\n",
              "                      [ 61],\n",
              "                      [135]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.fc2.bias',\n",
              "              tensor([-0.0247, -0.0308, -0.0028,  ...,  0.0130, -0.0241,  0.0237],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.fc2.weight.absmax',\n",
              "              tensor([255,   0,   0,  ..., 255,   0,   1], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.8.fc2.weight.nested_absmax',\n",
              "              tensor([0.0576, 0.0561, 0.0577, 0.0585, 0.0585, 0.0582, 0.0585, 0.0484, 0.0551,\n",
              "                      0.0331, 0.0542, 0.0582, 0.0534, 0.0440, 0.0445, 0.0584, 0.0584, 0.0570,\n",
              "                      0.0587, 0.0561, 0.0585, 0.0536, 0.0588, 0.0441, 0.1835, 0.0475, 0.0578,\n",
              "                      0.0577, 0.0586, 0.0577, 0.0389, 0.0584, 0.0578, 0.0581, 0.0588, 0.0580,\n",
              "                      0.0582, 0.0585, 0.0471, 0.0483, 0.0450, 0.0476, 0.0566, 0.0434, 0.0516,\n",
              "                      0.0561, 0.0586, 0.0430, 0.0566, 0.0582, 0.0348, 0.0566, 0.0586, 0.0585,\n",
              "                      0.0576, 0.0582, 0.0585, 0.0567, 0.0468, 0.0605, 0.0553, 0.0538, 0.0389,\n",
              "                      0.0584, 0.0580, 0.0520, 0.0472, 0.0574, 0.0582, 0.0569, 0.0582, 0.0568,\n",
              "                      0.0470, 0.0582, 0.0574, 0.0581, 0.0585, 0.0442, 0.0572, 0.0581, 0.0581,\n",
              "                      0.0585, 0.0563, 0.0398, 0.0493, 0.0585, 0.0582, 0.0340, 0.0489, 0.0543,\n",
              "                      0.0524, 0.0581, 0.0470, 0.0582, 0.0587, 0.0567, 0.0583, 0.0566, 0.0374,\n",
              "                      0.0553, 0.0579, 0.0577, 0.0559, 0.0569, 0.0531, 0.0572, 0.0410, 0.0585,\n",
              "                      0.0573, 0.0582, 0.0592, 0.0481, 0.0550, 0.0423, 0.0385, 0.0552, 0.0413,\n",
              "                      0.0585, 0.0572, 0.0585, 0.0585, 0.0587, 0.0573, 0.0583, 0.0450, 0.0571,\n",
              "                      0.0538, 0.0577, 0.0566, 0.0436, 0.0524, 0.0555, 0.0445, 0.0552, 0.0450,\n",
              "                      0.0513, 0.0573, 0.0583, 0.0421, 0.0467, 0.0575, 0.0584, 0.0579, 0.0527,\n",
              "                      0.0506, 0.0572, 0.0583, 0.0574, 0.0434, 0.0583, 0.0585, 0.0571, 0.0585,\n",
              "                      0.0559, 0.0582, 0.0584, 0.0579, 0.0583, 0.0603, 0.0419, 0.0567, 0.0585,\n",
              "                      0.0575, 0.0583, 0.0534, 0.0470, 0.0538, 0.0585, 0.0591, 0.0437, 0.0345,\n",
              "                      0.0500, 0.0562, 0.0568, 0.0585, 0.0456, 0.0436, 0.0571, 0.0536, 0.0513,\n",
              "                      0.0459, 0.0577, 0.0577, 0.0585, 0.0558, 0.0555, 0.0470, 0.0572, 0.0555,\n",
              "                      0.0583, 0.0462, 0.0567, 0.0583, 0.0584, 0.0583, 0.0569, 0.0461, 0.0337,\n",
              "                      0.0585, 0.0402, 0.0503, 0.0579, 0.0546, 0.0583, 0.0455, 0.0439, 0.0514,\n",
              "                      0.0593, 0.0538, 0.0558, 0.0586, 0.0584, 0.0530, 0.0514, 0.0498, 0.0591,\n",
              "                      0.0580, 0.0584, 0.0582, 0.0494, 0.0564, 0.0584, 0.0583, 0.0587, 0.0583,\n",
              "                      0.0503, 0.0583, 0.0383, 0.0587, 0.0586, 0.0560, 0.0533, 0.0567, 0.0582,\n",
              "                      0.0582, 0.0585, 0.0486, 0.0588, 0.0587, 0.0570, 0.0559, 0.0580, 0.0584,\n",
              "                      0.0585, 0.0398, 0.0446, 0.0572, 0.0591, 0.0561, 0.0587, 0.0580, 0.0588,\n",
              "                      0.0586, 0.0539, 0.0390, 0.0561], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.8.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  54,  53,  52,  49,  53,  57,  55,  50,  52,  55,  49,  50,  51,\n",
              "                       55,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.8.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.8.final_layer_norm.bias',\n",
              "              tensor([ 0.0105, -0.0114, -0.0341,  ..., -0.0285, -0.0314,  0.0246],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.self_attn.k_proj.weight',\n",
              "              tensor([[249],\n",
              "                      [ 19],\n",
              "                      [ 36],\n",
              "                      ...,\n",
              "                      [ 59],\n",
              "                      [152],\n",
              "                      [ 77]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0873,  0.0250,  0.1159,  ...,  0.0134, -0.0312,  0.0081],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.self_attn.k_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.9.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0408, 0.0425, 0.0427, 0.0425, 0.0375, 0.0363, 0.0402, 0.0362, 0.0514,\n",
              "                      0.0518, 0.0493, 0.0516, 0.0327, 0.0434, 0.0334, 0.0393, 0.0368, 0.0352,\n",
              "                      0.0369, 0.0361, 0.0369, 0.0374, 0.0385, 0.0373, 0.0368, 0.0413, 0.0412,\n",
              "                      0.0410, 0.0366, 0.0360, 0.0367, 0.0342, 0.0315, 0.0398, 0.0391, 0.0408,\n",
              "                      0.0433, 0.0465, 0.0443, 0.0394, 0.0343, 0.0365, 0.0386, 0.0373, 0.0374,\n",
              "                      0.0413, 0.0405, 0.0391, 0.0400, 0.0397, 0.0400, 0.0380, 0.0430, 0.0419,\n",
              "                      0.0451, 0.0418, 0.0373, 0.0364, 0.0432, 0.0369, 0.0563, 0.0407, 0.0453,\n",
              "                      0.0389], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.9.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       57,  49,  57,  55,  50,  50,  51,  57,  51,  49,  53,  53,  48,  57,\n",
              "                       56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.self_attn.v_proj.weight',\n",
              "              tensor([[117],\n",
              "                      [ 25],\n",
              "                      [ 88],\n",
              "                      ...,\n",
              "                      [135],\n",
              "                      [104],\n",
              "                      [ 52]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.self_attn.v_proj.bias',\n",
              "              tensor([-3.1250e-02,  9.1076e-05, -4.9500e-02,  ..., -1.6953e-02,\n",
              "                      -2.4734e-02, -9.7656e-03], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.self_attn.v_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ..., 255, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.9.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0375, 0.0352, 0.0386, 0.0365, 0.0303, 0.0268, 0.0248, 0.0239, 0.0372,\n",
              "                      0.0554, 0.0337, 0.0449, 0.0564, 0.0512, 0.0387, 0.0361, 0.0394, 0.0297,\n",
              "                      0.0450, 0.0445, 0.0375, 0.0355, 0.0377, 0.0363, 0.0250, 0.0330, 0.0369,\n",
              "                      0.0304, 0.0295, 0.0458, 0.0337, 0.0290, 0.0383, 0.0447, 0.0432, 0.0409,\n",
              "                      0.0425, 0.0375, 0.0353, 0.0292, 0.0406, 0.0406, 0.0394, 0.0296, 0.0351,\n",
              "                      0.0351, 0.0356, 0.0393, 0.0327, 0.0305, 0.0276, 0.0283, 0.0339, 0.0259,\n",
              "                      0.0378, 0.0443, 0.0263, 0.0411, 0.0395, 0.0374, 0.0344, 0.0359, 0.0314,\n",
              "                      0.0405], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.9.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  49,  56,  57,  51,  57,  51,  57,  57,  55,  49,  57,  50,  51,\n",
              "                       56,  51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.self_attn.q_proj.weight',\n",
              "              tensor([[174],\n",
              "                      [192],\n",
              "                      [241],\n",
              "                      ...,\n",
              "                      [171],\n",
              "                      [169],\n",
              "                      [235]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.self_attn.q_proj.bias',\n",
              "              tensor([ 0.0605, -0.0439,  0.0920,  ...,  0.0637,  0.0003,  0.0603],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.self_attn.q_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.9.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0405, 0.0440, 0.0397, 0.0400, 0.0324, 0.0331, 0.0337, 0.0344, 0.0489,\n",
              "                      0.0417, 0.0456, 0.0429, 0.0300, 0.0311, 0.0308, 0.0402, 0.0368, 0.0352,\n",
              "                      0.0317, 0.0313, 0.0319, 0.0326, 0.0329, 0.0320, 0.0328, 0.0315, 0.0313,\n",
              "                      0.0314, 0.0335, 0.0342, 0.0324, 0.0335, 0.0309, 0.0371, 0.0375, 0.0364,\n",
              "                      0.0448, 0.0435, 0.0399, 0.0424, 0.0360, 0.0360, 0.0329, 0.0346, 0.0361,\n",
              "                      0.0361, 0.0379, 0.0405, 0.0334, 0.0325, 0.0341, 0.0350, 0.0359, 0.0376,\n",
              "                      0.0375, 0.0435, 0.0337, 0.0360, 0.0347, 0.0415, 0.0343, 0.0393, 0.0441,\n",
              "                      0.0357], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.9.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       57,  51,  57,  51,  56,  54,  57,  51,  52,  48,  52,  49,  57,  55,\n",
              "                       54,  57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.self_attn.out_proj.weight',\n",
              "              tensor([[ 55],\n",
              "                      [ 28],\n",
              "                      [218],\n",
              "                      ...,\n",
              "                      [230],\n",
              "                      [103],\n",
              "                      [122]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.self_attn.out_proj.bias',\n",
              "              tensor([ 0.0206,  0.0295,  0.0018,  ...,  0.0408, -0.0535,  0.0327],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.self_attn.out_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.9.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0322, 0.0348, 0.0359, 0.0360, 0.0476, 0.0407, 0.0956, 0.0523, 0.0383,\n",
              "                      0.0576, 0.0533, 0.0371, 0.0352, 0.0421, 0.0332, 0.0338, 0.0336, 0.0342,\n",
              "                      0.0382, 0.0709, 0.0426, 0.0402, 0.0362, 0.0379, 0.0403, 0.0396, 0.0465,\n",
              "                      0.0391, 0.0348, 0.0341, 0.0294, 0.0288, 0.0342, 0.0359, 0.0327, 0.0320,\n",
              "                      0.0445, 0.0366, 0.0706, 0.0319, 0.0385, 0.0493, 0.0345, 0.0337, 0.0406,\n",
              "                      0.0456, 0.0338, 0.0394, 0.0391, 0.0320, 0.0354, 0.0442, 0.0351, 0.0319,\n",
              "                      0.0328, 0.0329, 0.0335, 0.0704, 0.0370, 0.0362, 0.0318, 0.0374, 0.0261,\n",
              "                      0.0412], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.9.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       53,  52,  48,  53,  51,  53,  51,  55,  53,  52,  55,  53,  56,  56,\n",
              "                       51,  53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.self_attn_layer_norm.bias',\n",
              "              tensor([-0.0533, -0.0209,  0.0026,  ..., -0.0640,  0.0645, -0.0345],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.fc1.weight',\n",
              "              tensor([[166],\n",
              "                      [161],\n",
              "                      [ 75],\n",
              "                      ...,\n",
              "                      [135],\n",
              "                      [197],\n",
              "                      [118]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.fc1.bias',\n",
              "              tensor([-0.0289, -0.0027, -0.0043,  ..., -0.0488, -0.0098, -0.0248],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.fc1.weight.absmax',\n",
              "              tensor([255, 255, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.9.fc1.weight.nested_absmax',\n",
              "              tensor([0.0348, 0.0476, 0.0424, 0.0414, 0.0388, 0.0505, 0.0489, 0.0471, 0.0460,\n",
              "                      0.0460, 0.0493, 0.0285, 0.0477, 0.0378, 0.0389, 0.0367, 0.0492, 0.0442,\n",
              "                      0.0372, 0.0423, 0.0494, 0.0426, 0.0453, 0.0429, 0.0363, 0.0350, 0.0388,\n",
              "                      0.0464, 0.0429, 0.0372, 0.0348, 0.0321, 0.0438, 0.0477, 0.0466, 0.0482,\n",
              "                      0.0363, 0.0487, 0.0409, 0.0482, 0.0446, 0.0449, 0.0493, 0.0447, 0.0344,\n",
              "                      0.0419, 0.0366, 0.0482, 0.0504, 0.0485, 0.0491, 0.0466, 0.0397, 0.0492,\n",
              "                      0.0456, 0.0491, 0.0361, 0.0405, 0.0369, 0.0432, 0.0346, 0.0493, 0.0494,\n",
              "                      0.0355, 0.0394, 0.0447, 0.0390, 0.0407, 0.0450, 0.0380, 0.0486, 0.0445,\n",
              "                      0.0388, 0.0449, 0.0484, 0.0445, 0.0493, 0.0465, 0.0496, 0.0489, 0.0471,\n",
              "                      0.0414, 0.0489, 0.0371, 0.0376, 0.0420, 0.0405, 0.0441, 0.0492, 0.0493,\n",
              "                      0.0485, 0.0404, 0.0477, 0.0493, 0.0508, 0.0325, 0.0319, 0.0497, 0.0470,\n",
              "                      0.0466, 0.0486, 0.0488, 0.0493, 0.0492, 0.0493, 0.0364, 0.0453, 0.0494,\n",
              "                      0.0463, 0.0323, 0.0459, 0.0373, 0.0475, 0.0400, 0.0453, 0.0296, 0.0499,\n",
              "                      0.0392, 0.0494, 0.0438, 0.0373, 0.0448, 0.0394, 0.0466, 0.0491, 0.0435,\n",
              "                      0.0476, 0.0491, 0.0292, 0.0424, 0.0484, 0.0344, 0.0491, 0.0433, 0.0433,\n",
              "                      0.0332, 0.0359, 0.0490, 0.0444, 0.0453, 0.0421, 0.0407, 0.0422, 0.0444,\n",
              "                      0.0403, 0.0492, 0.0313, 0.0377, 0.0362, 0.0426, 0.0486, 0.0486, 0.0366,\n",
              "                      0.0411, 0.0475, 0.0443, 0.0496, 0.0437, 0.0463, 0.0398, 0.0384, 0.0387,\n",
              "                      0.0469, 0.0490, 0.0466, 0.0419, 0.0402, 0.0347, 0.0489, 0.0490, 0.0454,\n",
              "                      0.0384, 0.0312, 0.0383, 0.0494, 0.0488, 0.0464, 0.0289, 0.0455, 0.0362,\n",
              "                      0.0433, 0.0422, 0.0491, 0.0479, 0.0445, 0.0494, 0.0419, 0.0431, 0.0451,\n",
              "                      0.0489, 0.0456, 0.0466, 0.0478, 0.0447, 0.0498, 0.0413, 0.0464, 0.0438,\n",
              "                      0.0459, 0.0491, 0.0490, 0.0348, 0.0421, 0.0492, 0.0427, 0.0353, 0.0369,\n",
              "                      0.0477, 0.0329, 0.0369, 0.0480, 0.0393, 0.0335, 0.0360, 0.0328, 0.0387,\n",
              "                      0.0493, 0.0485, 0.0422, 0.0472, 0.0463, 0.0477, 0.0482, 0.0460, 0.0489,\n",
              "                      0.0399, 0.0310, 0.0409, 0.0416, 0.0353, 0.0437, 0.0488, 0.0369, 0.0413,\n",
              "                      0.0433, 0.0391, 0.0492, 0.0413, 0.0317, 0.0373, 0.0413, 0.0412, 0.0493,\n",
              "                      0.0394, 0.0442, 0.0456, 0.0439, 0.0447, 0.0411, 0.0378, 0.0457, 0.0441,\n",
              "                      0.0472, 0.0380, 0.0484, 0.0398], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.9.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  53,  54,  56,  57,  56,  50,  50,  52,  51,  53,  51,  55,  57,\n",
              "                       48,  51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.fc2.weight',\n",
              "              tensor([[ 26],\n",
              "                      [119],\n",
              "                      [228],\n",
              "                      ...,\n",
              "                      [245],\n",
              "                      [ 23],\n",
              "                      [ 78]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.fc2.bias',\n",
              "              tensor([ 0.0310,  0.0064,  0.0461,  ...,  0.0417, -0.0529,  0.0497],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.fc2.weight.absmax',\n",
              "              tensor([255, 255, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.9.fc2.weight.nested_absmax',\n",
              "              tensor([0.0534, 0.0615, 0.0621, 0.0464, 0.0357, 0.0395, 0.0594, 0.0620, 0.0614,\n",
              "                      0.0362, 0.0576, 0.0562, 0.0621, 0.0615, 0.0369, 0.0538, 0.0509, 0.0620,\n",
              "                      0.0340, 0.0589, 0.0615, 0.0480, 0.0623, 0.0438, 0.1871, 0.0604, 0.0540,\n",
              "                      0.0615, 0.0621, 0.0611, 0.0548, 0.0623, 0.0596, 0.0621, 0.0370, 0.0621,\n",
              "                      0.0618, 0.0618, 0.0407, 0.0507, 0.0607, 0.0388, 0.0509, 0.0611, 0.0456,\n",
              "                      0.0626, 0.0634, 0.0429, 0.0545, 0.0565, 0.0571, 0.0406, 0.0457, 0.0557,\n",
              "                      0.0590, 0.0410, 0.0618, 0.0546, 0.0477, 0.0628, 0.0492, 0.0425, 0.0349,\n",
              "                      0.0396, 0.0526, 0.0400, 0.0339, 0.0604, 0.0620, 0.0621, 0.0515, 0.0396,\n",
              "                      0.0394, 0.0476, 0.0584, 0.0623, 0.0478, 0.0372, 0.0529, 0.0420, 0.0590,\n",
              "                      0.0609, 0.0621, 0.0448, 0.0612, 0.0613, 0.0613, 0.0621, 0.0515, 0.0505,\n",
              "                      0.0621, 0.0608, 0.0369, 0.0587, 0.0527, 0.0530, 0.0472, 0.0344, 0.0373,\n",
              "                      0.0562, 0.0620, 0.0512, 0.0612, 0.0457, 0.0463, 0.0590, 0.0455, 0.0621,\n",
              "                      0.0617, 0.0618, 0.0316, 0.0614, 0.0409, 0.0603, 0.0578, 0.0620, 0.0548,\n",
              "                      0.0344, 0.0520, 0.0473, 0.0376, 0.0559, 0.0620, 0.0386, 0.0612, 0.0415,\n",
              "                      0.0566, 0.0610, 0.0618, 0.0437, 0.0621, 0.0434, 0.0427, 0.0581, 0.0621,\n",
              "                      0.0621, 0.0424, 0.0554, 0.0436, 0.0380, 0.0621, 0.0510, 0.0621, 0.0623,\n",
              "                      0.0365, 0.0623, 0.0520, 0.0482, 0.0453, 0.0587, 0.0621, 0.0599, 0.0332,\n",
              "                      0.0479, 0.0488, 0.0624, 0.0454, 0.0529, 0.0618, 0.0424, 0.0553, 0.0494,\n",
              "                      0.0621, 0.0389, 0.0617, 0.0590, 0.0416, 0.0621, 0.0593, 0.0368, 0.0618,\n",
              "                      0.0586, 0.0621, 0.0617, 0.0524, 0.0281, 0.0555, 0.0621, 0.0553, 0.0547,\n",
              "                      0.0487, 0.0621, 0.0494, 0.0492, 0.0368, 0.0620, 0.0621, 0.0497, 0.0621,\n",
              "                      0.0292, 0.0537, 0.0575, 0.0621, 0.0324, 0.0522, 0.0614, 0.0628, 0.0543,\n",
              "                      0.0479, 0.0510, 0.0355, 0.0615, 0.0455, 0.0621, 0.0507, 0.0533, 0.0481,\n",
              "                      0.0621, 0.0376, 0.0610, 0.0609, 0.0433, 0.0509, 0.0542, 0.0581, 0.0495,\n",
              "                      0.0434, 0.0457, 0.0497, 0.0615, 0.0620, 0.0618, 0.0612, 0.0319, 0.0609,\n",
              "                      0.0602, 0.0610, 0.0612, 0.0621, 0.0621, 0.0459, 0.0508, 0.0431, 0.0447,\n",
              "                      0.0486, 0.0548, 0.0497, 0.0621, 0.0584, 0.0439, 0.0618, 0.0454, 0.0456,\n",
              "                      0.0452, 0.0540, 0.0621, 0.0615, 0.0427, 0.0608, 0.0607, 0.0620, 0.0587,\n",
              "                      0.0609, 0.0621, 0.0334, 0.0621], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.9.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  50,  56,  53,  48,  49,  57,  50,  49,  56,  57,  50,  49,  54,\n",
              "                       54,  49, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.9.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.9.final_layer_norm.bias',\n",
              "              tensor([ 0.0273,  0.0064, -0.0334,  ..., -0.0321, -0.0468,  0.0132],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.self_attn.k_proj.weight',\n",
              "              tensor([[216],\n",
              "                      [233],\n",
              "                      [114],\n",
              "                      ...,\n",
              "                      [  2],\n",
              "                      [181],\n",
              "                      [ 51]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0318,  0.0334, -0.0183,  ..., -0.0399, -0.0200, -0.0077],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.10.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0365, 0.0362, 0.0372, 0.0363, 0.0388, 0.0389, 0.0431, 0.0381, 0.0383,\n",
              "                      0.0398, 0.0394, 0.0378, 0.0362, 0.0435, 0.0367, 0.0368, 0.0489, 0.0417,\n",
              "                      0.0439, 0.0419, 0.0375, 0.0363, 0.0367, 0.0367, 0.0370, 0.0368, 0.0368,\n",
              "                      0.0383, 0.0378, 0.0379, 0.0381, 0.0372, 0.0368, 0.0366, 0.0349, 0.0363,\n",
              "                      0.0359, 0.0362, 0.0362, 0.0362, 0.0365, 0.0372, 0.0384, 0.0373, 0.0365,\n",
              "                      0.0365, 0.0370, 0.0363, 0.0413, 0.0398, 0.0398, 0.0359, 0.0345, 0.0354,\n",
              "                      0.0341, 0.0372, 0.0326, 0.0366, 0.0352, 0.0359, 0.0338, 0.0401, 0.0445,\n",
              "                      0.0391], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.10.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  56,  55,  55,  56,  52,  56,  56,  51,  51,  55,  57,  57,  51,\n",
              "                       54,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.self_attn.v_proj.weight',\n",
              "              tensor([[140],\n",
              "                      [ 86],\n",
              "                      [230],\n",
              "                      ...,\n",
              "                      [ 28],\n",
              "                      [131],\n",
              "                      [ 22]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.self_attn.v_proj.bias',\n",
              "              tensor([ 0.0244,  0.0083,  0.0042,  ..., -0.0061,  0.0239,  0.0252],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.self_attn.v_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.10.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0301, 0.0453, 0.0443, 0.0505, 0.0268, 0.0286, 0.0344, 0.0238, 0.0360,\n",
              "                      0.0297, 0.0304, 0.0298, 0.0362, 0.0299, 0.0377, 0.0333, 0.0502, 0.0452,\n",
              "                      0.0590, 0.0487, 0.0460, 0.0491, 0.0423, 0.0365, 0.0319, 0.0474, 0.0427,\n",
              "                      0.0456, 0.0278, 0.0304, 0.0269, 0.0404, 0.0329, 0.0371, 0.0266, 0.0268,\n",
              "                      0.0377, 0.0290, 0.0200, 0.0329, 0.0294, 0.0372, 0.0321, 0.0311, 0.0339,\n",
              "                      0.0307, 0.0348, 0.0480, 0.0320, 0.0416, 0.0514, 0.0437, 0.0495, 0.0377,\n",
              "                      0.0372, 0.0431, 0.0307, 0.0352, 0.0291, 0.0246, 0.0409, 0.0436, 0.0350,\n",
              "                      0.0376], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.10.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  53,  48,  52,  49,  55,  50,  48,  56,  54,  55,  49,  53,  54,\n",
              "                       57,  56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.self_attn.q_proj.weight',\n",
              "              tensor([[ 80],\n",
              "                      [226],\n",
              "                      [231],\n",
              "                      ...,\n",
              "                      [161],\n",
              "                      [165],\n",
              "                      [ 41]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.self_attn.q_proj.bias',\n",
              "              tensor([-0.0643, -0.0724,  0.0140,  ...,  0.0069, -0.0454, -0.0042],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.self_attn.q_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.10.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0344, 0.0410, 0.0381, 0.0328, 0.0351, 0.0385, 0.0349, 0.0347, 0.0385,\n",
              "                      0.0372, 0.0393, 0.0404, 0.0344, 0.0341, 0.0346, 0.0329, 0.0455, 0.0408,\n",
              "                      0.0462, 0.0367, 0.0289, 0.0339, 0.0351, 0.0346, 0.0357, 0.0347, 0.0355,\n",
              "                      0.0357, 0.0370, 0.0358, 0.0353, 0.0349, 0.0341, 0.0404, 0.0344, 0.0333,\n",
              "                      0.0342, 0.0340, 0.0359, 0.0343, 0.0363, 0.0353, 0.0355, 0.0359, 0.0355,\n",
              "                      0.0343, 0.0344, 0.0344, 0.0353, 0.0335, 0.0325, 0.0367, 0.0292, 0.0346,\n",
              "                      0.0342, 0.0351, 0.0343, 0.0334, 0.0339, 0.0341, 0.0289, 0.0322, 0.0312,\n",
              "                      0.0329], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.10.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       57,  48,  54,  56,  48,  49,  55,  52,  53,  50,  57,  53,  53,  50,\n",
              "                       52,  54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.self_attn.out_proj.weight',\n",
              "              tensor([[142],\n",
              "                      [150],\n",
              "                      [ 78],\n",
              "                      ...,\n",
              "                      [ 71],\n",
              "                      [  2],\n",
              "                      [ 81]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.self_attn.out_proj.bias',\n",
              "              tensor([ 0.0020,  0.0649,  0.0165,  ...,  0.0625,  0.0303, -0.0019],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.self_attn.out_proj.weight.absmax',\n",
              "              tensor([255,   0,   0,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.10.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0365, 0.0463, 0.0408, 0.0326, 0.0448, 0.0448, 0.0705, 0.0529, 0.0488,\n",
              "                      0.0387, 0.0608, 0.0693, 0.0504, 0.0413, 0.0684, 0.0506, 0.0373, 0.0436,\n",
              "                      0.0412, 0.0493, 0.0359, 0.0488, 0.0319, 0.0457, 0.0471, 0.0407, 0.0492,\n",
              "                      0.0504, 0.0481, 0.0448, 0.0338, 0.0339, 0.0422, 0.0418, 0.0336, 0.0390,\n",
              "                      0.0556, 0.0398, 0.0565, 0.0308, 0.0349, 0.0290, 0.0405, 0.0521, 0.0318,\n",
              "                      0.0464, 0.0487, 0.0305, 0.0375, 0.0497, 0.0448, 0.0381, 0.0399, 0.0329,\n",
              "                      0.0338, 0.0350, 0.0351, 0.0530, 0.0526, 0.0415, 0.0323, 0.0555, 0.0535,\n",
              "                      0.0471], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.10.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  49,  53,  54,  53,  52,  56,  49,  49,  50,  54,  51,  48,  56,\n",
              "                       52,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.self_attn_layer_norm.bias',\n",
              "              tensor([-0.0363, -0.0224, -0.0283,  ..., -0.0659,  0.0410, -0.0161],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.fc1.weight',\n",
              "              tensor([[106],\n",
              "                      [122],\n",
              "                      [250],\n",
              "                      ...,\n",
              "                      [116],\n",
              "                      [171],\n",
              "                      [110]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.fc1.bias',\n",
              "              tensor([-0.0117, -0.0617, -0.0588,  ..., -0.0289, -0.0203, -0.0036],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.fc1.weight.absmax',\n",
              "              tensor([255,   0,   0,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.10.fc1.weight.nested_absmax',\n",
              "              tensor([0.0533, 0.0485, 0.0484, 0.0491, 0.0435, 0.0474, 0.0417, 0.0485, 0.0483,\n",
              "                      0.0364, 0.0429, 0.0441, 0.0466, 0.0453, 0.0481, 0.0481, 0.0465, 0.0468,\n",
              "                      0.0433, 0.0485, 0.0446, 0.0487, 0.0474, 0.0451, 0.0335, 0.0433, 0.0428,\n",
              "                      0.0479, 0.0374, 0.0478, 0.0470, 0.0388, 0.0485, 0.0438, 0.0462, 0.0321,\n",
              "                      0.0476, 0.0475, 0.0478, 0.0489, 0.0348, 0.0469, 0.0486, 0.0462, 0.0462,\n",
              "                      0.0427, 0.0419, 0.0483, 0.0487, 0.0425, 0.0418, 0.0375, 0.0357, 0.0490,\n",
              "                      0.0481, 0.0458, 0.0438, 0.0441, 0.0475, 0.0449, 0.0486, 0.0460, 0.0334,\n",
              "                      0.0384, 0.0488, 0.0430, 0.0444, 0.0413, 0.0456, 0.0488, 0.0479, 0.0301,\n",
              "                      0.0483, 0.0328, 0.0486, 0.0485, 0.0465, 0.0404, 0.0468, 0.0454, 0.0348,\n",
              "                      0.0489, 0.0441, 0.0416, 0.0364, 0.0431, 0.0482, 0.0331, 0.0326, 0.0343,\n",
              "                      0.0375, 0.0461, 0.0444, 0.0487, 0.0505, 0.0490, 0.0314, 0.0491, 0.0439,\n",
              "                      0.0488, 0.0485, 0.0403, 0.0455, 0.0485, 0.0282, 0.0407, 0.0430, 0.0414,\n",
              "                      0.0322, 0.0477, 0.0449, 0.0484, 0.0473, 0.0473, 0.0435, 0.0436, 0.0478,\n",
              "                      0.0470, 0.0388, 0.0463, 0.0449, 0.0429, 0.0479, 0.0335, 0.0436, 0.0484,\n",
              "                      0.0488, 0.0423, 0.0411, 0.0493, 0.0488, 0.0493, 0.0402, 0.0403, 0.0474,\n",
              "                      0.0377, 0.0486, 0.0369, 0.0431, 0.0444, 0.0506, 0.0437, 0.0465, 0.0400,\n",
              "                      0.0486, 0.0433, 0.0375, 0.0484, 0.0482, 0.0482, 0.0490, 0.0449, 0.0410,\n",
              "                      0.0467, 0.0478, 0.0328, 0.0430, 0.0483, 0.0457, 0.0413, 0.0501, 0.0467,\n",
              "                      0.0451, 0.0482, 0.0358, 0.0370, 0.0435, 0.0482, 0.0479, 0.0401, 0.0447,\n",
              "                      0.0465, 0.0391, 0.0391, 0.0396, 0.0488, 0.0411, 0.0484, 0.0440, 0.0476,\n",
              "                      0.0489, 0.0380, 0.0462, 0.0408, 0.0390, 0.0374, 0.0487, 0.0487, 0.0365,\n",
              "                      0.0471, 0.0460, 0.0476, 0.0488, 0.0436, 0.0474, 0.0410, 0.0350, 0.0477,\n",
              "                      0.0410, 0.0415, 0.0469, 0.0424, 0.0487, 0.0348, 0.0399, 0.0363, 0.0361,\n",
              "                      0.0351, 0.0435, 0.0476, 0.0482, 0.0482, 0.0485, 0.0474, 0.0454, 0.0431,\n",
              "                      0.0419, 0.0407, 0.0429, 0.0402, 0.0414, 0.0469, 0.0465, 0.0444, 0.0454,\n",
              "                      0.0490, 0.0481, 0.0480, 0.0485, 0.0444, 0.0482, 0.0447, 0.0371, 0.0289,\n",
              "                      0.0366, 0.0432, 0.0456, 0.0375, 0.0468, 0.0402, 0.0473, 0.0493, 0.0469,\n",
              "                      0.0484, 0.0393, 0.0487, 0.0479, 0.0468, 0.0474, 0.0431, 0.0437, 0.0393,\n",
              "                      0.0487, 0.0482, 0.0402, 0.0486], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.10.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  54,  51,  52,  53,  50,  53,  48,  48,  49,  48,  52,  57,  48,\n",
              "                       52,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.fc2.weight',\n",
              "              tensor([[145],\n",
              "                      [ 74],\n",
              "                      [ 19],\n",
              "                      ...,\n",
              "                      [157],\n",
              "                      [122],\n",
              "                      [146]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.fc2.bias',\n",
              "              tensor([ 0.0178,  0.0102,  0.0103,  ...,  0.0626, -0.0428,  0.0316],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.fc2.weight.absmax',\n",
              "              tensor([255,   0,   9,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.10.fc2.weight.nested_absmax',\n",
              "              tensor([0.0614, 0.0612, 0.0606, 0.0553, 0.0577, 0.0609, 0.0395, 0.0608, 0.0618,\n",
              "                      0.0614, 0.0417, 0.0396, 0.0506, 0.0392, 0.0609, 0.0524, 0.0423, 0.0365,\n",
              "                      0.0605, 0.0609, 0.0539, 0.0318, 0.0487, 0.0617, 0.1864, 0.0440, 0.0610,\n",
              "                      0.0408, 0.0582, 0.0559, 0.0354, 0.0534, 0.0528, 0.0517, 0.0613, 0.0594,\n",
              "                      0.0526, 0.0266, 0.0406, 0.0453, 0.0612, 0.0564, 0.0490, 0.0359, 0.0520,\n",
              "                      0.0609, 0.0830, 0.0561, 0.0381, 0.0527, 0.0448, 0.0586, 0.0393, 0.0606,\n",
              "                      0.0617, 0.0412, 0.0602, 0.0614, 0.0429, 0.0645, 0.0465, 0.0319, 0.0593,\n",
              "                      0.0580, 0.0606, 0.0497, 0.0540, 0.0611, 0.0551, 0.0517, 0.0564, 0.0603,\n",
              "                      0.0407, 0.0422, 0.0482, 0.0589, 0.0613, 0.0562, 0.0561, 0.0516, 0.0395,\n",
              "                      0.0616, 0.0618, 0.0606, 0.0398, 0.0532, 0.0504, 0.0545, 0.0430, 0.0525,\n",
              "                      0.0553, 0.0607, 0.0452, 0.0587, 0.0562, 0.0498, 0.0489, 0.0607, 0.0583,\n",
              "                      0.0603, 0.0472, 0.0464, 0.0508, 0.0397, 0.0613, 0.0437, 0.0399, 0.0587,\n",
              "                      0.0600, 0.0595, 0.0500, 0.0401, 0.0437, 0.0426, 0.0453, 0.0408, 0.0392,\n",
              "                      0.0429, 0.0598, 0.0593, 0.0591, 0.0509, 0.0606, 0.0530, 0.0421, 0.0446,\n",
              "                      0.0340, 0.0604, 0.0396, 0.0443, 0.0605, 0.0613, 0.0608, 0.0609, 0.0572,\n",
              "                      0.0493, 0.0467, 0.0544, 0.0498, 0.0487, 0.0444, 0.0558, 0.0609, 0.0559,\n",
              "                      0.0529, 0.0522, 0.0614, 0.0616, 0.0313, 0.0570, 0.0499, 0.0423, 0.0510,\n",
              "                      0.0486, 0.0606, 0.0616, 0.0602, 0.0560, 0.0459, 0.0484, 0.0610, 0.0607,\n",
              "                      0.0473, 0.0622, 0.0567, 0.0414, 0.0572, 0.0543, 0.0449, 0.0566, 0.0371,\n",
              "                      0.0608, 0.0613, 0.0613, 0.0570, 0.0465, 0.0612, 0.0624, 0.0571, 0.0429,\n",
              "                      0.0557, 0.0442, 0.0502, 0.0614, 0.0611, 0.0436, 0.0579, 0.0586, 0.0614,\n",
              "                      0.0361, 0.0559, 0.0620, 0.0434, 0.0595, 0.0347, 0.0441, 0.0595, 0.0592,\n",
              "                      0.0529, 0.0589, 0.0583, 0.0609, 0.0486, 0.0614, 0.0609, 0.0447, 0.0511,\n",
              "                      0.0424, 0.0611, 0.0359, 0.0619, 0.0473, 0.0616, 0.0404, 0.0440, 0.0611,\n",
              "                      0.0614, 0.0594, 0.0619, 0.0614, 0.0494, 0.0584, 0.0567, 0.0361, 0.0605,\n",
              "                      0.0602, 0.0614, 0.0370, 0.0618, 0.0526, 0.0554, 0.0381, 0.0491, 0.0414,\n",
              "                      0.0606, 0.0614, 0.0523, 0.0564, 0.0609, 0.0486, 0.0360, 0.0620, 0.0612,\n",
              "                      0.0567, 0.0410, 0.0316, 0.0580, 0.0616, 0.0560, 0.0614, 0.0330, 0.0512,\n",
              "                      0.0517, 0.0539, 0.0432, 0.0614], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.10.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  51,  53,  54,  53,  51,  50,  56,  55,  49,  55,  50,  51,  49,\n",
              "                       55,  53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.10.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.10.final_layer_norm.bias',\n",
              "              tensor([ 0.0122, -0.0153, -0.0184,  ..., -0.0412, -0.0617,  0.0339],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.self_attn.k_proj.weight',\n",
              "              tensor([[250],\n",
              "                      [ 53],\n",
              "                      [181],\n",
              "                      ...,\n",
              "                      [232],\n",
              "                      [180],\n",
              "                      [ 74]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.self_attn.k_proj.bias',\n",
              "              tensor([0.0161, 0.0610, 0.0381,  ..., 0.0327, 0.0625, 0.0326], device='cuda:0',\n",
              "                     dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.11.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0477, 0.0451, 0.0418, 0.0419, 0.0411, 0.0411, 0.0415, 0.0410, 0.0411,\n",
              "                      0.0415, 0.0398, 0.0417, 0.0438, 0.0418, 0.0428, 0.0416, 0.0411, 0.0396,\n",
              "                      0.0412, 0.0390, 0.0438, 0.0415, 0.0426, 0.0417, 0.0435, 0.0430, 0.0421,\n",
              "                      0.0427, 0.0366, 0.0390, 0.0389, 0.0393, 0.0399, 0.0409, 0.0412, 0.0417,\n",
              "                      0.0436, 0.0406, 0.0399, 0.0415, 0.0411, 0.0410, 0.0367, 0.0347, 0.0407,\n",
              "                      0.0422, 0.0324, 0.0407, 0.0355, 0.0409, 0.0356, 0.0405, 0.0395, 0.0393,\n",
              "                      0.0377, 0.0410, 0.0397, 0.0412, 0.0397, 0.0402, 0.0471, 0.0461, 0.0466,\n",
              "                      0.0492], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.11.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  51,  57,  48,  53,  50,  53,  55,  50,  56,  52,  54,  52,  49,\n",
              "                       50,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.self_attn.v_proj.weight',\n",
              "              tensor([[195],\n",
              "                      [ 69],\n",
              "                      [ 89],\n",
              "                      ...,\n",
              "                      [120],\n",
              "                      [ 45],\n",
              "                      [106]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.self_attn.v_proj.bias',\n",
              "              tensor([-0.0006,  0.0124, -0.0245,  ...,  0.0222, -0.0074, -0.0051],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.self_attn.v_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ..., 255, 126,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.11.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0433, 0.0503, 0.0493, 0.0381, 0.0270, 0.0454, 0.0303, 0.0335, 0.0345,\n",
              "                      0.0344, 0.0301, 0.0300, 0.0379, 0.0311, 0.0274, 0.0308, 0.0307, 0.0481,\n",
              "                      0.0418, 0.0423, 0.0399, 0.0447, 0.0429, 0.0442, 0.0291, 0.0291, 0.0277,\n",
              "                      0.0287, 0.0315, 0.0327, 0.0292, 0.0318, 0.0297, 0.0324, 0.0369, 0.0289,\n",
              "                      0.0396, 0.0379, 0.0429, 0.0371, 0.0400, 0.0466, 0.0422, 0.0332, 0.0514,\n",
              "                      0.0359, 0.0330, 0.0337, 0.0472, 0.0346, 0.0418, 0.0443, 0.0400, 0.0334,\n",
              "                      0.0364, 0.0334, 0.0305, 0.0365, 0.0412, 0.0477, 0.0430, 0.0550, 0.0308,\n",
              "                      0.0379], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.11.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  56,  51,  49,  50,  51,  57,  49,  54,  51,  56,  55,  53,  53,\n",
              "                       56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.self_attn.q_proj.weight',\n",
              "              tensor([[178],\n",
              "                      [ 11],\n",
              "                      [140],\n",
              "                      ...,\n",
              "                      [ 54],\n",
              "                      [203],\n",
              "                      [109]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.self_attn.q_proj.bias',\n",
              "              tensor([0.0526, 0.0660, 0.0311,  ..., 0.0715, 0.0090, 0.0011], device='cuda:0',\n",
              "                     dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.self_attn.q_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ...,   0, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.11.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0407, 0.0402, 0.0402, 0.0400, 0.0316, 0.0321, 0.0402, 0.0328, 0.0402,\n",
              "                      0.0402, 0.0408, 0.0369, 0.0402, 0.0406, 0.0407, 0.0410, 0.0404, 0.0406,\n",
              "                      0.0407, 0.0406, 0.0401, 0.0404, 0.0406, 0.0402, 0.0404, 0.0412, 0.0402,\n",
              "                      0.0439, 0.0390, 0.0399, 0.0399, 0.0412, 0.0402, 0.0400, 0.0404, 0.0404,\n",
              "                      0.0285, 0.0363, 0.0402, 0.0412, 0.0402, 0.0379, 0.0362, 0.0373, 0.0397,\n",
              "                      0.0359, 0.0379, 0.0402, 0.0394, 0.0406, 0.0394, 0.0407, 0.0401, 0.0402,\n",
              "                      0.0373, 0.0402, 0.0261, 0.0338, 0.0402, 0.0360, 0.0402, 0.0391, 0.0350,\n",
              "                      0.0402], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.11.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  52,  55,  54,  52,  51,  48,  49,  55,  55,  54,  56,  56,  53,\n",
              "                       57,  57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.self_attn.out_proj.weight',\n",
              "              tensor([[ 86],\n",
              "                      [108],\n",
              "                      [ 59],\n",
              "                      ...,\n",
              "                      [ 52],\n",
              "                      [ 70],\n",
              "                      [115]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.self_attn.out_proj.bias',\n",
              "              tensor([ 0.0077,  0.0208,  0.0249,  ...,  0.0591,  0.0056, -0.0372],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.self_attn.out_proj.weight.absmax',\n",
              "              tensor([255,   0, 255,  ...,   0,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.11.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0298, 0.0353, 0.0443, 0.0381, 0.0413, 0.0385, 0.1010, 0.0475, 0.0556,\n",
              "                      0.0347, 0.0306, 0.0618, 0.0455, 0.0272, 0.0641, 0.0362, 0.0318, 0.0313,\n",
              "                      0.0385, 0.0405, 0.0363, 0.0556, 0.0348, 0.0389, 0.0361, 0.0445, 0.0584,\n",
              "                      0.0399, 0.0398, 0.0612, 0.0349, 0.0309, 0.0258, 0.0438, 0.0378, 0.0393,\n",
              "                      0.0337, 0.0467, 0.0575, 0.0508, 0.0603, 0.0416, 0.0588, 0.0396, 0.0462,\n",
              "                      0.0369, 0.0522, 0.0433, 0.0309, 0.0401, 0.0491, 0.0296, 0.0349, 0.0451,\n",
              "                      0.0583, 0.0480, 0.0273, 0.0584, 0.0398, 0.0294, 0.0390, 0.0380, 0.0491,\n",
              "                      0.0378], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.11.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  51,  54,  57,  48,  50,  54,  55,  53,  48,  51,  50,  54,  49,\n",
              "                       53,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.self_attn_layer_norm.bias',\n",
              "              tensor([-0.0394, -0.0083, -0.0201,  ..., -0.0638,  0.0321,  0.0159],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.fc1.weight',\n",
              "              tensor([[212],\n",
              "                      [168],\n",
              "                      [ 45],\n",
              "                      ...,\n",
              "                      [182],\n",
              "                      [ 40],\n",
              "                      [ 56]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.fc1.bias',\n",
              "              tensor([-0.0199, -0.0659, -0.0109,  ..., -0.0099, -0.0247,  0.0294],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.fc1.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ...,   0, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.11.fc1.weight.nested_absmax',\n",
              "              tensor([0.0457, 0.0488, 0.0488, 0.0494, 0.0427, 0.0429, 0.0391, 0.0498, 0.0478,\n",
              "                      0.0363, 0.0501, 0.0481, 0.0361, 0.0371, 0.0389, 0.0507, 0.0461, 0.0386,\n",
              "                      0.0454, 0.0446, 0.0355, 0.0474, 0.0477, 0.0436, 0.0491, 0.0488, 0.0368,\n",
              "                      0.0357, 0.0493, 0.0370, 0.0408, 0.0482, 0.0493, 0.0417, 0.0481, 0.0368,\n",
              "                      0.0376, 0.0452, 0.0498, 0.0415, 0.0399, 0.0434, 0.0360, 0.0487, 0.0490,\n",
              "                      0.0473, 0.0419, 0.0300, 0.0340, 0.0449, 0.0450, 0.0410, 0.0487, 0.0411,\n",
              "                      0.0468, 0.0384, 0.0366, 0.0404, 0.0385, 0.0465, 0.0391, 0.0482, 0.0468,\n",
              "                      0.0446, 0.0447, 0.0446, 0.0410, 0.0399, 0.0493, 0.0456, 0.0427, 0.0487,\n",
              "                      0.0422, 0.0494, 0.0391, 0.0482, 0.0493, 0.0484, 0.0366, 0.0462, 0.0427,\n",
              "                      0.0476, 0.0412, 0.0488, 0.0493, 0.0488, 0.0476, 0.0418, 0.0493, 0.0501,\n",
              "                      0.0469, 0.0479, 0.0374, 0.0494, 0.0501, 0.0494, 0.0484, 0.0492, 0.0493,\n",
              "                      0.0484, 0.0446, 0.0485, 0.0463, 0.0488, 0.0478, 0.0399, 0.0364, 0.0488,\n",
              "                      0.0462, 0.0466, 0.0466, 0.0460, 0.0482, 0.0396, 0.0493, 0.0403, 0.0407,\n",
              "                      0.0393, 0.0417, 0.0422, 0.0399, 0.0467, 0.0424, 0.0481, 0.0491, 0.0372,\n",
              "                      0.0487, 0.0491, 0.0493, 0.0450, 0.0492, 0.0466, 0.0454, 0.0439, 0.0451,\n",
              "                      0.0397, 0.0492, 0.0386, 0.0494, 0.0455, 0.0443, 0.0480, 0.0377, 0.0488,\n",
              "                      0.0352, 0.0482, 0.0411, 0.0379, 0.0389, 0.0432, 0.0490, 0.0410, 0.0480,\n",
              "                      0.0460, 0.0490, 0.0491, 0.0482, 0.0443, 0.0480, 0.0483, 0.0480, 0.0416,\n",
              "                      0.0407, 0.0349, 0.0412, 0.0357, 0.0482, 0.0488, 0.0433, 0.0426, 0.0472,\n",
              "                      0.0351, 0.0307, 0.0480, 0.0380, 0.0360, 0.0493, 0.0496, 0.0401, 0.0492,\n",
              "                      0.0394, 0.0490, 0.0435, 0.0333, 0.0416, 0.0411, 0.0341, 0.0363, 0.0432,\n",
              "                      0.0493, 0.0471, 0.0484, 0.0413, 0.0299, 0.0426, 0.0416, 0.0490, 0.0331,\n",
              "                      0.0444, 0.0404, 0.0446, 0.0490, 0.0465, 0.0344, 0.0496, 0.0369, 0.0469,\n",
              "                      0.0482, 0.0429, 0.0273, 0.0488, 0.0457, 0.0352, 0.0458, 0.0498, 0.0493,\n",
              "                      0.0402, 0.0322, 0.0457, 0.0401, 0.0429, 0.0484, 0.0405, 0.0488, 0.0493,\n",
              "                      0.0461, 0.0384, 0.0377, 0.0441, 0.0493, 0.0360, 0.0464, 0.0487, 0.0480,\n",
              "                      0.0407, 0.0464, 0.0491, 0.0338, 0.0424, 0.0382, 0.0484, 0.0455, 0.0451,\n",
              "                      0.0405, 0.0456, 0.0338, 0.0493, 0.0399, 0.0472, 0.0501, 0.0476, 0.0493,\n",
              "                      0.0409, 0.0425, 0.0487, 0.0480], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.11.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  53,  54,  55,  50,  50,  53,  51,  57,  54,  54,  51,  51,  49,\n",
              "                       52,  56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.fc2.weight',\n",
              "              tensor([[ 87],\n",
              "                      [103],\n",
              "                      [ 89],\n",
              "                      ...,\n",
              "                      [ 44],\n",
              "                      [216],\n",
              "                      [ 24]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.fc2.bias',\n",
              "              tensor([ 0.0037,  0.0370,  0.0435,  ...,  0.0622, -0.0342,  0.0138],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.fc2.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.11.fc2.weight.nested_absmax',\n",
              "              tensor([0.0350, 0.0430, 0.0602, 0.0494, 0.0319, 0.0573, 0.0618, 0.0405, 0.0488,\n",
              "                      0.0469, 0.0572, 0.0500, 0.0621, 0.0610, 0.0333, 0.0309, 0.0527, 0.0421,\n",
              "                      0.0591, 0.0600, 0.0406, 0.0394, 0.0613, 0.0552, 0.1868, 0.0561, 0.0616,\n",
              "                      0.0566, 0.0618, 0.0544, 0.0471, 0.0618, 0.0569, 0.0582, 0.0499, 0.0383,\n",
              "                      0.0471, 0.0590, 0.0448, 0.0378, 0.0538, 0.0498, 0.0464, 0.0593, 0.0613,\n",
              "                      0.0619, 0.1031, 0.0377, 0.0518, 0.0607, 0.0602, 0.0619, 0.0488, 0.0563,\n",
              "                      0.0514, 0.0612, 0.0316, 0.0430, 0.0425, 0.0622, 0.0466, 0.0592, 0.0557,\n",
              "                      0.0354, 0.0552, 0.0456, 0.0620, 0.0255, 0.0437, 0.0572, 0.0434, 0.0460,\n",
              "                      0.0607, 0.0315, 0.0386, 0.0618, 0.0617, 0.0553, 0.0547, 0.0545, 0.0374,\n",
              "                      0.0533, 0.0632, 0.0580, 0.0579, 0.0606, 0.0629, 0.0353, 0.0408, 0.0380,\n",
              "                      0.0610, 0.0454, 0.0550, 0.0613, 0.0524, 0.0618, 0.0369, 0.0612, 0.0507,\n",
              "                      0.0566, 0.0608, 0.0343, 0.0543, 0.0621, 0.0611, 0.0433, 0.0433, 0.0558,\n",
              "                      0.0422, 0.0522, 0.0626, 0.0400, 0.0491, 0.0446, 0.0398, 0.0469, 0.0448,\n",
              "                      0.0618, 0.0624, 0.0475, 0.0579, 0.0447, 0.0616, 0.0552, 0.0378, 0.0617,\n",
              "                      0.0460, 0.0616, 0.0619, 0.0412, 0.0502, 0.0619, 0.0443, 0.0412, 0.0536,\n",
              "                      0.0621, 0.0365, 0.0607, 0.0366, 0.0608, 0.0517, 0.0431, 0.0612, 0.0619,\n",
              "                      0.0374, 0.0515, 0.0493, 0.0610, 0.0627, 0.0455, 0.0566, 0.0521, 0.0353,\n",
              "                      0.0599, 0.0614, 0.0618, 0.0579, 0.0559, 0.0485, 0.0505, 0.0500, 0.0427,\n",
              "                      0.0321, 0.0554, 0.0442, 0.0353, 0.0341, 0.0616, 0.0492, 0.0480, 0.0396,\n",
              "                      0.0620, 0.0591, 0.0511, 0.0544, 0.0414, 0.0433, 0.0574, 0.0547, 0.0390,\n",
              "                      0.0563, 0.0417, 0.0626, 0.0377, 0.0499, 0.0566, 0.0503, 0.0537, 0.0619,\n",
              "                      0.0558, 0.0455, 0.0379, 0.0605, 0.0588, 0.0409, 0.0533, 0.0592, 0.0469,\n",
              "                      0.0612, 0.0421, 0.0433, 0.0467, 0.0614, 0.0451, 0.0494, 0.0358, 0.0424,\n",
              "                      0.0627, 0.0612, 0.0617, 0.0558, 0.0467, 0.0441, 0.0521, 0.0618, 0.0612,\n",
              "                      0.0558, 0.0507, 0.0618, 0.0420, 0.0583, 0.0389, 0.0491, 0.0561, 0.0343,\n",
              "                      0.0604, 0.0614, 0.0507, 0.0620, 0.0599, 0.0560, 0.0289, 0.0500, 0.0613,\n",
              "                      0.0498, 0.0430, 0.0463, 0.0547, 0.0450, 0.0423, 0.0517, 0.0618, 0.0331,\n",
              "                      0.0331, 0.0420, 0.0325, 0.0555, 0.0477, 0.0334, 0.0557, 0.0456, 0.0502,\n",
              "                      0.0326, 0.0367, 0.0278, 0.0618], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.11.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  51,  50,  51,  56,  50,  57,  50,  57,  51,  50,  53,  49,  48,\n",
              "                       51,  56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.11.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.11.final_layer_norm.bias',\n",
              "              tensor([ 0.0054, -0.0208, -0.0072,  ..., -0.0621, -0.0368,  0.0077],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.self_attn.k_proj.weight',\n",
              "              tensor([[ 57],\n",
              "                      [123],\n",
              "                      [101],\n",
              "                      ...,\n",
              "                      [128],\n",
              "                      [ 81],\n",
              "                      [150]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0316,  0.0312,  0.0484,  ..., -0.0625, -0.0337, -0.0398],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.self_attn.k_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.12.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0409, 0.0411, 0.0414, 0.0408, 0.0407, 0.0408, 0.0405, 0.0397, 0.0401,\n",
              "                      0.0415, 0.0409, 0.0406, 0.0393, 0.0398, 0.0406, 0.0394, 0.0363, 0.0407,\n",
              "                      0.0407, 0.0332, 0.0371, 0.0408, 0.0365, 0.0377, 0.0378, 0.0389, 0.0326,\n",
              "                      0.0352, 0.0420, 0.0425, 0.0411, 0.0474, 0.0410, 0.0416, 0.0409, 0.0414,\n",
              "                      0.0408, 0.0432, 0.0409, 0.0411, 0.0372, 0.0406, 0.0408, 0.0414, 0.0412,\n",
              "                      0.0410, 0.0408, 0.0408, 0.0409, 0.0333, 0.0408, 0.0394, 0.0420, 0.0422,\n",
              "                      0.0416, 0.0419, 0.0411, 0.0322, 0.0389, 0.0389, 0.0414, 0.0426, 0.0421,\n",
              "                      0.0432], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.12.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  52,  50,  52,  51,  54,  55,  48,  49,  48,  53,  57,  51,  52,\n",
              "                       49,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.self_attn.v_proj.weight',\n",
              "              tensor([[163],\n",
              "                      [220],\n",
              "                      [105],\n",
              "                      ...,\n",
              "                      [199],\n",
              "                      [135],\n",
              "                      [206]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.self_attn.v_proj.bias',\n",
              "              tensor([ 0.0155,  0.0018,  0.0141,  ..., -0.0079, -0.0072,  0.0210],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.self_attn.v_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ...,   0, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.12.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0407, 0.0300, 0.0345, 0.0312, 0.0365, 0.0286, 0.0340, 0.0424, 0.0357,\n",
              "                      0.0447, 0.0341, 0.0345, 0.0440, 0.0443, 0.0518, 0.0454, 0.0367, 0.0343,\n",
              "                      0.0455, 0.0392, 0.0521, 0.0408, 0.0348, 0.0357, 0.0418, 0.0454, 0.0290,\n",
              "                      0.0350, 0.0377, 0.0330, 0.0356, 0.0410, 0.0432, 0.0351, 0.0256, 0.0368,\n",
              "                      0.0465, 0.0521, 0.0435, 0.0526, 0.0429, 0.0392, 0.0390, 0.0450, 0.0314,\n",
              "                      0.0378, 0.0260, 0.0334, 0.0519, 0.0438, 0.0457, 0.0383, 0.0379, 0.0289,\n",
              "                      0.0273, 0.0288, 0.0524, 0.0385, 0.0429, 0.0381, 0.0349, 0.0438, 0.0454,\n",
              "                      0.0410], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.12.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  50,  53,  54,  55,  52,  49,  48,  55,  54,  55,  48,  55,  56,\n",
              "                       52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.self_attn.q_proj.weight',\n",
              "              tensor([[133],\n",
              "                      [ 82],\n",
              "                      [ 58],\n",
              "                      ...,\n",
              "                      [115],\n",
              "                      [182],\n",
              "                      [ 22]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.self_attn.q_proj.bias',\n",
              "              tensor([ 0.0174, -0.0306,  0.0571,  ..., -0.0622, -0.0603, -0.0545],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.self_attn.q_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.12.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0390, 0.0392, 0.0385, 0.0387, 0.0389, 0.0386, 0.0383, 0.0391, 0.0395,\n",
              "                      0.0400, 0.0389, 0.0392, 0.0384, 0.0392, 0.0378, 0.0384, 0.0256, 0.0354,\n",
              "                      0.0323, 0.0348, 0.0373, 0.0392, 0.0391, 0.0386, 0.0333, 0.0334, 0.0321,\n",
              "                      0.0312, 0.0423, 0.0459, 0.0434, 0.0434, 0.0386, 0.0375, 0.0391, 0.0392,\n",
              "                      0.0395, 0.0391, 0.0390, 0.0392, 0.0381, 0.0378, 0.0361, 0.0376, 0.0390,\n",
              "                      0.0392, 0.0373, 0.0391, 0.0366, 0.0347, 0.0395, 0.0376, 0.0389, 0.0387,\n",
              "                      0.0389, 0.0384, 0.0296, 0.0386, 0.0389, 0.0385, 0.0407, 0.0403, 0.0403,\n",
              "                      0.0396], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.12.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  53,  56,  57,  48,  53,  54,  49,  51,  52,  50,  50,  51,  57,\n",
              "                       51,  56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.self_attn.out_proj.weight',\n",
              "              tensor([[ 68],\n",
              "                      [ 66],\n",
              "                      [167],\n",
              "                      ...,\n",
              "                      [ 54],\n",
              "                      [ 87],\n",
              "                      [146]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.self_attn.out_proj.bias',\n",
              "              tensor([-0.0120,  0.0331,  0.0247,  ...,  0.0630,  0.0156, -0.0280],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.self_attn.out_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.12.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0544, 0.0420, 0.0361, 0.0357, 0.0486, 0.0419, 0.0565, 0.0529, 0.0368,\n",
              "                      0.0446, 0.0530, 0.0394, 0.0394, 0.0501, 0.0550, 0.0417, 0.0400, 0.0415,\n",
              "                      0.0442, 0.0559, 0.0467, 0.0426, 0.0358, 0.0563, 0.0439, 0.0390, 0.0446,\n",
              "                      0.0464, 0.0469, 0.0545, 0.0456, 0.0433, 0.0540, 0.0552, 0.0502, 0.0377,\n",
              "                      0.0373, 0.0505, 0.0544, 0.0544, 0.0491, 0.0429, 0.0417, 0.0533, 0.0440,\n",
              "                      0.0439, 0.0400, 0.0472, 0.0414, 0.0542, 0.0409, 0.0541, 0.0482, 0.0544,\n",
              "                      0.0386, 0.0461, 0.0351, 0.0549, 0.0515, 0.0483, 0.0417, 0.0539, 0.0544,\n",
              "                      0.0328], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.12.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  48,  56,  49,  55,  49,  56,  55,  52,  50,  56,  52,  55,  52,\n",
              "                       52,  51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.self_attn_layer_norm.bias',\n",
              "              tensor([-0.0479, -0.0113, -0.0185,  ..., -0.0647,  0.0290,  0.0436],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.fc1.weight',\n",
              "              tensor([[252],\n",
              "                      [105],\n",
              "                      [117],\n",
              "                      ...,\n",
              "                      [104],\n",
              "                      [151],\n",
              "                      [ 70]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.fc1.bias',\n",
              "              tensor([-0.0021, -0.0231, -0.0271,  ...,  0.0012, -0.0441, -0.0486],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.fc1.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ...,   0, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.12.fc1.weight.nested_absmax',\n",
              "              tensor([0.0472, 0.0481, 0.0445, 0.0463, 0.0468, 0.0468, 0.0473, 0.0441, 0.0393,\n",
              "                      0.0300, 0.0452, 0.0363, 0.0424, 0.0477, 0.0332, 0.0474, 0.0415, 0.0406,\n",
              "                      0.0487, 0.0487, 0.0352, 0.0371, 0.0407, 0.0379, 0.0475, 0.0419, 0.0487,\n",
              "                      0.0510, 0.0383, 0.0484, 0.0474, 0.0336, 0.0389, 0.0321, 0.0471, 0.0465,\n",
              "                      0.0468, 0.0455, 0.0463, 0.0482, 0.0486, 0.0479, 0.0357, 0.0481, 0.0372,\n",
              "                      0.0477, 0.0423, 0.0359, 0.0385, 0.0486, 0.0484, 0.0481, 0.0474, 0.0303,\n",
              "                      0.0351, 0.0408, 0.0486, 0.0462, 0.0343, 0.0443, 0.0482, 0.0482, 0.0480,\n",
              "                      0.0428, 0.0444, 0.0409, 0.0319, 0.0422, 0.0410, 0.0441, 0.0479, 0.0498,\n",
              "                      0.0325, 0.0487, 0.0481, 0.0414, 0.0426, 0.0333, 0.0411, 0.0447, 0.0354,\n",
              "                      0.0479, 0.0386, 0.0473, 0.0379, 0.0405, 0.0487, 0.0415, 0.0426, 0.0431,\n",
              "                      0.0486, 0.0485, 0.0338, 0.0342, 0.0478, 0.0359, 0.0459, 0.0328, 0.0424,\n",
              "                      0.0396, 0.0436, 0.0511, 0.0377, 0.0474, 0.0415, 0.0429, 0.0449, 0.0397,\n",
              "                      0.0372, 0.0485, 0.0448, 0.0491, 0.0487, 0.0482, 0.0474, 0.0459, 0.0485,\n",
              "                      0.0394, 0.0365, 0.0451, 0.0428, 0.0360, 0.0467, 0.0484, 0.0479, 0.0484,\n",
              "                      0.0293, 0.0471, 0.0333, 0.0350, 0.0467, 0.0351, 0.0473, 0.0387, 0.0478,\n",
              "                      0.0394, 0.0441, 0.0394, 0.0487, 0.0471, 0.0430, 0.0447, 0.0434, 0.0457,\n",
              "                      0.0460, 0.0434, 0.0465, 0.0493, 0.0472, 0.0476, 0.0360, 0.0478, 0.0469,\n",
              "                      0.0358, 0.0424, 0.0468, 0.0448, 0.0453, 0.0487, 0.0321, 0.0490, 0.0382,\n",
              "                      0.0483, 0.0489, 0.0382, 0.0487, 0.0419, 0.0493, 0.0484, 0.0490, 0.0464,\n",
              "                      0.0377, 0.0463, 0.0321, 0.0453, 0.0369, 0.0444, 0.0487, 0.0392, 0.0415,\n",
              "                      0.0468, 0.0470, 0.0402, 0.0348, 0.0430, 0.0487, 0.0479, 0.0365, 0.0490,\n",
              "                      0.0421, 0.0302, 0.0484, 0.0474, 0.0363, 0.0443, 0.0380, 0.0486, 0.0443,\n",
              "                      0.0409, 0.0490, 0.0457, 0.0312, 0.0413, 0.0449, 0.0354, 0.0441, 0.0454,\n",
              "                      0.0459, 0.0429, 0.0435, 0.0397, 0.0488, 0.0412, 0.0460, 0.0479, 0.0471,\n",
              "                      0.0502, 0.0418, 0.0293, 0.0391, 0.0486, 0.0485, 0.0441, 0.0486, 0.0485,\n",
              "                      0.0482, 0.0450, 0.0376, 0.0412, 0.0446, 0.0377, 0.0393, 0.0479, 0.0471,\n",
              "                      0.0494, 0.0344, 0.0369, 0.0310, 0.0444, 0.0459, 0.0404, 0.0376, 0.0330,\n",
              "                      0.0473, 0.0484, 0.0484, 0.0415, 0.0467, 0.0400, 0.0471, 0.0387, 0.0350,\n",
              "                      0.0347, 0.0479, 0.0451, 0.0488], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.12.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  54,  51,  51,  57,  54,  48,  50,  52,  55,  48,  51,  57,  55,\n",
              "                       57,  53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.fc2.weight',\n",
              "              tensor([[143],\n",
              "                      [246],\n",
              "                      [186],\n",
              "                      ...,\n",
              "                      [ 87],\n",
              "                      [102],\n",
              "                      [138]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.fc2.bias',\n",
              "              tensor([ 0.0520, -0.0297,  0.0146,  ...,  0.0657, -0.0623, -0.0044],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.fc2.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.12.fc2.weight.nested_absmax',\n",
              "              tensor([0.0620, 0.0362, 0.0623, 0.0394, 0.0294, 0.0466, 0.0562, 0.0529, 0.0328,\n",
              "                      0.0361, 0.0399, 0.0562, 0.0526, 0.0624, 0.0525, 0.0480, 0.0620, 0.0612,\n",
              "                      0.0555, 0.0620, 0.0409, 0.0361, 0.0574, 0.0640, 0.0763, 0.0627, 0.0421,\n",
              "                      0.0550, 0.0621, 0.0616, 0.0620, 0.0425, 0.0600, 0.0414, 0.0522, 0.0479,\n",
              "                      0.0451, 0.0338, 0.0496, 0.0589, 0.0608, 0.0317, 0.0606, 0.0559, 0.0292,\n",
              "                      0.0435, 0.0665, 0.0474, 0.0620, 0.0613, 0.0582, 0.0611, 0.0605, 0.0336,\n",
              "                      0.0330, 0.0407, 0.0339, 0.0609, 0.0502, 0.0610, 0.0523, 0.0537, 0.0607,\n",
              "                      0.0578, 0.0338, 0.0401, 0.0615, 0.0563, 0.0428, 0.0615, 0.0610, 0.0330,\n",
              "                      0.0449, 0.0491, 0.0614, 0.0534, 0.0629, 0.0340, 0.0624, 0.0495, 0.0498,\n",
              "                      0.0620, 0.0399, 0.0544, 0.0620, 0.0353, 0.0407, 0.0615, 0.0613, 0.0353,\n",
              "                      0.0557, 0.0507, 0.0449, 0.0618, 0.0572, 0.0419, 0.0317, 0.0386, 0.0422,\n",
              "                      0.0405, 0.0620, 0.0449, 0.0609, 0.0491, 0.0543, 0.0612, 0.0617, 0.0531,\n",
              "                      0.0618, 0.0376, 0.0566, 0.0303, 0.0405, 0.0610, 0.0620, 0.0607, 0.0442,\n",
              "                      0.0508, 0.0640, 0.0534, 0.0589, 0.0526, 0.0447, 0.0549, 0.0366, 0.0496,\n",
              "                      0.0482, 0.0468, 0.0452, 0.0402, 0.0613, 0.0288, 0.0525, 0.0585, 0.0581,\n",
              "                      0.0598, 0.0359, 0.0328, 0.0380, 0.0506, 0.0556, 0.0594, 0.0578, 0.0437,\n",
              "                      0.0332, 0.0576, 0.0468, 0.0446, 0.0398, 0.0308, 0.0561, 0.0575, 0.0449,\n",
              "                      0.0598, 0.0601, 0.0625, 0.0635, 0.0616, 0.0353, 0.0407, 0.0389, 0.0507,\n",
              "                      0.0617, 0.0595, 0.0492, 0.0590, 0.0429, 0.0612, 0.0329, 0.0605, 0.0585,\n",
              "                      0.0604, 0.0634, 0.0599, 0.0338, 0.0494, 0.0616, 0.0421, 0.0336, 0.0617,\n",
              "                      0.0447, 0.0374, 0.0609, 0.0625, 0.0620, 0.0372, 0.0620, 0.0295, 0.0525,\n",
              "                      0.0382, 0.0606, 0.0416, 0.0617, 0.0273, 0.0408, 0.0463, 0.0565, 0.0591,\n",
              "                      0.0388, 0.0468, 0.0620, 0.0262, 0.0280, 0.0617, 0.0569, 0.0487, 0.0610,\n",
              "                      0.0610, 0.0408, 0.0531, 0.0518, 0.0624, 0.0324, 0.0429, 0.0614, 0.0352,\n",
              "                      0.0462, 0.0598, 0.0553, 0.0379, 0.0592, 0.0559, 0.0619, 0.0623, 0.0552,\n",
              "                      0.0519, 0.0599, 0.0524, 0.0508, 0.0612, 0.0410, 0.0489, 0.0611, 0.0578,\n",
              "                      0.0555, 0.0609, 0.0541, 0.0577, 0.0555, 0.0618, 0.0478, 0.0618, 0.0609,\n",
              "                      0.0433, 0.0423, 0.0621, 0.0474, 0.0330, 0.0322, 0.0366, 0.0438, 0.0335,\n",
              "                      0.0442, 0.0359, 0.0486, 0.0463], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.12.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  50,  57,  56,  50,  56,  49,  50,  53,  50,  51,  56,  52,  49,\n",
              "                       56,  54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.12.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.12.final_layer_norm.bias',\n",
              "              tensor([ 0.0212, -0.0125, -0.0134,  ..., -0.0627, -0.0413, -0.0034],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.self_attn.k_proj.weight',\n",
              "              tensor([[243],\n",
              "                      [146],\n",
              "                      [101],\n",
              "                      ...,\n",
              "                      [221],\n",
              "                      [233],\n",
              "                      [249]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.self_attn.k_proj.bias',\n",
              "              tensor([-0.0233,  0.0252, -0.0385,  ...,  0.0325,  0.0157, -0.0238],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ...,   0, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.13.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0407, 0.0407, 0.0411, 0.0402, 0.0419, 0.0415, 0.0425, 0.0414, 0.0405,\n",
              "                      0.0400, 0.0407, 0.0412, 0.0408, 0.0408, 0.0408, 0.0407, 0.0409, 0.0411,\n",
              "                      0.0412, 0.0417, 0.0348, 0.0412, 0.0398, 0.0404, 0.0407, 0.0406, 0.0398,\n",
              "                      0.0408, 0.0407, 0.0377, 0.0394, 0.0420, 0.0393, 0.0408, 0.0408, 0.0408,\n",
              "                      0.0396, 0.0408, 0.0402, 0.0367, 0.0439, 0.0431, 0.0422, 0.0441, 0.0412,\n",
              "                      0.0409, 0.0430, 0.0412, 0.0428, 0.0420, 0.0408, 0.0415, 0.0404, 0.0397,\n",
              "                      0.0402, 0.0408, 0.0401, 0.0409, 0.0382, 0.0409, 0.0406, 0.0407, 0.0405,\n",
              "                      0.0395], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.13.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  52,  49,  57,  50,  49,  56,  54,  53,  57,  52,  48,  48,  57,\n",
              "                       52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.self_attn.v_proj.weight',\n",
              "              tensor([[149],\n",
              "                      [172],\n",
              "                      [228],\n",
              "                      ...,\n",
              "                      [146],\n",
              "                      [210],\n",
              "                      [ 87]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.self_attn.v_proj.bias',\n",
              "              tensor([ 0.0107,  0.0066, -0.0101,  ..., -0.0072,  0.0206, -0.0011],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.self_attn.v_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.13.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0276, 0.0454, 0.0372, 0.0442, 0.0261, 0.0343, 0.0364, 0.0279, 0.0502,\n",
              "                      0.0275, 0.0311, 0.0345, 0.0533, 0.0519, 0.0449, 0.0408, 0.0316, 0.0322,\n",
              "                      0.0296, 0.0319, 0.0311, 0.0314, 0.0263, 0.0333, 0.0496, 0.0472, 0.0352,\n",
              "                      0.0406, 0.0502, 0.0428, 0.0494, 0.0422, 0.0381, 0.0355, 0.0429, 0.0341,\n",
              "                      0.0341, 0.0411, 0.0469, 0.0454, 0.0251, 0.0328, 0.0363, 0.0363, 0.0339,\n",
              "                      0.0334, 0.0361, 0.0340, 0.0534, 0.0545, 0.0505, 0.0488, 0.0399, 0.0474,\n",
              "                      0.0315, 0.0499, 0.0363, 0.0255, 0.0428, 0.0437, 0.0349, 0.0381, 0.0363,\n",
              "                      0.0483], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.13.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  49,  55,  49,  53,  49,  54,  49,  50,  48,  52,  51,  51,  56,\n",
              "                       48,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.self_attn.q_proj.weight',\n",
              "              tensor([[140],\n",
              "                      [ 66],\n",
              "                      [217],\n",
              "                      ...,\n",
              "                      [147],\n",
              "                      [  4],\n",
              "                      [122]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.self_attn.q_proj.bias',\n",
              "              tensor([ 0.0267, -0.0311, -0.0743,  ..., -0.0228, -0.0620,  0.0060],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.self_attn.q_proj.weight.absmax',\n",
              "              tensor([255,   0,   0,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.13.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0316, 0.0400, 0.0390, 0.0398, 0.0373, 0.0398, 0.0373, 0.0372, 0.0398,\n",
              "                      0.0405, 0.0408, 0.0406, 0.0351, 0.0335, 0.0400, 0.0400, 0.0400, 0.0405,\n",
              "                      0.0398, 0.0397, 0.0340, 0.0341, 0.0340, 0.0392, 0.0390, 0.0313, 0.0399,\n",
              "                      0.0392, 0.0397, 0.0405, 0.0398, 0.0403, 0.0373, 0.0395, 0.0393, 0.0405,\n",
              "                      0.0396, 0.0405, 0.0322, 0.0346, 0.0403, 0.0398, 0.0399, 0.0409, 0.0382,\n",
              "                      0.0400, 0.0400, 0.0398, 0.0415, 0.0411, 0.0408, 0.0424, 0.0381, 0.0372,\n",
              "                      0.0382, 0.0402, 0.0406, 0.0390, 0.0402, 0.0378, 0.0350, 0.0398, 0.0342,\n",
              "                      0.0403], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.13.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  52,  57,  54,  56,  49,  53,  55,  49,  49,  50,  53,  57,  56,\n",
              "                       52,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.self_attn.out_proj.weight',\n",
              "              tensor([[106],\n",
              "                      [134],\n",
              "                      [154],\n",
              "                      ...,\n",
              "                      [222],\n",
              "                      [171],\n",
              "                      [231]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.self_attn.out_proj.bias',\n",
              "              tensor([-0.0072,  0.0376,  0.0179,  ...,  0.0630, -0.0201, -0.0529],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.self_attn.out_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ..., 255, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.13.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0533, 0.0410, 0.0577, 0.0556, 0.0337, 0.0369, 0.0555, 0.0433, 0.0413,\n",
              "                      0.0397, 0.0466, 0.0481, 0.0499, 0.0402, 0.0516, 0.0364, 0.0442, 0.0537,\n",
              "                      0.0545, 0.0560, 0.0456, 0.0556, 0.0534, 0.0548, 0.0451, 0.0547, 0.0374,\n",
              "                      0.0368, 0.0549, 0.0568, 0.0435, 0.0441, 0.0506, 0.0415, 0.0322, 0.0538,\n",
              "                      0.0523, 0.0391, 0.0543, 0.0390, 0.0513, 0.0418, 0.0552, 0.0534, 0.0422,\n",
              "                      0.0460, 0.0364, 0.0436, 0.0484, 0.0559, 0.0377, 0.0506, 0.0377, 0.0335,\n",
              "                      0.0405, 0.0426, 0.0514, 0.0471, 0.0538, 0.0501, 0.0446, 0.0566, 0.0552,\n",
              "                      0.0392], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.13.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  57,  57,  57,  52,  49,  56,  49,  51,  57,  52,  53,  55,  55,\n",
              "                       48,  51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.self_attn_layer_norm.bias',\n",
              "              tensor([-0.0362, -0.0245, -0.0244,  ..., -0.0706,  0.0106,  0.0626],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.fc1.weight',\n",
              "              tensor([[ 41],\n",
              "                      [143],\n",
              "                      [218],\n",
              "                      ...,\n",
              "                      [ 55],\n",
              "                      [ 72],\n",
              "                      [132]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.fc1.bias',\n",
              "              tensor([-0.0486, -0.0089, -0.0123,  ...,  0.0131, -0.0433,  0.0047],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.fc1.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.13.fc1.weight.nested_absmax',\n",
              "              tensor([0.0461, 0.0490, 0.0468, 0.0469, 0.0359, 0.0381, 0.0434, 0.0382, 0.0442,\n",
              "                      0.0293, 0.0500, 0.0458, 0.0468, 0.0425, 0.0457, 0.0472, 0.0474, 0.0491,\n",
              "                      0.0386, 0.0462, 0.0475, 0.0456, 0.0442, 0.0492, 0.0379, 0.0492, 0.0454,\n",
              "                      0.0388, 0.0388, 0.0475, 0.0487, 0.0485, 0.0431, 0.0429, 0.0491, 0.0418,\n",
              "                      0.0489, 0.0387, 0.0485, 0.0459, 0.0467, 0.0433, 0.0481, 0.0312, 0.0473,\n",
              "                      0.0465, 0.0493, 0.0340, 0.0412, 0.0440, 0.0490, 0.0447, 0.0487, 0.0481,\n",
              "                      0.0382, 0.0384, 0.0505, 0.0492, 0.0492, 0.0462, 0.0421, 0.0442, 0.0357,\n",
              "                      0.0407, 0.0465, 0.0379, 0.0487, 0.0492, 0.0472, 0.0449, 0.0493, 0.0489,\n",
              "                      0.0445, 0.0460, 0.0449, 0.0494, 0.0374, 0.0481, 0.0481, 0.0503, 0.0356,\n",
              "                      0.0431, 0.0366, 0.0383, 0.0373, 0.0486, 0.0312, 0.0377, 0.0486, 0.0498,\n",
              "                      0.0451, 0.0460, 0.0439, 0.0382, 0.0375, 0.0484, 0.0424, 0.0392, 0.0406,\n",
              "                      0.0355, 0.0433, 0.0359, 0.0459, 0.0486, 0.0440, 0.0330, 0.0453, 0.0399,\n",
              "                      0.0372, 0.0385, 0.0460, 0.0305, 0.0387, 0.0500, 0.0417, 0.0346, 0.0416,\n",
              "                      0.0477, 0.0488, 0.0404, 0.0398, 0.0374, 0.0475, 0.0445, 0.0337, 0.0492,\n",
              "                      0.0489, 0.0480, 0.0493, 0.0486, 0.0410, 0.0373, 0.0401, 0.0457, 0.0494,\n",
              "                      0.0494, 0.0423, 0.0489, 0.0453, 0.0383, 0.0487, 0.0385, 0.0460, 0.0443,\n",
              "                      0.0294, 0.0459, 0.0313, 0.0412, 0.0316, 0.0494, 0.0408, 0.0369, 0.0494,\n",
              "                      0.0481, 0.0457, 0.0410, 0.0412, 0.0493, 0.0401, 0.0360, 0.0450, 0.0454,\n",
              "                      0.0485, 0.0395, 0.0404, 0.0385, 0.0437, 0.0417, 0.0439, 0.0489, 0.0461,\n",
              "                      0.0399, 0.0432, 0.0439, 0.0338, 0.0319, 0.0379, 0.0359, 0.0475, 0.0492,\n",
              "                      0.0386, 0.0315, 0.0482, 0.0359, 0.0408, 0.0408, 0.0399, 0.0283, 0.0440,\n",
              "                      0.0362, 0.0362, 0.0393, 0.0452, 0.0368, 0.0486, 0.0495, 0.0310, 0.0486,\n",
              "                      0.0431, 0.0412, 0.0490, 0.0345, 0.0493, 0.0462, 0.0453, 0.0379, 0.0363,\n",
              "                      0.0495, 0.0376, 0.0338, 0.0373, 0.0424, 0.0272, 0.0423, 0.0479, 0.0342,\n",
              "                      0.0370, 0.0416, 0.0471, 0.0392, 0.0479, 0.0473, 0.0484, 0.0475, 0.0457,\n",
              "                      0.0468, 0.0479, 0.0395, 0.0484, 0.0467, 0.0446, 0.0380, 0.0473, 0.0465,\n",
              "                      0.0473, 0.0418, 0.0261, 0.0394, 0.0393, 0.0459, 0.0485, 0.0392, 0.0423,\n",
              "                      0.0311, 0.0464, 0.0495, 0.0486, 0.0493, 0.0401, 0.0474, 0.0471, 0.0490,\n",
              "                      0.0448, 0.0348, 0.0430, 0.0343], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.13.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  53,  56,  51,  56,  48,  50,  57,  51,  56,  52,  54,  49,  51,\n",
              "                       48,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.fc2.weight',\n",
              "              tensor([[ 26],\n",
              "                      [152],\n",
              "                      [183],\n",
              "                      ...,\n",
              "                      [ 80],\n",
              "                      [ 55],\n",
              "                      [147]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.fc2.bias',\n",
              "              tensor([ 0.0335, -0.0262,  0.0561,  ...,  0.0648, -0.0635, -0.0195],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.fc2.weight.absmax',\n",
              "              tensor([255, 255,   0,  ...,   0, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.13.fc2.weight.nested_absmax',\n",
              "              tensor([0.0602, 0.0591, 0.0574, 0.0552, 0.0460, 0.0538, 0.0630, 0.0370, 0.0630,\n",
              "                      0.0481, 0.0453, 0.0544, 0.0621, 0.0302, 0.0322, 0.0499, 0.0480, 0.0604,\n",
              "                      0.0451, 0.0626, 0.0492, 0.0491, 0.0461, 0.0621, 0.1000, 0.0496, 0.0416,\n",
              "                      0.0458, 0.0361, 0.0414, 0.0521, 0.0581, 0.0619, 0.0446, 0.0376, 0.0627,\n",
              "                      0.0267, 0.0466, 0.0621, 0.0377, 0.0513, 0.0444, 0.0630, 0.0514, 0.0627,\n",
              "                      0.0502, 0.0640, 0.0450, 0.0630, 0.0531, 0.0467, 0.0514, 0.0411, 0.0340,\n",
              "                      0.0287, 0.0632, 0.0416, 0.0477, 0.0280, 0.0619, 0.0392, 0.0339, 0.0439,\n",
              "                      0.0615, 0.0384, 0.0336, 0.0528, 0.0439, 0.0585, 0.0333, 0.0599, 0.0378,\n",
              "                      0.0624, 0.0455, 0.0503, 0.0629, 0.0367, 0.0458, 0.0382, 0.0392, 0.0598,\n",
              "                      0.0527, 0.0410, 0.0536, 0.0637, 0.0538, 0.0458, 0.0427, 0.0624, 0.0449,\n",
              "                      0.0381, 0.0348, 0.0628, 0.0566, 0.0513, 0.0361, 0.0624, 0.0486, 0.0514,\n",
              "                      0.0469, 0.0627, 0.0531, 0.0513, 0.0558, 0.0598, 0.0425, 0.0610, 0.0499,\n",
              "                      0.0477, 0.0630, 0.0433, 0.0413, 0.0619, 0.0365, 0.0403, 0.0619, 0.0571,\n",
              "                      0.0628, 0.1135, 0.0433, 0.0279, 0.0458, 0.0532, 0.0449, 0.0628, 0.0489,\n",
              "                      0.0630, 0.0524, 0.0321, 0.0630, 0.0469, 0.0587, 0.0610, 0.0423, 0.0464,\n",
              "                      0.0347, 0.0456, 0.0306, 0.0365, 0.0402, 0.0452, 0.0363, 0.0413, 0.0584,\n",
              "                      0.0540, 0.0630, 0.0616, 0.0488, 0.0300, 0.0512, 0.0565, 0.0466, 0.0305,\n",
              "                      0.0461, 0.0356, 0.0632, 0.0400, 0.0422, 0.0630, 0.0433, 0.0549, 0.0409,\n",
              "                      0.0478, 0.0628, 0.0322, 0.0380, 0.0529, 0.0386, 0.0503, 0.0505, 0.0524,\n",
              "                      0.0441, 0.0370, 0.0494, 0.0478, 0.0476, 0.0467, 0.0628, 0.0393, 0.0454,\n",
              "                      0.0477, 0.0400, 0.0359, 0.0316, 0.0530, 0.0355, 0.0599, 0.0615, 0.0355,\n",
              "                      0.0382, 0.0331, 0.0615, 0.0494, 0.0262, 0.0558, 0.0497, 0.0608, 0.0621,\n",
              "                      0.0495, 0.0474, 0.0478, 0.0444, 0.0383, 0.0424, 0.0406, 0.0593, 0.0585,\n",
              "                      0.0218, 0.0425, 0.0347, 0.0520, 0.0386, 0.0322, 0.0555, 0.0613, 0.0595,\n",
              "                      0.0505, 0.0458, 0.0630, 0.0457, 0.0430, 0.0462, 0.0359, 0.0598, 0.0392,\n",
              "                      0.0569, 0.0328, 0.0500, 0.0629, 0.0384, 0.0310, 0.0373, 0.0477, 0.0626,\n",
              "                      0.0380, 0.0392, 0.0317, 0.0499, 0.0396, 0.0427, 0.0392, 0.0599, 0.0497,\n",
              "                      0.0493, 0.0589, 0.0566, 0.0257, 0.0388, 0.0623, 0.0605, 0.0352, 0.0322,\n",
              "                      0.0469, 0.0600, 0.0630, 0.0568], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.13.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  49,  57,  53,  51,  54,  56,  50,  52,  53,  50,  52,  52,  48,\n",
              "                       50,  54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.13.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.13.final_layer_norm.bias',\n",
              "              tensor([ 0.0301, -0.0180, -0.0309,  ..., -0.0633, -0.0309, -0.0276],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.self_attn.k_proj.weight',\n",
              "              tensor([[ 18],\n",
              "                      [161],\n",
              "                      [158],\n",
              "                      ...,\n",
              "                      [104],\n",
              "                      [179],\n",
              "                      [  7]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0186, -0.0205, -0.0606,  ...,  0.0312,  0.0359,  0.0321],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ..., 255, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.14.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0398, 0.0378, 0.0392, 0.0396, 0.0399, 0.0393, 0.0394, 0.0393, 0.0353,\n",
              "                      0.0377, 0.0392, 0.0393, 0.0399, 0.0347, 0.0385, 0.0408, 0.0396, 0.0392,\n",
              "                      0.0394, 0.0398, 0.0401, 0.0397, 0.0397, 0.0397, 0.0398, 0.0392, 0.0396,\n",
              "                      0.0382, 0.0400, 0.0399, 0.0408, 0.0396, 0.0334, 0.0364, 0.0348, 0.0356,\n",
              "                      0.0399, 0.0395, 0.0400, 0.0397, 0.0413, 0.0402, 0.0400, 0.0415, 0.0399,\n",
              "                      0.0399, 0.0399, 0.0388, 0.0407, 0.0399, 0.0408, 0.0399, 0.0406, 0.0398,\n",
              "                      0.0405, 0.0398, 0.0406, 0.0401, 0.0400, 0.0404, 0.0395, 0.0413, 0.0383,\n",
              "                      0.0399], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.14.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  53,  49,  50,  56,  53,  53,  51,  50,  49,  49,  54,  56,  57,\n",
              "                      125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.self_attn.v_proj.weight',\n",
              "              tensor([[ 70],\n",
              "                      [230],\n",
              "                      [102],\n",
              "                      ...,\n",
              "                      [ 77],\n",
              "                      [185],\n",
              "                      [215]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.self_attn.v_proj.bias',\n",
              "              tensor([-0.0109,  0.0071, -0.0122,  ..., -0.0044,  0.0021, -0.0344],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.self_attn.v_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.14.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0366, 0.0467, 0.0438, 0.0471, 0.0338, 0.0283, 0.0376, 0.0366, 0.0363,\n",
              "                      0.0423, 0.0453, 0.0434, 0.0431, 0.0481, 0.0385, 0.0388, 0.0381, 0.0343,\n",
              "                      0.0356, 0.0331, 0.0356, 0.0429, 0.0429, 0.0499, 0.0440, 0.0438, 0.0343,\n",
              "                      0.0438, 0.0464, 0.0507, 0.0511, 0.0400, 0.0506, 0.0498, 0.0447, 0.0385,\n",
              "                      0.0494, 0.0387, 0.0394, 0.0382, 0.0338, 0.0322, 0.0273, 0.0414, 0.0293,\n",
              "                      0.0487, 0.0282, 0.0302, 0.0277, 0.0317, 0.0325, 0.0321, 0.0419, 0.0423,\n",
              "                      0.0476, 0.0459, 0.0316, 0.0330, 0.0349, 0.0336, 0.0330, 0.0407, 0.0449,\n",
              "                      0.0511], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.14.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  51,  56,  50,  50,  54,  48,  50,  54,  50,  57,  54,  54,  49,\n",
              "                       53,  54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.self_attn.q_proj.weight',\n",
              "              tensor([[ 80],\n",
              "                      [ 99],\n",
              "                      [174],\n",
              "                      ...,\n",
              "                      [146],\n",
              "                      [114],\n",
              "                      [222]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.self_attn.q_proj.bias',\n",
              "              tensor([-0.0341,  0.0530,  0.0651,  ...,  0.0007,  0.0224, -0.0396],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.self_attn.q_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.14.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0382, 0.0328, 0.0376, 0.0375, 0.0362, 0.0383, 0.0383, 0.0398, 0.0377,\n",
              "                      0.0375, 0.0372, 0.0380, 0.0372, 0.0312, 0.0317, 0.0381, 0.0372, 0.0387,\n",
              "                      0.0372, 0.0364, 0.0373, 0.0378, 0.0390, 0.0386, 0.0372, 0.0362, 0.0378,\n",
              "                      0.0380, 0.0373, 0.0373, 0.0386, 0.0365, 0.0358, 0.0323, 0.0369, 0.0383,\n",
              "                      0.0386, 0.0375, 0.0383, 0.0376, 0.0394, 0.0372, 0.0387, 0.0384, 0.0383,\n",
              "                      0.0382, 0.0383, 0.0382, 0.0380, 0.0381, 0.0383, 0.0387, 0.0354, 0.0383,\n",
              "                      0.0383, 0.0383, 0.0383, 0.0389, 0.0361, 0.0372, 0.0383, 0.0378, 0.0374,\n",
              "                      0.0397], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.14.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  54,  54,  56,  51,  50,  53,  56,  52,  49,  52,  50,  54,  56,\n",
              "                       53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.self_attn.out_proj.weight',\n",
              "              tensor([[229],\n",
              "                      [158],\n",
              "                      [126],\n",
              "                      ...,\n",
              "                      [ 89],\n",
              "                      [186],\n",
              "                      [177]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.self_attn.out_proj.bias',\n",
              "              tensor([-0.0120,  0.0374,  0.0341,  ...,  0.0625, -0.0310, -0.0297],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.self_attn.out_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.14.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0390, 0.0471, 0.0397, 0.0371, 0.0462, 0.0367, 0.0554, 0.0551, 0.0392,\n",
              "                      0.0346, 0.0474, 0.0534, 0.0457, 0.0492, 0.0517, 0.0338, 0.0423, 0.0313,\n",
              "                      0.0534, 0.0343, 0.0318, 0.0553, 0.0416, 0.0470, 0.0359, 0.0539, 0.0360,\n",
              "                      0.0478, 0.0504, 0.0606, 0.0332, 0.0383, 0.0418, 0.0420, 0.0346, 0.0304,\n",
              "                      0.0507, 0.0489, 0.0540, 0.0406, 0.0366, 0.0514, 0.0391, 0.0438, 0.0384,\n",
              "                      0.0400, 0.0407, 0.0454, 0.0455, 0.0294, 0.0426, 0.0345, 0.0539, 0.0463,\n",
              "                      0.0536, 0.0418, 0.0477, 0.0468, 0.0331, 0.0513, 0.0383, 0.0341, 0.0363,\n",
              "                      0.0393], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.14.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  48,  57,  54,  48,  53,  57,  54,  50,  48,  51,  56,  48,  52,\n",
              "                       48,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.self_attn_layer_norm.bias',\n",
              "              tensor([-0.0320, -0.0213, -0.0247,  ..., -0.0740,  0.0273,  0.0555],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.fc1.weight',\n",
              "              tensor([[ 57],\n",
              "                      [213],\n",
              "                      [ 71],\n",
              "                      ...,\n",
              "                      [ 72],\n",
              "                      [138],\n",
              "                      [ 44]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.fc1.bias',\n",
              "              tensor([-0.0157, -0.0196,  0.0112,  ..., -0.0305, -0.0454, -0.0598],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.fc1.weight.absmax',\n",
              "              tensor([255, 255,   0,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.14.fc1.weight.nested_absmax',\n",
              "              tensor([0.0422, 0.0333, 0.0479, 0.0345, 0.0383, 0.0482, 0.0417, 0.0359, 0.0485,\n",
              "                      0.0478, 0.0380, 0.0486, 0.0488, 0.0486, 0.0474, 0.0455, 0.0472, 0.0450,\n",
              "                      0.0394, 0.0444, 0.0345, 0.0363, 0.0472, 0.0487, 0.0475, 0.0388, 0.0448,\n",
              "                      0.0348, 0.0460, 0.0453, 0.0315, 0.0452, 0.0428, 0.0489, 0.0360, 0.0339,\n",
              "                      0.0483, 0.0462, 0.0489, 0.0481, 0.0483, 0.0468, 0.0479, 0.0464, 0.0373,\n",
              "                      0.0460, 0.0425, 0.0384, 0.0479, 0.0334, 0.0385, 0.0422, 0.0350, 0.0431,\n",
              "                      0.0417, 0.0430, 0.0471, 0.0375, 0.0366, 0.0408, 0.0451, 0.0450, 0.0380,\n",
              "                      0.0381, 0.0488, 0.0446, 0.0447, 0.0450, 0.0448, 0.0334, 0.0339, 0.0479,\n",
              "                      0.0350, 0.0400, 0.0324, 0.0362, 0.0487, 0.0477, 0.0387, 0.0443, 0.0330,\n",
              "                      0.0444, 0.0477, 0.0479, 0.0486, 0.0368, 0.0432, 0.0331, 0.0384, 0.0487,\n",
              "                      0.0301, 0.0348, 0.0471, 0.0427, 0.0498, 0.0485, 0.0483, 0.0330, 0.0479,\n",
              "                      0.0471, 0.0489, 0.0443, 0.0407, 0.0403, 0.0482, 0.0408, 0.0344, 0.0487,\n",
              "                      0.0405, 0.0322, 0.0402, 0.0391, 0.0486, 0.0393, 0.0469, 0.0387, 0.0428,\n",
              "                      0.0471, 0.0469, 0.0414, 0.0429, 0.0446, 0.0433, 0.0390, 0.0350, 0.0445,\n",
              "                      0.0411, 0.0303, 0.0392, 0.0389, 0.0477, 0.0378, 0.0345, 0.0458, 0.0339,\n",
              "                      0.0446, 0.0370, 0.0491, 0.0465, 0.0447, 0.0473, 0.0466, 0.0491, 0.0363,\n",
              "                      0.0487, 0.0477, 0.0374, 0.0367, 0.0384, 0.0486, 0.0465, 0.0416, 0.0431,\n",
              "                      0.0482, 0.0472, 0.0410, 0.0465, 0.0402, 0.0329, 0.0474, 0.0444, 0.0485,\n",
              "                      0.0462, 0.0480, 0.0467, 0.0381, 0.0485, 0.0431, 0.0441, 0.0439, 0.0485,\n",
              "                      0.0450, 0.0470, 0.0445, 0.0484, 0.0482, 0.0485, 0.0419, 0.0464, 0.0493,\n",
              "                      0.0489, 0.0441, 0.0464, 0.0468, 0.0489, 0.0416, 0.0474, 0.0419, 0.0411,\n",
              "                      0.0357, 0.0469, 0.0480, 0.0483, 0.0486, 0.0424, 0.0426, 0.0460, 0.0359,\n",
              "                      0.0350, 0.0464, 0.0405, 0.0439, 0.0360, 0.0357, 0.0339, 0.0338, 0.0320,\n",
              "                      0.0350, 0.0460, 0.0467, 0.0484, 0.0325, 0.0410, 0.0300, 0.0337, 0.0474,\n",
              "                      0.0442, 0.0453, 0.0441, 0.0415, 0.0482, 0.0426, 0.0432, 0.0413, 0.0488,\n",
              "                      0.0470, 0.0486, 0.0488, 0.0419, 0.0350, 0.0347, 0.0317, 0.0367, 0.0356,\n",
              "                      0.0472, 0.0475, 0.0343, 0.0485, 0.0474, 0.0492, 0.0488, 0.0409, 0.0472,\n",
              "                      0.0440, 0.0410, 0.0486, 0.0419, 0.0487, 0.0376, 0.0463, 0.0487, 0.0354,\n",
              "                      0.0430, 0.0476, 0.0485, 0.0492], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.14.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  54,  50,  57,  53,  52,  54,  53,  50,  51,  48,  57,  52,  49,\n",
              "                       55,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.fc2.weight',\n",
              "              tensor([[106],\n",
              "                      [179],\n",
              "                      [124],\n",
              "                      ...,\n",
              "                      [ 51],\n",
              "                      [117],\n",
              "                      [171]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.fc2.bias',\n",
              "              tensor([ 0.0087, -0.0305,  0.0401,  ...,  0.0631, -0.0630, -0.0199],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.fc2.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.14.fc2.weight.nested_absmax',\n",
              "              tensor([0.0475, 0.0375, 0.0419, 0.0638, 0.0411, 0.0322, 0.0306, 0.0521, 0.0430,\n",
              "                      0.0438, 0.0417, 0.0574, 0.0505, 0.0376, 0.0356, 0.0573, 0.0314, 0.0504,\n",
              "                      0.0630, 0.0636, 0.0411, 0.0419, 0.0310, 0.0502, 0.0645, 0.0619, 0.0583,\n",
              "                      0.0510, 0.0327, 0.0321, 0.0285, 0.0438, 0.0512, 0.0384, 0.0433, 0.0541,\n",
              "                      0.0448, 0.0525, 0.0296, 0.0369, 0.0457, 0.0291, 0.0578, 0.0391, 0.0421,\n",
              "                      0.0405, 0.0638, 0.0449, 0.0335, 0.0444, 0.0456, 0.0627, 0.0424, 0.0466,\n",
              "                      0.0411, 0.0599, 0.0400, 0.0530, 0.0360, 0.0472, 0.0631, 0.0435, 0.0490,\n",
              "                      0.0503, 0.0320, 0.0427, 0.0311, 0.0374, 0.0598, 0.0624, 0.0624, 0.0508,\n",
              "                      0.0365, 0.0292, 0.0235, 0.0509, 0.0363, 0.0620, 0.0471, 0.0403, 0.0598,\n",
              "                      0.0610, 0.0338, 0.0533, 0.0651, 0.0449, 0.0474, 0.0487, 0.0322, 0.0338,\n",
              "                      0.0395, 0.0455, 0.0638, 0.0495, 0.0308, 0.0421, 0.0285, 0.0551, 0.0493,\n",
              "                      0.0474, 0.0526, 0.0338, 0.0589, 0.0333, 0.0587, 0.0635, 0.0626, 0.0539,\n",
              "                      0.0341, 0.0640, 0.0346, 0.0449, 0.0546, 0.0627, 0.0256, 0.0345, 0.0469,\n",
              "                      0.0433, 0.1888, 0.0339, 0.0353, 0.0399, 0.0280, 0.0628, 0.0436, 0.0525,\n",
              "                      0.0306, 0.0549, 0.0565, 0.0386, 0.0518, 0.0458, 0.0563, 0.0373, 0.0591,\n",
              "                      0.0423, 0.0635, 0.0629, 0.0425, 0.0635, 0.0460, 0.0486, 0.0416, 0.0520,\n",
              "                      0.0634, 0.0319, 0.0314, 0.0390, 0.0375, 0.0298, 0.0391, 0.0372, 0.0418,\n",
              "                      0.0469, 0.0519, 0.0424, 0.0546, 0.0569, 0.0546, 0.0616, 0.0527, 0.0489,\n",
              "                      0.0346, 0.0382, 0.0635, 0.0527, 0.0620, 0.0635, 0.0350, 0.0404, 0.0496,\n",
              "                      0.0479, 0.0599, 0.0525, 0.0526, 0.0440, 0.0403, 0.0445, 0.0363, 0.0371,\n",
              "                      0.0638, 0.0630, 0.0629, 0.0622, 0.0468, 0.0446, 0.0568, 0.0442, 0.0552,\n",
              "                      0.0544, 0.0368, 0.0443, 0.0359, 0.0508, 0.0364, 0.0286, 0.0257, 0.0413,\n",
              "                      0.0357, 0.0503, 0.0509, 0.0475, 0.0326, 0.0415, 0.0631, 0.0459, 0.0491,\n",
              "                      0.0458, 0.0393, 0.0315, 0.0637, 0.0486, 0.0333, 0.0413, 0.0366, 0.0638,\n",
              "                      0.0619, 0.0561, 0.0630, 0.0375, 0.0636, 0.0631, 0.0447, 0.0430, 0.0546,\n",
              "                      0.0334, 0.0551, 0.0389, 0.0358, 0.0547, 0.0385, 0.0410, 0.0527, 0.0404,\n",
              "                      0.0480, 0.0363, 0.0389, 0.0479, 0.0598, 0.0455, 0.0334, 0.0360, 0.0588,\n",
              "                      0.0300, 0.0442, 0.0515, 0.0388, 0.0612, 0.0280, 0.0591, 0.0311, 0.0509,\n",
              "                      0.0317, 0.0456, 0.0586, 0.0350], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.14.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  49,  49,  53,  50,  50,  57,  48,  53,  53,  50,  56,  53,  52,\n",
              "                       53,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.14.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.14.final_layer_norm.bias',\n",
              "              tensor([ 0.0211, -0.0145, -0.0313,  ..., -0.0636, -0.0306, -0.0396],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.self_attn.k_proj.weight',\n",
              "              tensor([[ 64],\n",
              "                      [ 52],\n",
              "                      [156],\n",
              "                      ...,\n",
              "                      [ 22],\n",
              "                      [102],\n",
              "                      [152]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0331, -0.0358,  0.0437,  ...,  0.0163, -0.0301, -0.0061],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.15.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0395, 0.0397, 0.0394, 0.0413, 0.0383, 0.0389, 0.0394, 0.0377, 0.0449,\n",
              "                      0.0332, 0.0497, 0.0472, 0.0394, 0.0397, 0.0391, 0.0390, 0.0402, 0.0391,\n",
              "                      0.0386, 0.0396, 0.0389, 0.0376, 0.0393, 0.0395, 0.0397, 0.0394, 0.0393,\n",
              "                      0.0391, 0.0395, 0.0391, 0.0391, 0.0393, 0.0397, 0.0400, 0.0391, 0.0396,\n",
              "                      0.0396, 0.0372, 0.0389, 0.0394, 0.0386, 0.0405, 0.0381, 0.0388, 0.0397,\n",
              "                      0.0401, 0.0394, 0.0396, 0.0347, 0.0355, 0.0402, 0.0374, 0.0391, 0.0393,\n",
              "                      0.0394, 0.0391, 0.0410, 0.0391, 0.0389, 0.0402, 0.0385, 0.0348, 0.0389,\n",
              "                      0.0324], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.15.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  53,  56,  54,  57,  54,  54,  57,  57,  49,  52,  50,  52,  53,\n",
              "                       54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.self_attn.v_proj.weight',\n",
              "              tensor([[ 77],\n",
              "                      [215],\n",
              "                      [158],\n",
              "                      ...,\n",
              "                      [ 38],\n",
              "                      [211],\n",
              "                      [221]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.self_attn.v_proj.bias',\n",
              "              tensor([ 0.0245, -0.0058,  0.0116,  ...,  0.0008, -0.0050, -0.0059],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.self_attn.v_proj.weight.absmax',\n",
              "              tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.15.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0390, 0.0309, 0.0409, 0.0316, 0.0374, 0.0254, 0.0271, 0.0427, 0.0265,\n",
              "                      0.0262, 0.0315, 0.0235, 0.0334, 0.0496, 0.0492, 0.0428, 0.0495, 0.0520,\n",
              "                      0.0423, 0.0524, 0.0409, 0.0406, 0.0348, 0.0455, 0.0439, 0.0452, 0.0263,\n",
              "                      0.0340, 0.0507, 0.0492, 0.0429, 0.0503, 0.0522, 0.0467, 0.0515, 0.0509,\n",
              "                      0.0493, 0.0434, 0.0433, 0.0332, 0.0529, 0.0517, 0.0498, 0.0467, 0.0347,\n",
              "                      0.0359, 0.0285, 0.0322, 0.0472, 0.0352, 0.0525, 0.0373, 0.0355, 0.0343,\n",
              "                      0.0362, 0.0345, 0.0436, 0.0345, 0.0339, 0.0397, 0.0265, 0.0314, 0.0307,\n",
              "                      0.0301], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.15.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  50,  48,  53,  50,  56,  53,  49,  51,  49,  57,  51,  49,  51,\n",
              "                       48,  53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.self_attn.q_proj.weight',\n",
              "              tensor([[ 50],\n",
              "                      [101],\n",
              "                      [244],\n",
              "                      ...,\n",
              "                      [ 41],\n",
              "                      [ 75],\n",
              "                      [150]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.self_attn.q_proj.bias',\n",
              "              tensor([ 0.0611, -0.0780,  0.0249,  ...,  0.0774,  0.0568,  0.0080],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.self_attn.q_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ..., 255,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.15.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0374, 0.0379, 0.0376, 0.0354, 0.0323, 0.0370, 0.0320, 0.0372, 0.0424,\n",
              "                      0.0392, 0.0484, 0.0381, 0.0376, 0.0366, 0.0376, 0.0375, 0.0375, 0.0359,\n",
              "                      0.0376, 0.0355, 0.0360, 0.0372, 0.0364, 0.0364, 0.0350, 0.0374, 0.0375,\n",
              "                      0.0361, 0.0370, 0.0379, 0.0373, 0.0376, 0.0384, 0.0383, 0.0378, 0.0378,\n",
              "                      0.0373, 0.0376, 0.0372, 0.0364, 0.0326, 0.0378, 0.0375, 0.0347, 0.0384,\n",
              "                      0.0364, 0.0376, 0.0380, 0.0375, 0.0365, 0.0339, 0.0379, 0.0362, 0.0380,\n",
              "                      0.0396, 0.0376, 0.0375, 0.0379, 0.0387, 0.0376, 0.0360, 0.0373, 0.0367,\n",
              "                      0.0372], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.15.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  55,  52,  55,  54,  56,  48,  52,  56,  53,  50,  52,  56,  53,\n",
              "                       54,  54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.self_attn.out_proj.weight',\n",
              "              tensor([[150],\n",
              "                      [169],\n",
              "                      [ 87],\n",
              "                      ...,\n",
              "                      [103],\n",
              "                      [ 76],\n",
              "                      [ 25]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.self_attn.out_proj.bias',\n",
              "              tensor([-0.0066,  0.0219,  0.0210,  ...,  0.0330, -0.0055, -0.0321],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.self_attn.out_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ..., 255, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.15.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0579, 0.0473, 0.0302, 0.0368, 0.0425, 0.0575, 0.0602, 0.0582, 0.0579,\n",
              "                      0.0530, 0.0380, 0.0433, 0.0577, 0.0301, 0.0474, 0.0502, 0.0435, 0.0354,\n",
              "                      0.0366, 0.0409, 0.0513, 0.0616, 0.0387, 0.0404, 0.0416, 0.0579, 0.0560,\n",
              "                      0.0455, 0.0580, 0.1128, 0.0398, 0.0410, 0.0399, 0.0468, 0.0330, 0.0425,\n",
              "                      0.0444, 0.0353, 0.0581, 0.0533, 0.0518, 0.0365, 0.0568, 0.0329, 0.0414,\n",
              "                      0.0366, 0.0324, 0.0383, 0.0378, 0.0577, 0.0553, 0.0504, 0.0585, 0.0503,\n",
              "                      0.0604, 0.0413, 0.0493, 0.0587, 0.0508, 0.0417, 0.0548, 0.0338, 0.0345,\n",
              "                      0.0369], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.15.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  55,  48,  49,  52,  57,  50,  53,  49,  56,  49,  56,  54,  53,\n",
              "                       54,  57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.self_attn_layer_norm.bias',\n",
              "              tensor([ 0.0017,  0.0047, -0.0346,  ..., -0.0681,  0.0316,  0.0444],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.fc1.weight',\n",
              "              tensor([[124],\n",
              "                      [163],\n",
              "                      [ 97],\n",
              "                      ...,\n",
              "                      [250],\n",
              "                      [171],\n",
              "                      [ 70]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.fc1.bias',\n",
              "              tensor([ 0.0059,  0.0015, -0.0122,  ..., -0.0004, -0.0163, -0.0297],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.fc1.weight.absmax',\n",
              "              tensor([255,   0,   0,  ..., 255,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.15.fc1.weight.nested_absmax',\n",
              "              tensor([0.0450, 0.0420, 0.0370, 0.0430, 0.0450, 0.0471, 0.0439, 0.0458, 0.0470,\n",
              "                      0.0453, 0.0480, 0.0425, 0.0483, 0.0450, 0.0347, 0.0496, 0.0467, 0.0394,\n",
              "                      0.0482, 0.0430, 0.0452, 0.0441, 0.0417, 0.0436, 0.0480, 0.0359, 0.0469,\n",
              "                      0.0484, 0.0361, 0.0479, 0.0348, 0.0397, 0.0484, 0.0478, 0.0475, 0.0403,\n",
              "                      0.0384, 0.0464, 0.0365, 0.0378, 0.0474, 0.0435, 0.0467, 0.0474, 0.0354,\n",
              "                      0.0431, 0.0400, 0.0482, 0.0484, 0.0483, 0.0480, 0.0411, 0.0408, 0.0482,\n",
              "                      0.0454, 0.0464, 0.0481, 0.0358, 0.0418, 0.0484, 0.0484, 0.0493, 0.0338,\n",
              "                      0.0351, 0.0483, 0.0411, 0.0404, 0.0468, 0.0328, 0.0477, 0.0484, 0.0468,\n",
              "                      0.0422, 0.0479, 0.0478, 0.0428, 0.0460, 0.0296, 0.0482, 0.0430, 0.0336,\n",
              "                      0.0484, 0.0435, 0.0436, 0.0317, 0.0484, 0.0460, 0.0460, 0.0417, 0.0460,\n",
              "                      0.0346, 0.0369, 0.0434, 0.0395, 0.0341, 0.0485, 0.0490, 0.0483, 0.0483,\n",
              "                      0.0484, 0.0418, 0.0459, 0.0397, 0.0482, 0.0463, 0.0406, 0.0483, 0.0332,\n",
              "                      0.0417, 0.0421, 0.0476, 0.0478, 0.0482, 0.0406, 0.0320, 0.0476, 0.0478,\n",
              "                      0.0358, 0.0305, 0.0469, 0.0427, 0.0388, 0.0480, 0.0422, 0.0427, 0.0482,\n",
              "                      0.0468, 0.0430, 0.0380, 0.0478, 0.0484, 0.0494, 0.0437, 0.0437, 0.0383,\n",
              "                      0.0398, 0.0452, 0.0481, 0.0313, 0.0419, 0.0474, 0.0484, 0.0459, 0.0435,\n",
              "                      0.0471, 0.0378, 0.0351, 0.0484, 0.0484, 0.0483, 0.0453, 0.0478, 0.0479,\n",
              "                      0.0421, 0.0405, 0.0474, 0.0461, 0.0431, 0.0446, 0.0382, 0.0456, 0.0458,\n",
              "                      0.0414, 0.0476, 0.0358, 0.0430, 0.0367, 0.0484, 0.0459, 0.0399, 0.0476,\n",
              "                      0.0456, 0.0469, 0.0358, 0.0466, 0.0324, 0.0283, 0.0474, 0.0297, 0.0485,\n",
              "                      0.0484, 0.0412, 0.0468, 0.0464, 0.0303, 0.0407, 0.0393, 0.0457, 0.0461,\n",
              "                      0.0477, 0.0477, 0.0454, 0.0467, 0.0378, 0.0399, 0.0453, 0.0391, 0.0422,\n",
              "                      0.0489, 0.0363, 0.0480, 0.0463, 0.0484, 0.0400, 0.0463, 0.0477, 0.0464,\n",
              "                      0.0350, 0.0397, 0.0337, 0.0460, 0.0460, 0.0474, 0.0485, 0.0399, 0.0392,\n",
              "                      0.0392, 0.0447, 0.0508, 0.0416, 0.0374, 0.0484, 0.0383, 0.0484, 0.0434,\n",
              "                      0.0380, 0.0377, 0.0377, 0.0355, 0.0431, 0.0413, 0.0483, 0.0350, 0.0483,\n",
              "                      0.0319, 0.0485, 0.0362, 0.0430, 0.0345, 0.0457, 0.0335, 0.0386, 0.0473,\n",
              "                      0.0420, 0.0439, 0.0447, 0.0480, 0.0459, 0.0322, 0.0480, 0.0480, 0.0406,\n",
              "                      0.0420, 0.0447, 0.0441, 0.0436], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.15.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  54,  54,  48,  52,  49,  56,  48,  48,  51,  55,  57,  55,  53,\n",
              "                       51,  49, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.fc2.weight',\n",
              "              tensor([[202],\n",
              "                      [ 89],\n",
              "                      [134],\n",
              "                      ...,\n",
              "                      [169],\n",
              "                      [141],\n",
              "                      [178]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.fc2.bias',\n",
              "              tensor([-0.0243, -0.0295,  0.0212,  ...,  0.0645, -0.0632, -0.0431],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.fc2.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.15.fc2.weight.nested_absmax',\n",
              "              tensor([0.0639, 0.0381, 0.0540, 0.0624, 0.0559, 0.0464, 0.0300, 0.0507, 0.0382,\n",
              "                      0.0259, 0.0389, 0.0440, 0.0564, 0.0313, 0.0570, 0.0415, 0.0327, 0.0316,\n",
              "                      0.0451, 0.0644, 0.0496, 0.0325, 0.0438, 0.0467, 0.0657, 0.0489, 0.0303,\n",
              "                      0.0340, 0.0532, 0.0535, 0.0375, 0.0414, 0.0333, 0.0431, 0.0380, 0.0466,\n",
              "                      0.0261, 0.0466, 0.0205, 0.0495, 0.0463, 0.0513, 0.0618, 0.0319, 0.0387,\n",
              "                      0.0360, 0.0640, 0.0361, 0.0370, 0.0588, 0.0404, 0.0519, 0.0588, 0.0474,\n",
              "                      0.0438, 0.0419, 0.0427, 0.0453, 0.0391, 0.0326, 0.0330, 0.0623, 0.0337,\n",
              "                      0.0598, 0.0499, 0.0394, 0.0480, 0.0485, 0.0471, 0.0433, 0.0632, 0.0532,\n",
              "                      0.0419, 0.0419, 0.0307, 0.0642, 0.0399, 0.0368, 0.0369, 0.0566, 0.0613,\n",
              "                      0.0447, 0.0636, 0.0380, 0.0667, 0.0640, 0.0629, 0.0524, 0.0610, 0.0306,\n",
              "                      0.0446, 0.0308, 0.0257, 0.0389, 0.0343, 0.0519, 0.0581, 0.0472, 0.0290,\n",
              "                      0.0458, 0.0640, 0.0409, 0.0476, 0.0472, 0.0570, 0.0408, 0.0365, 0.0529,\n",
              "                      0.0410, 0.0389, 0.0313, 0.0257, 0.0448, 0.0644, 0.0414, 0.0553, 0.0238,\n",
              "                      0.0329, 0.1894, 0.0484, 0.0627, 0.0628, 0.0541, 0.0495, 0.0422, 0.0383,\n",
              "                      0.0425, 0.0519, 0.0328, 0.0496, 0.0640, 0.0472, 0.0416, 0.0489, 0.0280,\n",
              "                      0.0649, 0.0354, 0.0361, 0.0507, 0.0562, 0.0446, 0.0507, 0.0331, 0.0466,\n",
              "                      0.0555, 0.0534, 0.0619, 0.0430, 0.0323, 0.0641, 0.0475, 0.0563, 0.0450,\n",
              "                      0.0516, 0.0365, 0.0643, 0.0644, 0.0547, 0.0372, 0.0440, 0.0582, 0.0547,\n",
              "                      0.0405, 0.0427, 0.0631, 0.0421, 0.0496, 0.0461, 0.0449, 0.0488, 0.0482,\n",
              "                      0.0427, 0.0494, 0.0567, 0.0632, 0.0502, 0.0585, 0.0420, 0.0619, 0.0616,\n",
              "                      0.0617, 0.0535, 0.0335, 0.0417, 0.0285, 0.0365, 0.0588, 0.0360, 0.0368,\n",
              "                      0.0635, 0.0635, 0.0415, 0.0372, 0.0631, 0.0617, 0.0270, 0.0433, 0.0644,\n",
              "                      0.0346, 0.0360, 0.0241, 0.0273, 0.0343, 0.0621, 0.0400, 0.0421, 0.0387,\n",
              "                      0.0365, 0.0542, 0.0502, 0.0644, 0.0382, 0.0405, 0.0620, 0.0330, 0.0373,\n",
              "                      0.0476, 0.0360, 0.0666, 0.0253, 0.0443, 0.0639, 0.0475, 0.0282, 0.0375,\n",
              "                      0.0289, 0.0646, 0.0463, 0.0307, 0.0564, 0.0506, 0.0479, 0.0485, 0.0557,\n",
              "                      0.0468, 0.0590, 0.0365, 0.0521, 0.0551, 0.0463, 0.0290, 0.0638, 0.0399,\n",
              "                      0.0430, 0.0400, 0.0430, 0.0482, 0.0362, 0.0379, 0.0278, 0.0379, 0.0407,\n",
              "                      0.0452, 0.0432, 0.0426, 0.0461], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.15.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  48,  54,  48,  48,  51,  49,  56,  48,  49,  52,  54,  50,  49,\n",
              "                       55,  51,  53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.15.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.15.final_layer_norm.bias',\n",
              "              tensor([ 0.0195, -0.0222, -0.0164,  ..., -0.0628, -0.0362, -0.0266],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.self_attn.k_proj.weight',\n",
              "              tensor([[ 29],\n",
              "                      [  6],\n",
              "                      [153],\n",
              "                      ...,\n",
              "                      [ 88],\n",
              "                      [207],\n",
              "                      [103]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.self_attn.k_proj.bias',\n",
              "              tensor([-0.0314,  0.0316,  0.0112,  ..., -0.0204, -0.0625, -0.0312],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.16.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0374, 0.0380, 0.0360, 0.0378, 0.0377, 0.0376, 0.0381, 0.0378, 0.0373,\n",
              "                      0.0381, 0.0374, 0.0382, 0.0379, 0.0379, 0.0382, 0.0374, 0.0370, 0.0378,\n",
              "                      0.0363, 0.0378, 0.0396, 0.0385, 0.0379, 0.0397, 0.0368, 0.0370, 0.0378,\n",
              "                      0.0374, 0.0370, 0.0372, 0.0364, 0.0381, 0.0380, 0.0381, 0.0378, 0.0377,\n",
              "                      0.0350, 0.0363, 0.0357, 0.0373, 0.0339, 0.0337, 0.0378, 0.0370, 0.0385,\n",
              "                      0.0390, 0.0392, 0.0384, 0.0318, 0.0387, 0.0420, 0.0453, 0.0371, 0.0373,\n",
              "                      0.0376, 0.0344, 0.0376, 0.0365, 0.0379, 0.0380, 0.0384, 0.0378, 0.0381,\n",
              "                      0.0380], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.16.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  55,  50,  52,  57,  56,  57,  55,  52,  50,  48,  52,  48,  54,\n",
              "                       51,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.self_attn.v_proj.weight',\n",
              "              tensor([[ 76],\n",
              "                      [168],\n",
              "                      [120],\n",
              "                      ...,\n",
              "                      [ 54],\n",
              "                      [198],\n",
              "                      [  0]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.self_attn.v_proj.bias',\n",
              "              tensor([-0.0200,  0.0143, -0.0014,  ...,  0.0174,  0.0026, -0.0171],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.self_attn.v_proj.weight.absmax',\n",
              "              tensor([255,   0, 255,  ..., 255, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.16.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0492, 0.0493, 0.0496, 0.0501, 0.0425, 0.0492, 0.0490, 0.0401, 0.0460,\n",
              "                      0.0431, 0.0476, 0.0372, 0.0457, 0.0393, 0.0471, 0.0330, 0.0492, 0.0404,\n",
              "                      0.0374, 0.0368, 0.0367, 0.0363, 0.0377, 0.0363, 0.0288, 0.0409, 0.0486,\n",
              "                      0.0299, 0.0474, 0.0301, 0.0290, 0.0488, 0.0351, 0.0360, 0.0348, 0.0301,\n",
              "                      0.0450, 0.0445, 0.0474, 0.0438, 0.0473, 0.0455, 0.0486, 0.0467, 0.0486,\n",
              "                      0.0438, 0.0500, 0.0451, 0.0464, 0.0321, 0.0385, 0.0473, 0.0428, 0.0474,\n",
              "                      0.0391, 0.0413, 0.0441, 0.0290, 0.0275, 0.0379, 0.0287, 0.0327, 0.0263,\n",
              "                      0.0348], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.16.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  53,  55,  55,  56,  50,  54,  48,  56,  50,  55,  48,  54,  52,\n",
              "                       53,  49, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.self_attn.q_proj.weight',\n",
              "              tensor([[145],\n",
              "                      [160],\n",
              "                      [ 63],\n",
              "                      ...,\n",
              "                      [ 47],\n",
              "                      [ 21],\n",
              "                      [ 50]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.self_attn.q_proj.bias',\n",
              "              tensor([ 0.0124, -0.0177,  0.0858,  ...,  0.0030,  0.0622, -0.0745],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.self_attn.q_proj.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.16.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0354, 0.0354, 0.0358, 0.0352, 0.0345, 0.0365, 0.0352, 0.0352, 0.0363,\n",
              "                      0.0359, 0.0382, 0.0355, 0.0350, 0.0355, 0.0364, 0.0365, 0.0349, 0.0319,\n",
              "                      0.0352, 0.0351, 0.0359, 0.0358, 0.0352, 0.0355, 0.0348, 0.0354, 0.0352,\n",
              "                      0.0354, 0.0350, 0.0354, 0.0353, 0.0352, 0.0351, 0.0353, 0.0354, 0.0359,\n",
              "                      0.0352, 0.0327, 0.0344, 0.0325, 0.0355, 0.0338, 0.0337, 0.0354, 0.0361,\n",
              "                      0.0369, 0.0356, 0.0356, 0.0386, 0.0354, 0.0358, 0.0423, 0.0329, 0.0356,\n",
              "                      0.0339, 0.0322, 0.0356, 0.0338, 0.0356, 0.0356, 0.0342, 0.0352, 0.0342,\n",
              "                      0.0344], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.16.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  57,  54,  48,  48,  48,  55,  56,  55,  54,  49,  53,  55,  55,\n",
              "                       54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.self_attn.out_proj.weight',\n",
              "              tensor([[140],\n",
              "                      [ 36],\n",
              "                      [242],\n",
              "                      ...,\n",
              "                      [101],\n",
              "                      [218],\n",
              "                      [ 50]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.self_attn.out_proj.bias',\n",
              "              tensor([-0.0141,  0.0435,  0.0137,  ...,  0.0638, -0.0112, -0.0274],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.self_attn.out_proj.weight.absmax',\n",
              "              tensor([255,   0, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.16.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0535, 0.0524, 0.0472, 0.0489, 0.0475, 0.0520, 0.0553, 0.0536, 0.0529,\n",
              "                      0.0453, 0.0542, 0.0532, 0.0544, 0.0532, 0.0557, 0.0389, 0.0535, 0.0543,\n",
              "                      0.0374, 0.0523, 0.0441, 0.0568, 0.0382, 0.0397, 0.0515, 0.0552, 0.0488,\n",
              "                      0.0542, 0.0532, 0.0909, 0.0513, 0.0418, 0.0533, 0.0525, 0.0436, 0.0470,\n",
              "                      0.0459, 0.0577, 0.0538, 0.0501, 0.0554, 0.0558, 0.0554, 0.0532, 0.0474,\n",
              "                      0.0537, 0.0536, 0.0505, 0.0513, 0.0535, 0.0549, 0.0488, 0.0536, 0.0348,\n",
              "                      0.0566, 0.0542, 0.0426, 0.0532, 0.0366, 0.0512, 0.0541, 0.0407, 0.0542,\n",
              "                      0.0547], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.16.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  49,  54,  52,  57,  53,  51,  54,  52,  57,  48,  52,  52,  48,\n",
              "                       51,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.self_attn_layer_norm.bias',\n",
              "              tensor([ 0.0263,  0.0100, -0.0236,  ..., -0.0688,  0.0215,  0.0295],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.fc1.weight',\n",
              "              tensor([[ 70],\n",
              "                      [100],\n",
              "                      [212],\n",
              "                      ...,\n",
              "                      [ 18],\n",
              "                      [ 74],\n",
              "                      [117]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.fc1.bias',\n",
              "              tensor([-0.0299, -0.0059, -0.0382,  ..., -0.0277, -0.0343, -0.0038],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.fc1.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ..., 255,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.16.fc1.weight.nested_absmax',\n",
              "              tensor([0.0432, 0.0470, 0.0355, 0.0491, 0.0485, 0.0465, 0.0471, 0.0486, 0.0343,\n",
              "                      0.0346, 0.0418, 0.0352, 0.0486, 0.0482, 0.0497, 0.0384, 0.0409, 0.0486,\n",
              "                      0.0478, 0.0453, 0.0468, 0.0476, 0.0375, 0.0426, 0.0431, 0.0384, 0.0341,\n",
              "                      0.0406, 0.0481, 0.0365, 0.0390, 0.0467, 0.0314, 0.0470, 0.0372, 0.0485,\n",
              "                      0.0414, 0.0376, 0.0476, 0.0398, 0.0465, 0.0460, 0.0391, 0.0491, 0.0440,\n",
              "                      0.0438, 0.0416, 0.0419, 0.0345, 0.0486, 0.0470, 0.0360, 0.0432, 0.0398,\n",
              "                      0.0340, 0.0380, 0.0417, 0.0482, 0.0420, 0.0314, 0.0446, 0.0474, 0.0388,\n",
              "                      0.0451, 0.0395, 0.0470, 0.0408, 0.0444, 0.0409, 0.0466, 0.0482, 0.0474,\n",
              "                      0.0477, 0.0337, 0.0366, 0.0469, 0.0379, 0.0480, 0.0378, 0.0369, 0.0485,\n",
              "                      0.0498, 0.0480, 0.0434, 0.0308, 0.0467, 0.0403, 0.0471, 0.0394, 0.0440,\n",
              "                      0.0463, 0.0461, 0.0473, 0.0444, 0.0411, 0.0439, 0.0372, 0.0436, 0.0485,\n",
              "                      0.0333, 0.0459, 0.0479, 0.0469, 0.0411, 0.0416, 0.0381, 0.0477, 0.0357,\n",
              "                      0.0382, 0.0453, 0.0340, 0.0395, 0.0487, 0.0297, 0.0485, 0.0364, 0.0384,\n",
              "                      0.0476, 0.0372, 0.0492, 0.0477, 0.0460, 0.0483, 0.0485, 0.0487, 0.0375,\n",
              "                      0.0471, 0.0300, 0.0380, 0.0350, 0.0457, 0.0378, 0.0354, 0.0463, 0.0484,\n",
              "                      0.0389, 0.0485, 0.0453, 0.0332, 0.0480, 0.0398, 0.0433, 0.0484, 0.0419,\n",
              "                      0.0376, 0.0300, 0.0383, 0.0400, 0.0431, 0.0478, 0.0485, 0.0484, 0.0427,\n",
              "                      0.0347, 0.0461, 0.0464, 0.0477, 0.0409, 0.0388, 0.0482, 0.0483, 0.0502,\n",
              "                      0.0403, 0.0477, 0.0398, 0.0445, 0.0488, 0.0383, 0.0367, 0.0445, 0.0474,\n",
              "                      0.0446, 0.0430, 0.0472, 0.0452, 0.0486, 0.0402, 0.0419, 0.0483, 0.0352,\n",
              "                      0.0485, 0.0395, 0.0437, 0.0471, 0.0486, 0.0492, 0.0361, 0.0468, 0.0414,\n",
              "                      0.0386, 0.0329, 0.0304, 0.0472, 0.0435, 0.0455, 0.0427, 0.0439, 0.0429,\n",
              "                      0.0389, 0.0481, 0.0483, 0.0485, 0.0478, 0.0414, 0.0483, 0.0484, 0.0483,\n",
              "                      0.0307, 0.0318, 0.0484, 0.0432, 0.0472, 0.0343, 0.0456, 0.0494, 0.0485,\n",
              "                      0.0481, 0.0372, 0.0481, 0.0477, 0.0452, 0.0457, 0.0487, 0.0472, 0.0462,\n",
              "                      0.0341, 0.0419, 0.0392, 0.0444, 0.0479, 0.0475, 0.0398, 0.0477, 0.0387,\n",
              "                      0.0412, 0.0450, 0.0483, 0.0393, 0.0435, 0.0445, 0.0452, 0.0427, 0.0466,\n",
              "                      0.0345, 0.0374, 0.0486, 0.0416, 0.0410, 0.0458, 0.0441, 0.0404, 0.0402,\n",
              "                      0.0406, 0.0416, 0.0467, 0.0485], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.16.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  54,  53,  52,  55,  55,  49,  50,  48,  56,  55,  54,  51,  49,\n",
              "                       50,  51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.fc2.weight',\n",
              "              tensor([[236],\n",
              "                      [ 75],\n",
              "                      [ 90],\n",
              "                      ...,\n",
              "                      [184],\n",
              "                      [118],\n",
              "                      [167]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.fc2.bias',\n",
              "              tensor([-0.0224, -0.0149,  0.0207,  ...,  0.0653, -0.0624, -0.0552],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.fc2.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.16.fc2.weight.nested_absmax',\n",
              "              tensor([0.0558, 0.0533, 0.0649, 0.0383, 0.0406, 0.0382, 0.0349, 0.0537, 0.0493,\n",
              "                      0.0356, 0.0467, 0.0444, 0.0507, 0.0279, 0.0499, 0.0389, 0.0308, 0.0550,\n",
              "                      0.0647, 0.0456, 0.0614, 0.0334, 0.0477, 0.0334, 0.0679, 0.0381, 0.0460,\n",
              "                      0.0375, 0.0406, 0.0339, 0.0462, 0.0649, 0.0360, 0.0644, 0.0586, 0.0649,\n",
              "                      0.0331, 0.0461, 0.0643, 0.0356, 0.0423, 0.0443, 0.0243, 0.0344, 0.0467,\n",
              "                      0.0619, 0.0651, 0.0535, 0.0474, 0.0530, 0.0378, 0.0481, 0.0433, 0.0324,\n",
              "                      0.0308, 0.0303, 0.0350, 0.0480, 0.0376, 0.0433, 0.0442, 0.0361, 0.0646,\n",
              "                      0.0345, 0.0364, 0.0234, 0.0459, 0.0332, 0.0351, 0.0245, 0.0335, 0.0495,\n",
              "                      0.0471, 0.0480, 0.0285, 0.0426, 0.0616, 0.0328, 0.0554, 0.0387, 0.0368,\n",
              "                      0.0514, 0.0642, 0.0632, 0.0682, 0.0597, 0.0558, 0.0589, 0.0615, 0.0336,\n",
              "                      0.0351, 0.0503, 0.0381, 0.0344, 0.0263, 0.0442, 0.0484, 0.0469, 0.0459,\n",
              "                      0.0450, 0.0649, 0.0530, 0.0453, 0.0345, 0.0484, 0.0434, 0.0356, 0.0440,\n",
              "                      0.0387, 0.0649, 0.0476, 0.0330, 0.0363, 0.0648, 0.0649, 0.0342, 0.0435,\n",
              "                      0.0464, 0.1899, 0.0441, 0.0401, 0.0356, 0.0546, 0.0375, 0.0386, 0.0347,\n",
              "                      0.0322, 0.0311, 0.0371, 0.0370, 0.0640, 0.0324, 0.0505, 0.0304, 0.0491,\n",
              "                      0.0405, 0.0339, 0.0599, 0.0647, 0.0289, 0.0431, 0.0494, 0.0321, 0.0287,\n",
              "                      0.0295, 0.0511, 0.0477, 0.0477, 0.0594, 0.0442, 0.0314, 0.0444, 0.0466,\n",
              "                      0.0425, 0.0418, 0.0623, 0.0427, 0.0374, 0.0362, 0.0604, 0.0491, 0.0383,\n",
              "                      0.0532, 0.0609, 0.0314, 0.0391, 0.0209, 0.0320, 0.0448, 0.0289, 0.0414,\n",
              "                      0.0305, 0.0364, 0.0465, 0.0379, 0.0359, 0.0355, 0.0398, 0.0353, 0.0342,\n",
              "                      0.0451, 0.0524, 0.0319, 0.0489, 0.0555, 0.0406, 0.0636, 0.0308, 0.0647,\n",
              "                      0.0306, 0.0566, 0.0648, 0.0384, 0.0438, 0.0488, 0.0470, 0.0335, 0.0331,\n",
              "                      0.0383, 0.0404, 0.0254, 0.0624, 0.0587, 0.0494, 0.0407, 0.0485, 0.0489,\n",
              "                      0.0373, 0.0645, 0.0363, 0.0651, 0.0336, 0.0646, 0.0378, 0.0455, 0.0442,\n",
              "                      0.0552, 0.0279, 0.0786, 0.0449, 0.0499, 0.0441, 0.0641, 0.0416, 0.0321,\n",
              "                      0.0373, 0.0368, 0.0581, 0.0578, 0.0649, 0.0647, 0.0644, 0.0310, 0.0602,\n",
              "                      0.0326, 0.0630, 0.0405, 0.0514, 0.0423, 0.0367, 0.0403, 0.0641, 0.0376,\n",
              "                      0.0380, 0.0401, 0.0267, 0.0339, 0.0453, 0.0503, 0.0519, 0.0455, 0.0483,\n",
              "                      0.0416, 0.0343, 0.0410, 0.0411], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.16.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  48,  48,  54,  56,  48,  49,  49,  50,  56,  51,  56,  55,  52,\n",
              "                       53,  49, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.16.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.16.final_layer_norm.bias',\n",
              "              tensor([-0.0098, -0.0159, -0.0090,  ..., -0.0635, -0.0384, -0.0376],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.self_attn.k_proj.weight',\n",
              "              tensor([[ 40],\n",
              "                      [ 43],\n",
              "                      [233],\n",
              "                      ...,\n",
              "                      [177],\n",
              "                      [137],\n",
              "                      [148]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.self_attn.k_proj.bias',\n",
              "              tensor([-0.0154, -0.0703,  0.0317,  ..., -0.0140, -0.0188, -0.0170],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.17.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0382, 0.0386, 0.0365, 0.0387, 0.0391, 0.0390, 0.0385, 0.0392, 0.0488,\n",
              "                      0.0467, 0.0471, 0.0449, 0.0387, 0.0385, 0.0372, 0.0386, 0.0387, 0.0385,\n",
              "                      0.0390, 0.0354, 0.0392, 0.0386, 0.0386, 0.0384, 0.0382, 0.0360, 0.0383,\n",
              "                      0.0390, 0.0382, 0.0378, 0.0387, 0.0384, 0.0341, 0.0375, 0.0331, 0.0351,\n",
              "                      0.0344, 0.0395, 0.0376, 0.0384, 0.0380, 0.0372, 0.0386, 0.0383, 0.0390,\n",
              "                      0.0380, 0.0383, 0.0364, 0.0389, 0.0390, 0.0385, 0.0386, 0.0385, 0.0383,\n",
              "                      0.0328, 0.0385, 0.0403, 0.0386, 0.0383, 0.0383, 0.0385, 0.0376, 0.0358,\n",
              "                      0.0390], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.17.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  54,  52,  57,  53,  56,  52,  54,  53,  48,  57,  57,  51,  51,\n",
              "                       52,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.self_attn.v_proj.weight',\n",
              "              tensor([[116],\n",
              "                      [235],\n",
              "                      [ 81],\n",
              "                      ...,\n",
              "                      [ 84],\n",
              "                      [200],\n",
              "                      [ 98]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.self_attn.v_proj.bias',\n",
              "              tensor([-0.0299, -0.0148, -0.0037,  ..., -0.0063,  0.0237, -0.0043],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.self_attn.v_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ...,   0,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.17.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0411, 0.0500, 0.0496, 0.0490, 0.0345, 0.0339, 0.0337, 0.0321, 0.0487,\n",
              "                      0.0481, 0.0409, 0.0491, 0.0279, 0.0351, 0.0267, 0.0279, 0.0256, 0.0255,\n",
              "                      0.0394, 0.0223, 0.0305, 0.0289, 0.0293, 0.0317, 0.0498, 0.0459, 0.0490,\n",
              "                      0.0469, 0.0500, 0.0492, 0.0497, 0.0501, 0.0483, 0.0467, 0.0433, 0.0417,\n",
              "                      0.0495, 0.0489, 0.0472, 0.0399, 0.0320, 0.0281, 0.0271, 0.0263, 0.0497,\n",
              "                      0.0478, 0.0499, 0.0505, 0.0306, 0.0339, 0.0391, 0.0387, 0.0456, 0.0289,\n",
              "                      0.0326, 0.0489, 0.0495, 0.0466, 0.0419, 0.0406, 0.0412, 0.0492, 0.0358,\n",
              "                      0.0462], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.17.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  52,  57,  48,  50,  48,  53,  55,  54,  52,  55,  55,  48,  53,\n",
              "                       48,  56, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.self_attn.q_proj.weight',\n",
              "              tensor([[ 49],\n",
              "                      [ 50],\n",
              "                      [151],\n",
              "                      ...,\n",
              "                      [ 53],\n",
              "                      [194],\n",
              "                      [179]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.self_attn.q_proj.bias',\n",
              "              tensor([ 0.0188,  0.0540, -0.0464,  ..., -0.0276, -0.0210, -0.0007],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.self_attn.q_proj.weight.absmax',\n",
              "              tensor([255,   0, 255,  ..., 255,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.17.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0355, 0.0358, 0.0355, 0.0350, 0.0356, 0.0361, 0.0356, 0.0355, 0.0372,\n",
              "                      0.0429, 0.0478, 0.0435, 0.0360, 0.0355, 0.0355, 0.0350, 0.0355, 0.0363,\n",
              "                      0.0353, 0.0361, 0.0370, 0.0355, 0.0356, 0.0355, 0.0359, 0.0352, 0.0359,\n",
              "                      0.0361, 0.0359, 0.0363, 0.0358, 0.0356, 0.0338, 0.0356, 0.0322, 0.0366,\n",
              "                      0.0356, 0.0365, 0.0352, 0.0358, 0.0344, 0.0331, 0.0360, 0.0358, 0.0352,\n",
              "                      0.0354, 0.0342, 0.0358, 0.0356, 0.0361, 0.0355, 0.0353, 0.0355, 0.0350,\n",
              "                      0.0355, 0.0342, 0.0355, 0.0367, 0.0338, 0.0359, 0.0359, 0.0342, 0.0364,\n",
              "                      0.0356], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.17.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  57,  52,  55,  54,  55,  54,  52,  50,  48,  50,  49,  49,  55,\n",
              "                       57,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.self_attn.out_proj.weight',\n",
              "              tensor([[ 84],\n",
              "                      [170],\n",
              "                      [161],\n",
              "                      ...,\n",
              "                      [150],\n",
              "                      [ 99],\n",
              "                      [ 67]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.self_attn.out_proj.bias',\n",
              "              tensor([-0.0258,  0.0234,  0.0099,  ...,  0.0600, -0.0025, -0.0316],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.self_attn.out_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ...,   7, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.17.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0554, 0.0363, 0.0300, 0.0536, 0.0423, 0.0449, 0.0581, 0.0559, 0.0552,\n",
              "                      0.0418, 0.0497, 0.0398, 0.0552, 0.0436, 0.0401, 0.0511, 0.0355, 0.0526,\n",
              "                      0.0503, 0.0539, 0.0353, 0.0589, 0.0337, 0.0321, 0.0528, 0.0552, 0.0528,\n",
              "                      0.0531, 0.0551, 0.0983, 0.0435, 0.0384, 0.0338, 0.0325, 0.0492, 0.0351,\n",
              "                      0.0357, 0.0462, 0.0551, 0.0450, 0.0449, 0.0513, 0.0437, 0.0435, 0.0381,\n",
              "                      0.0352, 0.0410, 0.0550, 0.0495, 0.0502, 0.0550, 0.0442, 0.0559, 0.0554,\n",
              "                      0.0608, 0.0418, 0.0363, 0.0487, 0.0551, 0.0538, 0.0327, 0.0407, 0.0468,\n",
              "                      0.0530], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.17.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  57,  57,  50,  48,  48,  50,  53,  55,  54,  53,  56,  57,  53,\n",
              "                       56,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.self_attn_layer_norm.bias',\n",
              "              tensor([ 0.0279, -0.0033,  0.0016,  ..., -0.0684,  0.0266,  0.0313],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.fc1.weight',\n",
              "              tensor([[ 52],\n",
              "                      [149],\n",
              "                      [ 92],\n",
              "                      ...,\n",
              "                      [ 55],\n",
              "                      [187],\n",
              "                      [128]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.fc1.bias',\n",
              "              tensor([-0.0358, -0.0229, -0.0149,  ..., -0.0305, -0.0265,  0.0044],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.fc1.weight.absmax',\n",
              "              tensor([255, 255,   0,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.17.fc1.weight.nested_absmax',\n",
              "              tensor([0.0416, 0.0396, 0.0498, 0.0353, 0.0399, 0.0455, 0.0377, 0.0406, 0.0483,\n",
              "                      0.0482, 0.0503, 0.0483, 0.0375, 0.0488, 0.0411, 0.0386, 0.0483, 0.0477,\n",
              "                      0.0387, 0.0358, 0.0407, 0.0403, 0.0409, 0.0477, 0.0408, 0.0392, 0.0399,\n",
              "                      0.0434, 0.0471, 0.0258, 0.0475, 0.0472, 0.0469, 0.0360, 0.0385, 0.0472,\n",
              "                      0.0403, 0.0457, 0.0425, 0.0299, 0.0475, 0.0390, 0.0303, 0.0491, 0.0355,\n",
              "                      0.0488, 0.0359, 0.0414, 0.0442, 0.0463, 0.0486, 0.0464, 0.0430, 0.0375,\n",
              "                      0.0397, 0.0486, 0.0411, 0.0424, 0.0483, 0.0452, 0.0479, 0.0469, 0.0463,\n",
              "                      0.0420, 0.0388, 0.0397, 0.0360, 0.0455, 0.0482, 0.0454, 0.0307, 0.0408,\n",
              "                      0.0457, 0.0482, 0.0308, 0.0467, 0.0455, 0.0449, 0.0463, 0.0446, 0.0483,\n",
              "                      0.0414, 0.0486, 0.0459, 0.0481, 0.0380, 0.0471, 0.0449, 0.0385, 0.0487,\n",
              "                      0.0488, 0.0483, 0.0333, 0.0322, 0.0480, 0.0403, 0.0482, 0.0487, 0.0482,\n",
              "                      0.0416, 0.0396, 0.0436, 0.0411, 0.0483, 0.0490, 0.0417, 0.0466, 0.0431,\n",
              "                      0.0485, 0.0331, 0.0312, 0.0425, 0.0377, 0.0483, 0.0366, 0.0411, 0.0474,\n",
              "                      0.0414, 0.0421, 0.0396, 0.0337, 0.0485, 0.0485, 0.0477, 0.0469, 0.0407,\n",
              "                      0.0425, 0.0456, 0.0455, 0.0442, 0.0470, 0.0372, 0.0452, 0.0358, 0.0482,\n",
              "                      0.0372, 0.0444, 0.0464, 0.0488, 0.0284, 0.0472, 0.0440, 0.0453, 0.0357,\n",
              "                      0.0318, 0.0487, 0.0458, 0.0499, 0.0476, 0.0345, 0.0483, 0.0483, 0.0322,\n",
              "                      0.0380, 0.0463, 0.0475, 0.0384, 0.0370, 0.0361, 0.0374, 0.0479, 0.0436,\n",
              "                      0.0441, 0.0415, 0.0401, 0.0426, 0.0411, 0.0405, 0.0420, 0.0487, 0.0345,\n",
              "                      0.0428, 0.0483, 0.0286, 0.0482, 0.0472, 0.0364, 0.0492, 0.0453, 0.0451,\n",
              "                      0.0452, 0.0411, 0.0452, 0.0490, 0.0338, 0.0387, 0.0397, 0.0390, 0.0483,\n",
              "                      0.0483, 0.0403, 0.0471, 0.0416, 0.0405, 0.0341, 0.0483, 0.0483, 0.0485,\n",
              "                      0.0337, 0.0399, 0.0464, 0.0391, 0.0394, 0.0483, 0.0436, 0.0400, 0.0444,\n",
              "                      0.0479, 0.0450, 0.0462, 0.0483, 0.0454, 0.0443, 0.0358, 0.0367, 0.0459,\n",
              "                      0.0389, 0.0402, 0.0468, 0.0482, 0.0483, 0.0434, 0.0388, 0.0455, 0.0472,\n",
              "                      0.0437, 0.0422, 0.0316, 0.0371, 0.0376, 0.0458, 0.0368, 0.0460, 0.0361,\n",
              "                      0.0463, 0.0455, 0.0488, 0.0482, 0.0409, 0.0383, 0.0470, 0.0472, 0.0452,\n",
              "                      0.0302, 0.0468, 0.0483, 0.0430, 0.0475, 0.0483, 0.0355, 0.0452, 0.0485,\n",
              "                      0.0393, 0.0414, 0.0443, 0.0349], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.17.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  54,  54,  53,  56,  53,  51,  50,  48,  50,  51,  52,  50,  57,\n",
              "                       56,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.fc2.weight',\n",
              "              tensor([[186],\n",
              "                      [ 74],\n",
              "                      [102],\n",
              "                      ...,\n",
              "                      [207],\n",
              "                      [ 87],\n",
              "                      [ 13]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.fc2.bias',\n",
              "              tensor([-0.0027, -0.0177,  0.0118,  ...,  0.0350, -0.0272, -0.0132],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.fc2.weight.absmax',\n",
              "              tensor([255,   0, 255,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.17.fc2.weight.nested_absmax',\n",
              "              tensor([0.0454, 0.0536, 0.0660, 0.0293, 0.0351, 0.0319, 0.0411, 0.0530, 0.0427,\n",
              "                      0.0478, 0.0335, 0.0278, 0.0624, 0.0396, 0.0385, 0.0286, 0.0308, 0.0308,\n",
              "                      0.0422, 0.0428, 0.0440, 0.0436, 0.0408, 0.0368, 0.0690, 0.0292, 0.0438,\n",
              "                      0.0269, 0.0320, 0.0264, 0.0423, 0.0660, 0.0653, 0.0376, 0.0408, 0.0575,\n",
              "                      0.0524, 0.0402, 0.0337, 0.0535, 0.0336, 0.0398, 0.0290, 0.0538, 0.0342,\n",
              "                      0.0612, 0.0660, 0.0289, 0.0604, 0.0342, 0.0416, 0.0660, 0.0342, 0.0449,\n",
              "                      0.0577, 0.0420, 0.0347, 0.0510, 0.0553, 0.0300, 0.0397, 0.0342, 0.0330,\n",
              "                      0.0260, 0.0404, 0.0337, 0.0439, 0.0402, 0.0554, 0.0405, 0.0301, 0.0241,\n",
              "                      0.0258, 0.0440, 0.0303, 0.0319, 0.0412, 0.0456, 0.0333, 0.0515, 0.0386,\n",
              "                      0.0489, 0.0494, 0.0336, 0.0691, 0.0571, 0.0500, 0.0351, 0.0264, 0.0308,\n",
              "                      0.0363, 0.0344, 0.0450, 0.0377, 0.0433, 0.0316, 0.0400, 0.0393, 0.0370,\n",
              "                      0.0499, 0.0659, 0.0336, 0.0654, 0.0638, 0.0377, 0.0304, 0.0273, 0.0326,\n",
              "                      0.0363, 0.0589, 0.0470, 0.0514, 0.0408, 0.0659, 0.0463, 0.0334, 0.0451,\n",
              "                      0.0488, 0.1910, 0.0394, 0.0287, 0.0508, 0.0372, 0.0373, 0.0218, 0.0395,\n",
              "                      0.0288, 0.0356, 0.0351, 0.0353, 0.0642, 0.0329, 0.0361, 0.0311, 0.0453,\n",
              "                      0.0415, 0.0295, 0.0656, 0.0367, 0.0367, 0.0386, 0.0472, 0.0353, 0.0334,\n",
              "                      0.0328, 0.0355, 0.0445, 0.0357, 0.0345, 0.0511, 0.0338, 0.0285, 0.0394,\n",
              "                      0.0437, 0.0611, 0.0576, 0.0343, 0.0336, 0.0589, 0.0464, 0.0383, 0.0448,\n",
              "                      0.0395, 0.0407, 0.0497, 0.0499, 0.0457, 0.0269, 0.0659, 0.0511, 0.0470,\n",
              "                      0.0388, 0.0431, 0.0500, 0.0346, 0.0315, 0.0241, 0.0389, 0.0445, 0.0394,\n",
              "                      0.0351, 0.0538, 0.0294, 0.0406, 0.0300, 0.0387, 0.0427, 0.0651, 0.0349,\n",
              "                      0.0328, 0.0405, 0.0474, 0.0295, 0.0329, 0.0392, 0.0373, 0.0270, 0.0290,\n",
              "                      0.0365, 0.0523, 0.0376, 0.0411, 0.0485, 0.0367, 0.0407, 0.0303, 0.0539,\n",
              "                      0.0290, 0.0420, 0.0308, 0.0660, 0.0532, 0.0361, 0.0447, 0.0318, 0.0483,\n",
              "                      0.0397, 0.0660, 0.0840, 0.0378, 0.0585, 0.0375, 0.0276, 0.0362, 0.0647,\n",
              "                      0.0616, 0.0557, 0.0557, 0.0305, 0.0657, 0.0365, 0.0329, 0.0345, 0.0395,\n",
              "                      0.0569, 0.0420, 0.0356, 0.0502, 0.0425, 0.0389, 0.0340, 0.0461, 0.0276,\n",
              "                      0.0342, 0.0585, 0.0340, 0.0371, 0.0413, 0.0424, 0.0429, 0.0364, 0.0474,\n",
              "                      0.0329, 0.0271, 0.0253, 0.0660], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.17.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       53,  57,  48,  51,  50,  48,  52,  53,  51,  48,  52,  55,  55,  53,\n",
              "                       50,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.17.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.17.final_layer_norm.bias',\n",
              "              tensor([-0.0118,  0.0029, -0.0206,  ..., -0.0681, -0.0331, -0.0334],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.self_attn.k_proj.weight',\n",
              "              tensor([[ 81],\n",
              "                      [ 82],\n",
              "                      [ 73],\n",
              "                      ...,\n",
              "                      [110],\n",
              "                      [145],\n",
              "                      [216]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0677,  0.0187,  0.0314,  ..., -0.0184,  0.0190,  0.0184],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.18.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0383, 0.0403, 0.0385, 0.0380, 0.0438, 0.0376, 0.0353, 0.0343, 0.0380,\n",
              "                      0.0385, 0.0366, 0.0379, 0.0528, 0.0526, 0.0523, 0.0543, 0.0385, 0.0394,\n",
              "                      0.0385, 0.0382, 0.0389, 0.0377, 0.0383, 0.0375, 0.0387, 0.0383, 0.0389,\n",
              "                      0.0391, 0.0386, 0.0381, 0.0385, 0.0385, 0.0387, 0.0388, 0.0372, 0.0382,\n",
              "                      0.0360, 0.0382, 0.0389, 0.0386, 0.0386, 0.0387, 0.0379, 0.0385, 0.0383,\n",
              "                      0.0385, 0.0383, 0.0387, 0.0380, 0.0386, 0.0379, 0.0380, 0.0383, 0.0383,\n",
              "                      0.0385, 0.0386, 0.0386, 0.0383, 0.0380, 0.0381, 0.0383, 0.0383, 0.0398,\n",
              "                      0.0386], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.18.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  54,  54,  54,  48,  52,  54,  55,  48,  56,  56,  50,  50,  50,\n",
              "                       53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.self_attn.v_proj.weight',\n",
              "              tensor([[128],\n",
              "                      [ 91],\n",
              "                      [172],\n",
              "                      ...,\n",
              "                      [129],\n",
              "                      [ 25],\n",
              "                      [245]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.self_attn.v_proj.bias',\n",
              "              tensor([-0.0186,  0.0040,  0.0181,  ...,  0.0003, -0.0111,  0.0004],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.self_attn.v_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.18.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0496, 0.0369, 0.0449, 0.0451, 0.0345, 0.0302, 0.0412, 0.0491, 0.0474,\n",
              "                      0.0457, 0.0412, 0.0463, 0.0497, 0.0415, 0.0416, 0.0326, 0.0423, 0.0352,\n",
              "                      0.0339, 0.0336, 0.0429, 0.0448, 0.0464, 0.0460, 0.0403, 0.0304, 0.0342,\n",
              "                      0.0321, 0.0453, 0.0380, 0.0444, 0.0341, 0.0443, 0.0325, 0.0412, 0.0423,\n",
              "                      0.0317, 0.0275, 0.0480, 0.0352, 0.0435, 0.0451, 0.0344, 0.0427, 0.0300,\n",
              "                      0.0338, 0.0338, 0.0296, 0.0339, 0.0448, 0.0354, 0.0393, 0.0360, 0.0381,\n",
              "                      0.0349, 0.0437, 0.0496, 0.0430, 0.0376, 0.0374, 0.0482, 0.0487, 0.0379,\n",
              "                      0.0497], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.18.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  53,  51,  48,  51,  49,  57,  54,  57,  48,  55,  48,  52,  51,\n",
              "                       52,  54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.self_attn.q_proj.weight',\n",
              "              tensor([[ 93],\n",
              "                      [220],\n",
              "                      [107],\n",
              "                      ...,\n",
              "                      [169],\n",
              "                      [205],\n",
              "                      [183]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.self_attn.q_proj.bias',\n",
              "              tensor([-0.0654,  0.0270, -0.0287,  ..., -0.0216,  0.0608, -0.0707],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.self_attn.q_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.18.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0343, 0.0347, 0.0347, 0.0343, 0.0340, 0.0349, 0.0343, 0.0357, 0.0338,\n",
              "                      0.0384, 0.0349, 0.0343, 0.0490, 0.0570, 0.0584, 0.0514, 0.0363, 0.0347,\n",
              "                      0.0357, 0.0351, 0.0345, 0.0345, 0.0365, 0.0346, 0.0343, 0.0334, 0.0329,\n",
              "                      0.0335, 0.0347, 0.0347, 0.0345, 0.0368, 0.0343, 0.0345, 0.0353, 0.0354,\n",
              "                      0.0337, 0.0347, 0.0319, 0.0338, 0.0345, 0.0341, 0.0343, 0.0342, 0.0343,\n",
              "                      0.0347, 0.0312, 0.0345, 0.0345, 0.0354, 0.0345, 0.0345, 0.0346, 0.0343,\n",
              "                      0.0345, 0.0343, 0.0338, 0.0345, 0.0334, 0.0349, 0.0343, 0.0365, 0.0353,\n",
              "                      0.0347], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.18.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       57,  48,  54,  55,  48,  55,  55,  57,  51,  52,  55,  52,  49,  57,\n",
              "                       55,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.self_attn.out_proj.weight',\n",
              "              tensor([[  4],\n",
              "                      [ 13],\n",
              "                      [227],\n",
              "                      ...,\n",
              "                      [218],\n",
              "                      [ 34],\n",
              "                      [ 18]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.self_attn.out_proj.bias',\n",
              "              tensor([-0.0082, -0.0012,  0.0118,  ...,  0.0624, -0.0086, -0.0050],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.self_attn.out_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.18.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0555, 0.0481, 0.0427, 0.0338, 0.0494, 0.0513, 0.0609, 0.0557, 0.0555,\n",
              "                      0.0308, 0.0373, 0.0391, 0.0312, 0.0434, 0.0383, 0.0313, 0.0515, 0.0429,\n",
              "                      0.0343, 0.0322, 0.0414, 0.0573, 0.0486, 0.0416, 0.0436, 0.0554, 0.0349,\n",
              "                      0.0449, 0.0554, 0.0858, 0.0295, 0.0507, 0.0416, 0.0399, 0.0318, 0.0488,\n",
              "                      0.0557, 0.0527, 0.0552, 0.0413, 0.0414, 0.0422, 0.0442, 0.0363, 0.0348,\n",
              "                      0.0395, 0.0460, 0.0362, 0.0317, 0.0455, 0.0388, 0.0303, 0.0563, 0.0335,\n",
              "                      0.0660, 0.0306, 0.0402, 0.0327, 0.0316, 0.0435, 0.0381, 0.0429, 0.0488,\n",
              "                      0.0406], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.18.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  57,  53,  55,  53,  48,  55,  49,  51,  51,  52,  56,  51,  56,\n",
              "                       56,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.self_attn_layer_norm.bias',\n",
              "              tensor([ 0.0264,  0.0257, -0.0234,  ..., -0.0693,  0.0100,  0.0107],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.fc1.weight',\n",
              "              tensor([[  0],\n",
              "                      [124],\n",
              "                      [ 69],\n",
              "                      ...,\n",
              "                      [148],\n",
              "                      [249],\n",
              "                      [129]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.fc1.bias',\n",
              "              tensor([-0.0128, -0.0235, -0.0352,  ..., -0.0136, -0.0031,  0.0124],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.fc1.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ..., 255, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.18.fc1.weight.nested_absmax',\n",
              "              tensor([0.0485, 0.0476, 0.0481, 0.0377, 0.0393, 0.0413, 0.0314, 0.0330, 0.0486,\n",
              "                      0.0482, 0.0485, 0.0485, 0.0490, 0.0485, 0.0474, 0.0444, 0.0358, 0.0472,\n",
              "                      0.0487, 0.0492, 0.0429, 0.0473, 0.0460, 0.0487, 0.0360, 0.0482, 0.0362,\n",
              "                      0.0351, 0.0372, 0.0484, 0.0480, 0.0365, 0.0467, 0.0485, 0.0446, 0.0445,\n",
              "                      0.0469, 0.0491, 0.0425, 0.0435, 0.0477, 0.0482, 0.0448, 0.0342, 0.0444,\n",
              "                      0.0391, 0.0492, 0.0422, 0.0397, 0.0365, 0.0424, 0.0483, 0.0464, 0.0463,\n",
              "                      0.0334, 0.0436, 0.0405, 0.0315, 0.0433, 0.0451, 0.0482, 0.0405, 0.0405,\n",
              "                      0.0487, 0.0451, 0.0471, 0.0474, 0.0437, 0.0339, 0.0468, 0.0484, 0.0349,\n",
              "                      0.0466, 0.0484, 0.0430, 0.0386, 0.0473, 0.0485, 0.0435, 0.0418, 0.0492,\n",
              "                      0.0474, 0.0366, 0.0487, 0.0475, 0.0477, 0.0416, 0.0406, 0.0444, 0.0475,\n",
              "                      0.0490, 0.0490, 0.0478, 0.0476, 0.0487, 0.0484, 0.0452, 0.0479, 0.0427,\n",
              "                      0.0376, 0.0344, 0.0413, 0.0440, 0.0431, 0.0483, 0.0373, 0.0393, 0.0412,\n",
              "                      0.0503, 0.0348, 0.0487, 0.0480, 0.0465, 0.0423, 0.0358, 0.0472, 0.0487,\n",
              "                      0.0440, 0.0419, 0.0445, 0.0350, 0.0339, 0.0316, 0.0493, 0.0400, 0.0491,\n",
              "                      0.0482, 0.0438, 0.0469, 0.0398, 0.0449, 0.0357, 0.0487, 0.0385, 0.0487,\n",
              "                      0.0445, 0.0412, 0.0477, 0.0461, 0.0478, 0.0377, 0.0474, 0.0459, 0.0330,\n",
              "                      0.0419, 0.0413, 0.0486, 0.0441, 0.0488, 0.0407, 0.0492, 0.0485, 0.0480,\n",
              "                      0.0357, 0.0376, 0.0429, 0.0483, 0.0483, 0.0462, 0.0470, 0.0448, 0.0350,\n",
              "                      0.0390, 0.0487, 0.0444, 0.0452, 0.0450, 0.0452, 0.0456, 0.0451, 0.0461,\n",
              "                      0.0485, 0.0468, 0.0490, 0.0490, 0.0455, 0.0478, 0.0380, 0.0362, 0.0474,\n",
              "                      0.0451, 0.0365, 0.0477, 0.0390, 0.0487, 0.0443, 0.0495, 0.0492, 0.0473,\n",
              "                      0.0379, 0.0397, 0.0444, 0.0437, 0.0479, 0.0447, 0.0448, 0.0417, 0.0320,\n",
              "                      0.0487, 0.0296, 0.0482, 0.0487, 0.0391, 0.0331, 0.0490, 0.0439, 0.0444,\n",
              "                      0.0398, 0.0488, 0.0414, 0.0448, 0.0435, 0.0454, 0.0477, 0.0487, 0.0487,\n",
              "                      0.0470, 0.0410, 0.0492, 0.0355, 0.0451, 0.0479, 0.0460, 0.0401, 0.0398,\n",
              "                      0.0487, 0.0356, 0.0473, 0.0404, 0.0483, 0.0487, 0.0398, 0.0311, 0.0456,\n",
              "                      0.0426, 0.0491, 0.0386, 0.0463, 0.0484, 0.0434, 0.0410, 0.0455, 0.0386,\n",
              "                      0.0477, 0.0483, 0.0429, 0.0487, 0.0486, 0.0433, 0.0439, 0.0358, 0.0382,\n",
              "                      0.0357, 0.0383, 0.0395, 0.0496], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.18.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  54,  50,  55,  57,  52,  51,  49,  53,  56,  49,  52,  57,  55,\n",
              "                       49,  57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.fc2.weight',\n",
              "              tensor([[229],\n",
              "                      [ 89],\n",
              "                      [145],\n",
              "                      ...,\n",
              "                      [246],\n",
              "                      [ 88],\n",
              "                      [134]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.fc2.bias',\n",
              "              tensor([-0.0180, -0.0219,  0.0145,  ...,  0.0111, -0.0309, -0.0056],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.fc2.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ...,   0,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.18.fc2.weight.nested_absmax',\n",
              "              tensor([0.0437, 0.0267, 0.0676, 0.0516, 0.0316, 0.0302, 0.0403, 0.0477, 0.0294,\n",
              "                      0.0306, 0.0610, 0.0350, 0.0394, 0.0402, 0.0205, 0.0402, 0.0336, 0.0406,\n",
              "                      0.0656, 0.0317, 0.0357, 0.0402, 0.0429, 0.0336, 0.0687, 0.0549, 0.0366,\n",
              "                      0.0418, 0.0434, 0.0440, 0.0290, 0.0671, 0.0226, 0.0398, 0.0384, 0.0665,\n",
              "                      0.0299, 0.0346, 0.0553, 0.0282, 0.0370, 0.0292, 0.0318, 0.0279, 0.0398,\n",
              "                      0.0388, 0.0460, 0.0421, 0.0354, 0.0576, 0.0358, 0.0655, 0.0633, 0.0454,\n",
              "                      0.0401, 0.0267, 0.0530, 0.0307, 0.0577, 0.0327, 0.0422, 0.0320, 0.0391,\n",
              "                      0.0455, 0.0471, 0.0337, 0.0476, 0.0400, 0.0373, 0.0396, 0.0462, 0.0509,\n",
              "                      0.0397, 0.0316, 0.0577, 0.0267, 0.0479, 0.0331, 0.0377, 0.0665, 0.0433,\n",
              "                      0.0431, 0.0369, 0.0653, 0.0670, 0.0253, 0.0260, 0.0628, 0.0568, 0.0369,\n",
              "                      0.0458, 0.0355, 0.0617, 0.0321, 0.0407, 0.0357, 0.0319, 0.0648, 0.0323,\n",
              "                      0.0404, 0.0664, 0.0309, 0.0451, 0.0460, 0.0391, 0.0316, 0.0336, 0.0432,\n",
              "                      0.0314, 0.0415, 0.0333, 0.0598, 0.0457, 0.0664, 0.0356, 0.0477, 0.0378,\n",
              "                      0.0532, 0.1298, 0.0355, 0.0458, 0.0385, 0.0341, 0.0421, 0.0488, 0.0651,\n",
              "                      0.0330, 0.0350, 0.0374, 0.0260, 0.0293, 0.0393, 0.0535, 0.0322, 0.0399,\n",
              "                      0.0486, 0.0271, 0.0294, 0.0364, 0.0440, 0.0414, 0.0350, 0.0388, 0.0455,\n",
              "                      0.0344, 0.0515, 0.0372, 0.0422, 0.0393, 0.0298, 0.0446, 0.0430, 0.0445,\n",
              "                      0.0484, 0.0664, 0.0600, 0.0335, 0.0456, 0.0401, 0.0278, 0.0349, 0.0321,\n",
              "                      0.0463, 0.0386, 0.0395, 0.0324, 0.0485, 0.0376, 0.0662, 0.0419, 0.0429,\n",
              "                      0.0549, 0.0401, 0.0661, 0.0397, 0.0361, 0.0454, 0.0315, 0.0458, 0.0458,\n",
              "                      0.0346, 0.0659, 0.0325, 0.0506, 0.0350, 0.0303, 0.0461, 0.0251, 0.0659,\n",
              "                      0.0542, 0.0551, 0.0635, 0.0305, 0.0390, 0.0546, 0.0426, 0.0393, 0.0317,\n",
              "                      0.0357, 0.0425, 0.0355, 0.0365, 0.0567, 0.0350, 0.0294, 0.0377, 0.0460,\n",
              "                      0.0368, 0.0283, 0.0245, 0.0675, 0.0301, 0.0314, 0.0578, 0.0341, 0.0294,\n",
              "                      0.0664, 0.0506, 0.0744, 0.0302, 0.0395, 0.0313, 0.0480, 0.0369, 0.0441,\n",
              "                      0.0329, 0.0402, 0.0626, 0.0360, 0.0498, 0.0421, 0.0484, 0.0499, 0.0382,\n",
              "                      0.0519, 0.0404, 0.0327, 0.0664, 0.0339, 0.0333, 0.0655, 0.0466, 0.0256,\n",
              "                      0.0482, 0.0380, 0.0372, 0.0385, 0.0318, 0.0308, 0.0391, 0.0422, 0.0664,\n",
              "                      0.0488, 0.0453, 0.0273, 0.0664], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.18.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       53,  56,  54,  52,  49,  50,  51,  54,  50,  55,  53,  52,  51,  52,\n",
              "                       52,  57,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.18.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.18.final_layer_norm.bias',\n",
              "              tensor([-0.0174, -0.0218, -0.0046,  ..., -0.0690, -0.0308, -0.0276],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.self_attn.k_proj.weight',\n",
              "              tensor([[152],\n",
              "                      [ 58],\n",
              "                      [210],\n",
              "                      ...,\n",
              "                      [ 90],\n",
              "                      [122],\n",
              "                      [120]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0199, -0.0310, -0.0165,  ...,  0.0623, -0.0241,  0.0259],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.self_attn.k_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.19.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0368, 0.0371, 0.0383, 0.0374, 0.0550, 0.0598, 0.0534, 0.0594, 0.0368,\n",
              "                      0.0369, 0.0373, 0.0382, 0.0437, 0.0424, 0.0402, 0.0448, 0.0379, 0.0383,\n",
              "                      0.0371, 0.0372, 0.0369, 0.0366, 0.0366, 0.0365, 0.0374, 0.0368, 0.0371,\n",
              "                      0.0366, 0.0405, 0.0371, 0.0380, 0.0366, 0.0371, 0.0376, 0.0376, 0.0382,\n",
              "                      0.0320, 0.0301, 0.0377, 0.0382, 0.0378, 0.0369, 0.0374, 0.0371, 0.0379,\n",
              "                      0.0388, 0.0372, 0.0376, 0.0435, 0.0448, 0.0411, 0.0417, 0.0368, 0.0371,\n",
              "                      0.0367, 0.0369, 0.0377, 0.0407, 0.0372, 0.0371, 0.0374, 0.0379, 0.0378,\n",
              "                      0.0373], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.19.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  56,  49,  55,  57,  55,  48,  55,  53,  50,  55,  49,  54,  48,\n",
              "                       54,  52, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.self_attn.v_proj.weight',\n",
              "              tensor([[ 50],\n",
              "                      [ 18],\n",
              "                      [114],\n",
              "                      ...,\n",
              "                      [ 22],\n",
              "                      [200],\n",
              "                      [131]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.self_attn.v_proj.bias',\n",
              "              tensor([ 0.0022, -0.0011, -0.0293,  ...,  0.0166,  0.0168,  0.0048],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.self_attn.v_proj.weight.absmax',\n",
              "              tensor([255,   0,   0,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.19.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0327, 0.0330, 0.0303, 0.0317, 0.0328, 0.0284, 0.0259, 0.0341, 0.0457,\n",
              "                      0.0349, 0.0428, 0.0333, 0.0374, 0.0328, 0.0302, 0.0368, 0.0500, 0.0513,\n",
              "                      0.0494, 0.0486, 0.0437, 0.0342, 0.0273, 0.0306, 0.0474, 0.0509, 0.0539,\n",
              "                      0.0491, 0.0519, 0.0515, 0.0513, 0.0369, 0.0363, 0.0483, 0.0371, 0.0250,\n",
              "                      0.0421, 0.0448, 0.0414, 0.0428, 0.0508, 0.0527, 0.0428, 0.0397, 0.0540,\n",
              "                      0.0522, 0.0435, 0.0530, 0.0439, 0.0370, 0.0378, 0.0386, 0.0290, 0.0385,\n",
              "                      0.0253, 0.0332, 0.0324, 0.0283, 0.0374, 0.0350, 0.0311, 0.0340, 0.0316,\n",
              "                      0.0347], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.19.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  48,  57,  56,  51,  49,  52,  57,  49,  49,  49,  50,  55,  48,\n",
              "                       57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.self_attn.q_proj.weight',\n",
              "              tensor([[ 22],\n",
              "                      [157],\n",
              "                      [ 69],\n",
              "                      ...,\n",
              "                      [144],\n",
              "                      [251],\n",
              "                      [154]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.self_attn.q_proj.bias',\n",
              "              tensor([-0.0106,  0.0714, -0.0292,  ...,  0.0622,  0.0535,  0.0382],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.self_attn.q_proj.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ..., 255, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.19.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0345, 0.0335, 0.0339, 0.0335, 0.0614, 0.0643, 0.0647, 0.0670, 0.0340,\n",
              "                      0.0335, 0.0336, 0.0337, 0.0344, 0.0338, 0.0366, 0.0407, 0.0322, 0.0333,\n",
              "                      0.0335, 0.0346, 0.0333, 0.0340, 0.0334, 0.0341, 0.0331, 0.0337, 0.0342,\n",
              "                      0.0334, 0.0331, 0.0327, 0.0331, 0.0345, 0.0337, 0.0342, 0.0334, 0.0337,\n",
              "                      0.0336, 0.0337, 0.0331, 0.0341, 0.0304, 0.0335, 0.0333, 0.0331, 0.0336,\n",
              "                      0.0334, 0.0329, 0.0339, 0.0467, 0.0445, 0.0529, 0.0465, 0.0339, 0.0336,\n",
              "                      0.0337, 0.0336, 0.0332, 0.0334, 0.0335, 0.0335, 0.0359, 0.0350, 0.0364,\n",
              "                      0.0357], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.19.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       57,  49,  54,  51,  52,  52,  54,  55,  50,  52,  52,  49,  52,  56,\n",
              "                       50,  53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.self_attn.out_proj.weight',\n",
              "              tensor([[178],\n",
              "                      [179],\n",
              "                      [157],\n",
              "                      ...,\n",
              "                      [122],\n",
              "                      [132],\n",
              "                      [118]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.self_attn.out_proj.bias',\n",
              "              tensor([ 0.0218,  0.0358,  0.0286,  ..., -0.0027,  0.0128, -0.0115],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.self_attn.out_proj.weight.absmax',\n",
              "              tensor([255,   0, 255,  ..., 255,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.19.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0615, 0.0397, 0.0397, 0.0458, 0.0445, 0.0477, 0.0619, 0.0596, 0.0618,\n",
              "                      0.0403, 0.0273, 0.0596, 0.0491, 0.0377, 0.0323, 0.0356, 0.0313, 0.0372,\n",
              "                      0.0514, 0.0545, 0.0464, 0.0627, 0.0349, 0.0398, 0.0433, 0.0597, 0.0347,\n",
              "                      0.0380, 0.0511, 0.0823, 0.0417, 0.0388, 0.0446, 0.0538, 0.0338, 0.0401,\n",
              "                      0.0338, 0.0377, 0.0603, 0.0333, 0.0512, 0.0468, 0.0535, 0.0464, 0.0359,\n",
              "                      0.0411, 0.0327, 0.0547, 0.0340, 0.0298, 0.0298, 0.0326, 0.0605, 0.0409,\n",
              "                      0.0627, 0.0394, 0.0420, 0.0396, 0.0415, 0.0430, 0.0333, 0.0433, 0.0314,\n",
              "                      0.0370], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.19.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  51,  49,  49,  48,  50,  57,  49,  57,  53,  55,  56,  53,  53,\n",
              "                       50,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.self_attn_layer_norm.bias',\n",
              "              tensor([ 0.0216,  0.0095,  0.0092,  ..., -0.0668,  0.0025,  0.0050],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.fc1.weight',\n",
              "              tensor([[147],\n",
              "                      [183],\n",
              "                      [231],\n",
              "                      ...,\n",
              "                      [105],\n",
              "                      [102],\n",
              "                      [203]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.fc1.bias',\n",
              "              tensor([-0.0008, -0.0043, -0.0149,  ...,  0.0013, -0.0293, -0.0288],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.fc1.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ..., 255, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.19.fc1.weight.nested_absmax',\n",
              "              tensor([0.0492, 0.0439, 0.0376, 0.0326, 0.0383, 0.0500, 0.0432, 0.0495, 0.0407,\n",
              "                      0.0392, 0.0353, 0.0426, 0.0498, 0.0401, 0.0396, 0.0417, 0.0464, 0.0421,\n",
              "                      0.0454, 0.0489, 0.0493, 0.0481, 0.0364, 0.0476, 0.0481, 0.0486, 0.0503,\n",
              "                      0.0498, 0.0454, 0.0368, 0.0322, 0.0472, 0.0483, 0.0495, 0.0389, 0.0300,\n",
              "                      0.0437, 0.0510, 0.0437, 0.0483, 0.0485, 0.0460, 0.0347, 0.0396, 0.0474,\n",
              "                      0.0446, 0.0493, 0.0437, 0.0358, 0.0497, 0.0330, 0.0355, 0.0520, 0.0498,\n",
              "                      0.0481, 0.0360, 0.0468, 0.0449, 0.0423, 0.0418, 0.0448, 0.0392, 0.0490,\n",
              "                      0.0493, 0.0313, 0.0487, 0.0486, 0.0495, 0.0558, 0.0387, 0.0472, 0.0486,\n",
              "                      0.0340, 0.0489, 0.0493, 0.0473, 0.0494, 0.0453, 0.0455, 0.0501, 0.0415,\n",
              "                      0.0329, 0.0500, 0.0492, 0.0495, 0.0422, 0.0462, 0.0469, 0.0390, 0.0428,\n",
              "                      0.0434, 0.0498, 0.0340, 0.0308, 0.0459, 0.0478, 0.0457, 0.0499, 0.0408,\n",
              "                      0.0435, 0.0490, 0.0321, 0.0497, 0.0307, 0.0504, 0.0431, 0.0494, 0.0400,\n",
              "                      0.0397, 0.0369, 0.0484, 0.0494, 0.0467, 0.0470, 0.0443, 0.0456, 0.0393,\n",
              "                      0.0368, 0.0495, 0.0319, 0.0462, 0.0489, 0.0339, 0.0409, 0.0497, 0.0371,\n",
              "                      0.0370, 0.0354, 0.0338, 0.0497, 0.0484, 0.0487, 0.0369, 0.0415, 0.0494,\n",
              "                      0.0414, 0.0393, 0.0298, 0.0407, 0.0490, 0.0489, 0.0450, 0.0363, 0.0494,\n",
              "                      0.0373, 0.0492, 0.0298, 0.0490, 0.0476, 0.0409, 0.0494, 0.0405, 0.0351,\n",
              "                      0.0368, 0.0424, 0.0437, 0.0473, 0.0456, 0.0494, 0.0465, 0.0401, 0.0428,\n",
              "                      0.0398, 0.0462, 0.0478, 0.0396, 0.0490, 0.0343, 0.0440, 0.0403, 0.0465,\n",
              "                      0.0384, 0.0393, 0.0493, 0.0493, 0.0488, 0.0473, 0.0417, 0.0485, 0.0455,\n",
              "                      0.0499, 0.0453, 0.0298, 0.0445, 0.0465, 0.0465, 0.0498, 0.0493, 0.0489,\n",
              "                      0.0495, 0.0493, 0.0405, 0.0470, 0.0479, 0.0351, 0.0503, 0.0484, 0.0406,\n",
              "                      0.0471, 0.0430, 0.0465, 0.0440, 0.0362, 0.0328, 0.0347, 0.0493, 0.0452,\n",
              "                      0.0473, 0.0398, 0.0430, 0.0389, 0.0349, 0.0344, 0.0462, 0.0413, 0.0493,\n",
              "                      0.0472, 0.0484, 0.0494, 0.0501, 0.0381, 0.0403, 0.0354, 0.0493, 0.0478,\n",
              "                      0.0331, 0.0404, 0.0495, 0.0433, 0.0483, 0.0412, 0.0396, 0.0367, 0.0409,\n",
              "                      0.0389, 0.0494, 0.0506, 0.0478, 0.0428, 0.0444, 0.0490, 0.0391, 0.0368,\n",
              "                      0.0393, 0.0477, 0.0464, 0.0426, 0.0493, 0.0493, 0.0432, 0.0501, 0.0478,\n",
              "                      0.0494, 0.0413, 0.0490, 0.0495], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.19.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  53,  53,  57,  48,  49,  57,  51,  50,  55,  49,  54,  51,  54,\n",
              "                       57,  54, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.fc2.weight',\n",
              "              tensor([[ 73],\n",
              "                      [109],\n",
              "                      [232],\n",
              "                      ...,\n",
              "                      [ 87],\n",
              "                      [220],\n",
              "                      [198]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.fc2.bias',\n",
              "              tensor([-0.0258, -0.0101, -0.0082,  ...,  0.0323, -0.0193,  0.0101],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.fc2.weight.absmax',\n",
              "              tensor([  0,   0,   0,  ...,   0, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.19.fc2.weight.nested_absmax',\n",
              "              tensor([0.0501, 0.0572, 0.0665, 0.0394, 0.0247, 0.0455, 0.0431, 0.0440, 0.0665,\n",
              "                      0.0493, 0.0363, 0.0349, 0.0521, 0.0498, 0.0305, 0.0271, 0.0534, 0.0313,\n",
              "                      0.0258, 0.0534, 0.0390, 0.0431, 0.0341, 0.0348, 0.0686, 0.0366, 0.0461,\n",
              "                      0.0339, 0.0554, 0.0351, 0.0258, 0.0663, 0.0512, 0.0412, 0.0436, 0.0662,\n",
              "                      0.0385, 0.0333, 0.0476, 0.0274, 0.0346, 0.0588, 0.0377, 0.0409, 0.0408,\n",
              "                      0.0527, 0.0663, 0.0298, 0.0393, 0.0406, 0.0402, 0.0523, 0.0329, 0.0607,\n",
              "                      0.0660, 0.0418, 0.0346, 0.0617, 0.0568, 0.0376, 0.0504, 0.0407, 0.0392,\n",
              "                      0.0385, 0.0322, 0.0292, 0.0443, 0.0584, 0.0275, 0.0315, 0.0300, 0.0437,\n",
              "                      0.0301, 0.0436, 0.0515, 0.0372, 0.0663, 0.0581, 0.0368, 0.0660, 0.0549,\n",
              "                      0.0372, 0.0372, 0.0518, 0.0668, 0.0532, 0.0422, 0.0409, 0.0646, 0.0430,\n",
              "                      0.0655, 0.0334, 0.0337, 0.0504, 0.0498, 0.0435, 0.0526, 0.0328, 0.0430,\n",
              "                      0.0440, 0.0468, 0.0404, 0.0428, 0.0280, 0.0358, 0.0573, 0.0405, 0.0408,\n",
              "                      0.0350, 0.0649, 0.0349, 0.0463, 0.0649, 0.0662, 0.0415, 0.0477, 0.0356,\n",
              "                      0.0464, 0.1195, 0.0415, 0.0661, 0.0323, 0.0377, 0.0293, 0.0432, 0.0360,\n",
              "                      0.0659, 0.0545, 0.0264, 0.0362, 0.0499, 0.0419, 0.0458, 0.0430, 0.0384,\n",
              "                      0.0590, 0.0272, 0.0498, 0.0339, 0.0479, 0.0471, 0.0404, 0.0468, 0.0273,\n",
              "                      0.0378, 0.0377, 0.0413, 0.0386, 0.0391, 0.0377, 0.0383, 0.0511, 0.0454,\n",
              "                      0.0303, 0.0387, 0.0459, 0.0480, 0.0251, 0.0308, 0.0321, 0.0524, 0.0315,\n",
              "                      0.0511, 0.0364, 0.0599, 0.0448, 0.0292, 0.0505, 0.0483, 0.0658, 0.0659,\n",
              "                      0.0393, 0.0429, 0.0284, 0.0344, 0.0380, 0.0395, 0.0621, 0.0410, 0.0371,\n",
              "                      0.0526, 0.0457, 0.0317, 0.0501, 0.0648, 0.0432, 0.0366, 0.0290, 0.0531,\n",
              "                      0.0478, 0.0311, 0.0288, 0.0498, 0.0391, 0.0305, 0.0318, 0.0392, 0.0420,\n",
              "                      0.0470, 0.0432, 0.0629, 0.0333, 0.0482, 0.0524, 0.0429, 0.0521, 0.0608,\n",
              "                      0.0624, 0.0543, 0.0466, 0.0669, 0.0471, 0.0521, 0.0391, 0.0528, 0.0662,\n",
              "                      0.0663, 0.0446, 0.0668, 0.0554, 0.0406, 0.0497, 0.0413, 0.0520, 0.0428,\n",
              "                      0.0389, 0.0252, 0.0526, 0.0660, 0.0467, 0.0621, 0.0357, 0.0518, 0.0364,\n",
              "                      0.0480, 0.0382, 0.0411, 0.0663, 0.0437, 0.0317, 0.0390, 0.0561, 0.0368,\n",
              "                      0.0299, 0.0526, 0.0447, 0.0349, 0.0396, 0.0405, 0.0326, 0.0507, 0.0662,\n",
              "                      0.0358, 0.0440, 0.0468, 0.0662], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.19.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       53,  56,  55,  48,  54,  48,  54,  55,  53,  48,  50,  52,  57,  56,\n",
              "                       54,  51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.19.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.19.final_layer_norm.bias',\n",
              "              tensor([ 0.0036, -0.0061, -0.0291,  ..., -0.0665, -0.0209, -0.0318],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.self_attn.k_proj.weight',\n",
              "              tensor([[131],\n",
              "                      [ 83],\n",
              "                      [226],\n",
              "                      ...,\n",
              "                      [ 85],\n",
              "                      [ 32],\n",
              "                      [ 36]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0317, -0.0113, -0.0163,  ...,  0.0625,  0.0779,  0.0302],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.self_attn.k_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ..., 255,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.20.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0365, 0.0359, 0.0368, 0.0365, 0.0467, 0.0388, 0.0436, 0.0468, 0.0419,\n",
              "                      0.0413, 0.0435, 0.0424, 0.0599, 0.0512, 0.0585, 0.0541, 0.0364, 0.0364,\n",
              "                      0.0358, 0.0360, 0.0399, 0.0364, 0.0408, 0.0365, 0.0467, 0.0438, 0.0444,\n",
              "                      0.0418, 0.0394, 0.0395, 0.0382, 0.0379, 0.0420, 0.0417, 0.0439, 0.0409,\n",
              "                      0.0358, 0.0363, 0.0351, 0.0363, 0.0482, 0.0448, 0.0485, 0.0504, 0.0417,\n",
              "                      0.0445, 0.0402, 0.0421, 0.0447, 0.0406, 0.0392, 0.0470, 0.0409, 0.0412,\n",
              "                      0.0406, 0.0434, 0.0412, 0.0384, 0.0392, 0.0403, 0.0379, 0.0368, 0.0374,\n",
              "                      0.0369], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.20.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  56,  53,  56,  55,  52,  48,  51,  50,  57,  55,  52,  50,  52,\n",
              "                       51,  50, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.self_attn.v_proj.weight',\n",
              "              tensor([[245],\n",
              "                      [154],\n",
              "                      [100],\n",
              "                      ...,\n",
              "                      [ 83],\n",
              "                      [  3],\n",
              "                      [149]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.self_attn.v_proj.bias',\n",
              "              tensor([-0.0144,  0.0024, -0.0161,  ...,  0.0065, -0.0075, -0.0309],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.self_attn.v_proj.weight.absmax',\n",
              "              tensor([255,   0, 255,  ..., 255, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.20.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0335, 0.0335, 0.0416, 0.0531, 0.0353, 0.0435, 0.0482, 0.0457, 0.0394,\n",
              "                      0.0253, 0.0305, 0.0419, 0.0258, 0.0371, 0.0308, 0.0251, 0.0511, 0.0496,\n",
              "                      0.0458, 0.0508, 0.0250, 0.0406, 0.0283, 0.0328, 0.0459, 0.0401, 0.0508,\n",
              "                      0.0479, 0.0276, 0.0259, 0.0267, 0.0278, 0.0377, 0.0362, 0.0365, 0.0372,\n",
              "                      0.0566, 0.0514, 0.0467, 0.0497, 0.0369, 0.0376, 0.0477, 0.0363, 0.0250,\n",
              "                      0.0312, 0.0391, 0.0257, 0.0422, 0.0317, 0.0323, 0.0540, 0.0319, 0.0345,\n",
              "                      0.0309, 0.0332, 0.0378, 0.0374, 0.0551, 0.0456, 0.0565, 0.0569, 0.0545,\n",
              "                      0.0559], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.20.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  55,  57,  57,  50,  55,  52,  54,  56,  50,  57,  57,  56,  54,\n",
              "                       53,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.self_attn.q_proj.weight',\n",
              "              tensor([[127],\n",
              "                      [154],\n",
              "                      [ 78],\n",
              "                      ...,\n",
              "                      [123],\n",
              "                      [118],\n",
              "                      [150]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.self_attn.q_proj.bias',\n",
              "              tensor([ 0.0022,  0.0151,  0.0061,  ..., -0.0752, -0.0651, -0.0549],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.self_attn.q_proj.weight.absmax',\n",
              "              tensor([255, 255, 255,  ...,   0, 255,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.20.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0338, 0.0340, 0.0340, 0.0333, 0.0482, 0.0500, 0.0467, 0.0472, 0.0344,\n",
              "                      0.0354, 0.0360, 0.0355, 0.0622, 0.0640, 0.0642, 0.0628, 0.0319, 0.0321,\n",
              "                      0.0339, 0.0329, 0.0345, 0.0333, 0.0335, 0.0360, 0.0522, 0.0469, 0.0488,\n",
              "                      0.0506, 0.0265, 0.0332, 0.0331, 0.0329, 0.0343, 0.0339, 0.0338, 0.0343,\n",
              "                      0.0316, 0.0318, 0.0327, 0.0367, 0.0363, 0.0387, 0.0343, 0.0344, 0.0356,\n",
              "                      0.0355, 0.0360, 0.0352, 0.0343, 0.0336, 0.0361, 0.0354, 0.0351, 0.0358,\n",
              "                      0.0345, 0.0355, 0.0352, 0.0341, 0.0346, 0.0351, 0.0335, 0.0326, 0.0334,\n",
              "                      0.0325], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.20.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       57,  49,  56,  52,  52,  51,  54,  53,  48,  48,  48,  55,  50,  52,\n",
              "                       55,  57, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.self_attn.out_proj.weight',\n",
              "              tensor([[ 33],\n",
              "                      [146],\n",
              "                      [149],\n",
              "                      ...,\n",
              "                      [  5],\n",
              "                      [136],\n",
              "                      [ 36]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.self_attn.out_proj.bias',\n",
              "              tensor([ 0.0119,  0.0193, -0.0013,  ...,  0.0056,  0.0072, -0.0188],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.self_attn.out_proj.weight.absmax',\n",
              "              tensor([255, 255,   0,  ...,   0, 255, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.self_attn.out_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.20.self_attn.out_proj.weight.nested_absmax',\n",
              "              tensor([0.0541, 0.0428, 0.0393, 0.0342, 0.0652, 0.0464, 0.0634, 0.0536, 0.0533,\n",
              "                      0.0463, 0.0504, 0.0427, 0.0525, 0.0447, 0.0425, 0.0474, 0.0452, 0.0446,\n",
              "                      0.0349, 0.0554, 0.0402, 0.0664, 0.0655, 0.0414, 0.0489, 0.0538, 0.0536,\n",
              "                      0.0416, 0.0575, 0.0740, 0.0450, 0.0455, 0.0432, 0.0476, 0.0401, 0.0599,\n",
              "                      0.0331, 0.0465, 0.0646, 0.0466, 0.0551, 0.0444, 0.0523, 0.0416, 0.0361,\n",
              "                      0.0638, 0.0650, 0.0485, 0.0602, 0.0554, 0.0358, 0.0401, 0.0544, 0.0495,\n",
              "                      0.0555, 0.0563, 0.0554, 0.0569, 0.0367, 0.0494, 0.0415, 0.0477, 0.0526,\n",
              "                      0.0504], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.self_attn.out_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.20.self_attn.out_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       53,  57,  52,  52,  49,  53,  55,  48,  49,  57,  50,  53,  55,  53,\n",
              "                       52,  53,  53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.self_attn_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.self_attn_layer_norm.bias',\n",
              "              tensor([ 0.0344,  0.0156, -0.0149,  ..., -0.0640, -0.0026, -0.0080],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.fc1.weight',\n",
              "              tensor([[167],\n",
              "                      [129],\n",
              "                      [ 33],\n",
              "                      ...,\n",
              "                      [165],\n",
              "                      [ 55],\n",
              "                      [119]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.fc1.bias',\n",
              "              tensor([-0.0148, -0.0110, -0.0074,  ..., -0.0563, -0.0182,  0.0153],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.fc1.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ...,   0,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.fc1.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.20.fc1.weight.nested_absmax',\n",
              "              tensor([0.0469, 0.0435, 0.0496, 0.0505, 0.0460, 0.0341, 0.0468, 0.0426, 0.0360,\n",
              "                      0.0496, 0.0495, 0.0496, 0.0473, 0.0424, 0.0411, 0.0498, 0.0406, 0.0425,\n",
              "                      0.0495, 0.0366, 0.0427, 0.0485, 0.0488, 0.0497, 0.0383, 0.0416, 0.0491,\n",
              "                      0.0451, 0.0365, 0.0449, 0.0323, 0.0493, 0.0468, 0.0441, 0.0448, 0.0439,\n",
              "                      0.0495, 0.0469, 0.0479, 0.0491, 0.0492, 0.0467, 0.0330, 0.0384, 0.0473,\n",
              "                      0.0475, 0.0435, 0.0481, 0.0494, 0.0419, 0.0484, 0.0494, 0.0493, 0.0437,\n",
              "                      0.0458, 0.0491, 0.0488, 0.0354, 0.0424, 0.0493, 0.0369, 0.0427, 0.0495,\n",
              "                      0.0493, 0.0495, 0.0443, 0.0493, 0.0499, 0.0432, 0.0501, 0.0497, 0.0435,\n",
              "                      0.0368, 0.0413, 0.0475, 0.0397, 0.0505, 0.0444, 0.0394, 0.0333, 0.0356,\n",
              "                      0.0458, 0.0486, 0.0454, 0.0484, 0.0448, 0.0454, 0.0420, 0.0468, 0.0460,\n",
              "                      0.0494, 0.0366, 0.0367, 0.0492, 0.0472, 0.0493, 0.0504, 0.0490, 0.0421,\n",
              "                      0.0469, 0.0477, 0.0493, 0.0504, 0.0494, 0.0496, 0.0449, 0.0461, 0.0495,\n",
              "                      0.0385, 0.0369, 0.0411, 0.0459, 0.0410, 0.0494, 0.0490, 0.0350, 0.0407,\n",
              "                      0.0491, 0.0493, 0.0377, 0.0404, 0.0478, 0.0397, 0.0405, 0.0426, 0.0377,\n",
              "                      0.0493, 0.0387, 0.0365, 0.0490, 0.0379, 0.0493, 0.0447, 0.0458, 0.0497,\n",
              "                      0.0397, 0.0361, 0.0433, 0.0515, 0.0405, 0.0327, 0.0363, 0.0491, 0.0491,\n",
              "                      0.0592, 0.0401, 0.0480, 0.0463, 0.0328, 0.0343, 0.0436, 0.0495, 0.0495,\n",
              "                      0.0432, 0.0421, 0.0501, 0.0495, 0.0493, 0.0438, 0.0324, 0.0491, 0.0328,\n",
              "                      0.0412, 0.0502, 0.0388, 0.0489, 0.0380, 0.0336, 0.0495, 0.0330, 0.0504,\n",
              "                      0.0403, 0.0505, 0.0492, 0.0476, 0.0464, 0.0358, 0.0350, 0.0472, 0.0495,\n",
              "                      0.0495, 0.0480, 0.0340, 0.0489, 0.0482, 0.0495, 0.0469, 0.0487, 0.0487,\n",
              "                      0.0405, 0.0404, 0.0369, 0.0407, 0.0449, 0.0380, 0.0369, 0.0352, 0.0466,\n",
              "                      0.0385, 0.0332, 0.0308, 0.0467, 0.0438, 0.0451, 0.0487, 0.0497, 0.0491,\n",
              "                      0.0492, 0.0496, 0.0479, 0.0485, 0.0440, 0.0495, 0.0491, 0.0409, 0.0411,\n",
              "                      0.0408, 0.0482, 0.0488, 0.0348, 0.0386, 0.0492, 0.0490, 0.0316, 0.0470,\n",
              "                      0.0372, 0.0474, 0.0498, 0.0488, 0.0458, 0.0466, 0.0341, 0.0495, 0.0356,\n",
              "                      0.0441, 0.0347, 0.0489, 0.0487, 0.0401, 0.0349, 0.0393, 0.0410, 0.0433,\n",
              "                      0.0374, 0.0493, 0.0521, 0.0484, 0.0418, 0.0423, 0.0509, 0.0435, 0.0353,\n",
              "                      0.0491, 0.0495, 0.0488, 0.0472], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.fc1.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.20.fc1.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  52,\n",
              "                       48,  57,  54,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       55,  53,  53,  52,  57,  50,  52,  52,  56,  56,  48,  54,  55,  54,\n",
              "                       50,  55, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.fc2.weight',\n",
              "              tensor([[ 70],\n",
              "                      [162],\n",
              "                      [ 42],\n",
              "                      ...,\n",
              "                      [180],\n",
              "                      [117],\n",
              "                      [167]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.fc2.bias',\n",
              "              tensor([-0.0336, -0.0038,  0.0199,  ...,  0.0104, -0.0198,  0.0128],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.fc2.weight.absmax',\n",
              "              tensor([  0,   0, 255,  ..., 255, 255, 113], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.fc2.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.20.fc2.weight.nested_absmax',\n",
              "              tensor([0.0427, 0.0437, 0.0532, 0.0543, 0.0507, 0.0472, 0.0634, 0.0633, 0.0578,\n",
              "                      0.0450, 0.0454, 0.0573, 0.0535, 0.0377, 0.0477, 0.0569, 0.0579, 0.0627,\n",
              "                      0.0455, 0.0622, 0.0492, 0.0558, 0.0638, 0.0403, 0.0643, 0.0502, 0.0488,\n",
              "                      0.0504, 0.0423, 0.0480, 0.0612, 0.0637, 0.0530, 0.0523, 0.0505, 0.0556,\n",
              "                      0.0398, 0.0364, 0.0413, 0.0380, 0.0475, 0.0443, 0.0282, 0.0543, 0.0432,\n",
              "                      0.0386, 0.0595, 0.0394, 0.0374, 0.0410, 0.0394, 0.0528, 0.0565, 0.0638,\n",
              "                      0.0541, 0.0604, 0.0624, 0.0635, 0.0413, 0.0599, 0.0482, 0.0637, 0.0632,\n",
              "                      0.0559, 0.0637, 0.0418, 0.0639, 0.0424, 0.0438, 0.0637, 0.0427, 0.0562,\n",
              "                      0.0466, 0.0516, 0.0488, 0.0406, 0.0402, 0.0608, 0.0565, 0.0361, 0.0508,\n",
              "                      0.0418, 0.0476, 0.0611, 0.0659, 0.0400, 0.0383, 0.0424, 0.0373, 0.0580,\n",
              "                      0.0323, 0.0604, 0.0635, 0.0388, 0.0332, 0.0592, 0.0466, 0.0478, 0.0394,\n",
              "                      0.0348, 0.0557, 0.0459, 0.0529, 0.0472, 0.0549, 0.0427, 0.0542, 0.0339,\n",
              "                      0.0627, 0.0594, 0.0472, 0.0494, 0.0457, 0.0545, 0.0480, 0.0420, 0.0611,\n",
              "                      0.0413, 0.1782, 0.0562, 0.0513, 0.0386, 0.0629, 0.0584, 0.0444, 0.0429,\n",
              "                      0.0529, 0.0466, 0.0560, 0.0632, 0.0638, 0.0382, 0.0607, 0.0545, 0.0537,\n",
              "                      0.0634, 0.0409, 0.0355, 0.0587, 0.0424, 0.0518, 0.0635, 0.0373, 0.0449,\n",
              "                      0.0344, 0.0640, 0.0431, 0.0634, 0.0452, 0.0455, 0.0390, 0.0607, 0.0463,\n",
              "                      0.0406, 0.0490, 0.0629, 0.0374, 0.0626, 0.0639, 0.0593, 0.0335, 0.0418,\n",
              "                      0.0639, 0.0642, 0.0570, 0.0422, 0.0390, 0.0479, 0.0640, 0.0396, 0.0621,\n",
              "                      0.0547, 0.0406, 0.0638, 0.0434, 0.0472, 0.0375, 0.0576, 0.0579, 0.0305,\n",
              "                      0.0478, 0.0412, 0.0322, 0.0591, 0.0458, 0.0638, 0.0414, 0.0569, 0.0433,\n",
              "                      0.0433, 0.0471, 0.0385, 0.0638, 0.0526, 0.0348, 0.0597, 0.0523, 0.0511,\n",
              "                      0.0639, 0.0618, 0.0581, 0.0515, 0.0463, 0.0595, 0.0639, 0.0618, 0.0397,\n",
              "                      0.0574, 0.0434, 0.0543, 0.0557, 0.0440, 0.0615, 0.0569, 0.0363, 0.0535,\n",
              "                      0.0500, 0.0364, 0.0638, 0.0638, 0.0462, 0.0435, 0.0562, 0.0407, 0.0434,\n",
              "                      0.0613, 0.0394, 0.0582, 0.0451, 0.0552, 0.0460, 0.0469, 0.0388, 0.0502,\n",
              "                      0.0382, 0.0480, 0.0477, 0.0487, 0.0637, 0.0574, 0.0477, 0.0607, 0.0392,\n",
              "                      0.0538, 0.0573, 0.0458, 0.0449, 0.0547, 0.0617, 0.0540, 0.0405, 0.0440,\n",
              "                      0.0487, 0.0430, 0.0534, 0.0638], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.fc2.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.20.fc2.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  52,  48,  57,  54,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  49,  48,  56,  57,  48,  57,  56,  52,  53,  51,  53,  50,  49,\n",
              "                       55,  51, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.20.final_layer_norm.weight',\n",
              "              tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.20.final_layer_norm.bias',\n",
              "              tensor([ 0.0057, -0.0097, -0.0164,  ..., -0.0613, -0.0065, -0.0152],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.21.self_attn.k_proj.weight',\n",
              "              tensor([[141],\n",
              "                      [181],\n",
              "                      [212],\n",
              "                      ...,\n",
              "                      [133],\n",
              "                      [162],\n",
              "                      [201]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.21.self_attn.k_proj.bias',\n",
              "              tensor([ 0.0264,  0.0384,  0.0203,  ...,  0.0313, -0.0515,  0.0169],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.21.self_attn.k_proj.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ...,   0,   0, 255], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.21.self_attn.k_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.21.self_attn.k_proj.weight.nested_absmax',\n",
              "              tensor([0.0380, 0.0380, 0.0472, 0.0460, 0.0365, 0.0361, 0.0361, 0.0316, 0.0452,\n",
              "                      0.0438, 0.0456, 0.0434, 0.0409, 0.0391, 0.0415, 0.0401, 0.0486, 0.0474,\n",
              "                      0.0516, 0.0532, 0.0521, 0.0492, 0.0484, 0.0474, 0.0530, 0.0443, 0.0466,\n",
              "                      0.0437, 0.0417, 0.0480, 0.0422, 0.0353, 0.0484, 0.0438, 0.0441, 0.0474,\n",
              "                      0.0484, 0.0517, 0.0516, 0.0471, 0.0451, 0.0452, 0.0465, 0.0466, 0.0361,\n",
              "                      0.0361, 0.0358, 0.0346, 0.0471, 0.0521, 0.0514, 0.0565, 0.0434, 0.0439,\n",
              "                      0.0461, 0.0488, 0.0485, 0.0535, 0.0487, 0.0496, 0.0421, 0.0461, 0.0447,\n",
              "                      0.0411], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.21.self_attn.k_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.21.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       56,  56,  57,  48,  54,  56,  57,  49,  54,  52,  52,  48,  48,  49,\n",
              "                       48,  49, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.21.self_attn.v_proj.weight',\n",
              "              tensor([[ 23],\n",
              "                      [ 90],\n",
              "                      [146],\n",
              "                      ...,\n",
              "                      [153],\n",
              "                      [170],\n",
              "                      [222]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.21.self_attn.v_proj.bias',\n",
              "              tensor([-0.0189, -0.0191, -0.0077,  ...,  0.0212, -0.0034, -0.0010],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.21.self_attn.v_proj.weight.absmax',\n",
              "              tensor([  0, 255, 255,  ...,   0,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.21.self_attn.v_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.21.self_attn.v_proj.weight.nested_absmax',\n",
              "              tensor([0.0432, 0.0354, 0.0309, 0.0434, 0.0359, 0.0272, 0.0299, 0.0264, 0.0276,\n",
              "                      0.0295, 0.0224, 0.0413, 0.0410, 0.0574, 0.0487, 0.0563, 0.0329, 0.0320,\n",
              "                      0.0275, 0.0303, 0.0254, 0.0353, 0.0290, 0.0285, 0.0337, 0.0375, 0.0395,\n",
              "                      0.0338, 0.0564, 0.0423, 0.0563, 0.0525, 0.0340, 0.0275, 0.0335, 0.0299,\n",
              "                      0.0344, 0.0301, 0.0358, 0.0346, 0.0512, 0.0426, 0.0469, 0.0440, 0.0575,\n",
              "                      0.0576, 0.0539, 0.0575, 0.0407, 0.0550, 0.0460, 0.0432, 0.0541, 0.0520,\n",
              "                      0.0575, 0.0559, 0.0368, 0.0328, 0.0322, 0.0316, 0.0387, 0.0312, 0.0296,\n",
              "                      0.0450], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.21.self_attn.v_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ('model.decoder.layers.21.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4',\n",
              "              tensor([123,  34, 113, 117,  97, 110, 116,  95, 116, 121, 112, 101,  34,  58,\n",
              "                       32,  34, 110, 102,  52,  34,  44,  32,  34,  98, 108, 111,  99, 107,\n",
              "                      115, 105, 122, 101,  34,  58,  32,  54,  52,  44,  32,  34, 100, 116,\n",
              "                      121, 112, 101,  34,  58,  32,  34, 102, 108, 111,  97, 116,  51,  50,\n",
              "                       34,  44,  32,  34, 115, 104,  97, 112, 101,  34,  58,  32,  91,  49,\n",
              "                       48,  50,  52,  44,  32,  49,  48,  50,  52,  93,  44,  32,  34, 110,\n",
              "                      101, 115, 116, 101, 100,  95,  98, 108, 111,  99, 107, 115, 105, 122,\n",
              "                      101,  34,  58,  32,  50,  53,  54,  44,  32,  34, 110, 101, 115, 116,\n",
              "                      101, 100,  95, 100, 116, 121, 112, 101,  34,  58,  32,  34, 102, 108,\n",
              "                      111,  97, 116,  51,  50,  34,  44,  32,  34, 110, 101, 115, 116, 101,\n",
              "                      100,  95, 111, 102, 102, 115, 101, 116,  34,  58,  32,  48,  46,  48,\n",
              "                       54,  55,  52,  48,  52,  52,  57,  51,  54,  56,  57,  53,  51,  55,\n",
              "                       48,  53, 125], dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.21.self_attn.q_proj.weight',\n",
              "              tensor([[170],\n",
              "                      [200],\n",
              "                      [168],\n",
              "                      ...,\n",
              "                      [ 19],\n",
              "                      [209],\n",
              "                      [166]], device='cuda:0', dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.21.self_attn.q_proj.bias',\n",
              "              tensor([ 0.1126, -0.1066, -0.0827,  ..., -0.1136,  0.1242,  0.1251],\n",
              "                     device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.21.self_attn.q_proj.weight.absmax',\n",
              "              tensor([  0, 255,   0,  ..., 255,   0,   0], device='cuda:0',\n",
              "                     dtype=torch.uint8)),\n",
              "             ('model.decoder.layers.21.self_attn.q_proj.weight.quant_map',\n",
              "              tensor([-1.0000, -0.6963, -0.5249, -0.3950, -0.2844, -0.1848, -0.0911,  0.0000,\n",
              "                       0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5625,  0.7231,  1.0000],\n",
              "                     device='cuda:0')),\n",
              "             ('model.decoder.layers.21.self_attn.q_proj.weight.nested_absmax',\n",
              "              tensor([0.0339, 0.0396, 0.0390, 0.0332, 0.0306, 0.0334, 0.0303, 0.0308, 0.0352,\n",
              "                      0.0346, 0.0363, 0.0363, 0.0362, 0.0413, 0.0428, 0.0324, 0.0609, 0.0606,\n",
              "                      0.0527, 0.0639, 0.0327, 0.0334, 0.0317, 0.0328, 0.0572, 0.0433, 0.0458,\n",
              "                      0.0466, 0.0385, 0.0437, 0.0462, 0.0467, 0.0328, 0.0309, 0.0314, 0.0335,\n",
              "                      0.0351, 0.0352, 0.0334, 0.0330, 0.0392, 0.0398, 0.0379, 0.0415, 0.0313,\n",
              "                      0.0306, 0.0338, 0.0326, 0.0333, 0.0335, 0.0367, 0.0342, 0.0324, 0.0325,\n",
              "                      0.0325, 0.0351, 0.0364, 0.0353, 0.0351, 0.0384, 0.0323, 0.0370, 0.0328,\n",
              "                      0.0359], device='cuda:0', dtype=torch.float32)),\n",
              "             ('model.decoder.layers.21.self_attn.q_proj.weight.nested_quant_map',\n",
              "              tensor([-9.9316e-01, -9.7852e-01, -9.6484e-01, -9.5117e-01, -9.3652e-01,\n",
              "                      -9.2285e-01, -9.0820e-01, -8.9453e-01, -8.8086e-01, -8.6621e-01,\n",
              "                      -8.5254e-01, -8.3789e-01, -8.2422e-01, -8.1055e-01, -7.9590e-01,\n",
              "                      -7.8223e-01, -7.6758e-01, -7.5391e-01, -7.4023e-01, -7.2559e-01,\n",
              "                      -7.1191e-01, -6.9775e-01, -6.8359e-01, -6.6992e-01, -6.5527e-01,\n",
              "                      -6.4160e-01, -6.2744e-01, -6.1328e-01, -5.9961e-01, -5.8496e-01,\n",
              "                      -5.7129e-01, -5.5713e-01, -5.4297e-01, -5.2881e-01, -5.1465e-01,\n",
              "                      -5.0098e-01, -4.8657e-01, -4.7241e-01, -4.5850e-01, -4.4458e-01,\n",
              "                      -4.3042e-01, -4.1626e-01, -4.0210e-01, -3.8818e-01, -3.7427e-01,\n",
              "                      -3.6011e-01, -3.4595e-01, -3.3203e-01, -3.1787e-01, -3.0396e-01,\n",
              "                      -2.8979e-01, -2.7563e-01, -2.6172e-01, -2.4756e-01, -2.3364e-01,\n",
              "                      -2.1948e-01, -2.0544e-01, -1.9141e-01, -1.7725e-01, -1.6333e-01,\n",
              "                      -1.4917e-01, -1.3501e-01, -1.2103e-01, -1.0699e-01, -9.8572e-02,\n",
              "                      -9.5825e-02, -9.2957e-02, -9.0149e-02, -8.7341e-02, -8.4534e-02,\n",
              "                      -8.1726e-02, -7.8918e-02, -7.6050e-02, -7.3303e-02, -7.0435e-02,\n",
              "                      -6.7688e-02, -6.4819e-02, -6.2073e-02, -5.9235e-02, -5.6396e-02,\n",
              "                      -5.3619e-02, -5.0781e-02, -4.7943e-02, -4.5135e-02, -4.2328e-02,\n",
              "                      -3.9520e-02, -3.6713e-02, -3.3875e-02, -3.1097e-02, -2.8275e-02,\n",
              "                      -2.5467e-02, -2.2659e-02, -1.9836e-02, -1.7029e-02, -1.4206e-02,\n",
              "                      -1.1398e-02, -9.7198e-03, -9.1629e-03, -8.5907e-03, -8.0261e-03,\n",
              "                      -7.4692e-03, -6.9046e-03, -6.3477e-03, -5.7831e-03, -5.2147e-03,\n",
              "                      -4.6539e-03, -4.0932e-03, -3.5305e-03, -2.9678e-03, -2.4052e-03,\n",
              "                      -1.8435e-03, -1.2817e-03, -9.4366e-04, -8.3113e-04, -7.1859e-04,\n",
              "                      -6.0654e-04, -4.9353e-04, -3.8123e-04, -2.6846e-04, -1.5628e-04,\n",
              "                      -8.8751e-05, -6.6221e-05, -4.3750e-05, -2.1219e-05, -7.7486e-06,\n",
              "                      -3.2783e-06, -5.3644e-07,  0.0000e+00,  5.3644e-07,  3.2783e-06,\n",
              "                       7.7486e-06,  2.1219e-05,  4.3750e-05,  6.6221e-05,  8.8751e-05,\n",
              "                       1.5628e-04,  2.6846e-04,  3.8123e-04,  4.9353e-04,  6.0654e-04,\n",
              "                       7.1859e-04,  8.3113e-04,  9.4366e-04,  1.2817e-03,  1.8435e-03,\n",
              "                       2.4052e-03,  2.9678e-03,  3.5305e-03,  4.0932e-03,  4.6539e-03,\n",
              "                       5.2147e-03,  5.7831e-03,  6.3477e-03,  6.9046e-03,  7.4692e-03,\n",
              "                       8.0261e-03,  8.5907e-03,  9.1629e-03,  9.7198e-03,  1.1398e-02,\n",
              "                       1.4206e-02,  1.7029e-02,  1.9836e-02,  2.2659e-02,  2.5467e-02,\n",
              "                       2.8275e-02,  3.1097e-02,  3.3875e-02,  3.6713e-02,  3.9520e-02,\n",
              "                       4.2328e-02,  4.5135e-02,  4.7943e-02,  5.0781e-02,  5.3619e-02,\n",
              "                       5.6396e-02,  5.9235e-02,  6.2073e-02,  6.4819e-02,  6.7688e-02,\n",
              "                       7.0435e-02,  7.3303e-02,  7.6050e-02,  7.8918e-02,  8.1726e-02,\n",
              "                       8.4534e-02,  8.7341e-02,  9.0149e-02,  9.2957e-02,  9.5825e-02,\n",
              "                       9.8572e-02,  1.0699e-01,  1.2103e-01,  1.3501e-01,  1.4917e-01,\n",
              "                       1.6333e-01,  1.7725e-01,  1.9141e-01,  2.0544e-01,  2.1948e-01,\n",
              "                       2.3364e-01,  2.4756e-01,  2.6172e-01,  2.7563e-01,  2.8979e-01,\n",
              "                       3.0396e-01,  3.1787e-01,  3.3203e-01,  3.4595e-01,  3.6011e-01,\n",
              "                       3.7427e-01,  3.8818e-01,  4.0210e-01,  4.1626e-01,  4.3042e-01,\n",
              "                       4.4458e-01,  4.5850e-01,  4.7241e-01,  4.8657e-01,  5.0098e-01,\n",
              "                       5.1465e-01,  5.2881e-01,  5.4297e-01,  5.5713e-01,  5.7129e-01,\n",
              "                       5.8496e-01,  5.9961e-01,  6.1328e-01,  6.2744e-01,  6.4160e-01,\n",
              "                       6.5527e-01,  6.6992e-01,  6.8359e-01,  6.9775e-01,  7.1191e-01,\n",
              "                       7.2559e-01,  7.4023e-01,  7.5391e-01,  7.6758e-01,  7.8223e-01,\n",
              "                       7.9590e-01,  8.1055e-01,  8.2422e-01,  8.3789e-01,  8.5254e-01,\n",
              "                       8.6621e-01,  8.8086e-01,  8.9453e-01,  9.0820e-01,  9.2285e-01,\n",
              "                       9.3652e-01,  9.5117e-01,  9.6484e-01,  9.7852e-01,  9.9316e-01,\n",
              "                       1.0000e+00], device='cuda:0')),\n",
              "             ...])"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXaOZL5KbyIG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}